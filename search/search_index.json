{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"BrainLayer","text":"<p>Persistent memory for AI agents. Search, think, recall \u2014 across every conversation you've ever had.</p> <p>Your AI agent forgets everything between sessions. Every architecture decision, every debugging session, every preference you've expressed \u2014 gone.</p> <p>BrainLayer fixes this. It's a local-first memory layer that gives any MCP-compatible AI agent the ability to remember, think, and recall across conversations.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>14 MCP tools \u2014 think, recall, search, session analysis, file history, and more</li> <li>Local-first \u2014 SQLite + sqlite-vec, single file, no cloud, no Docker</li> <li>Hybrid search \u2014 semantic vectors + keyword, merged with Reciprocal Rank Fusion</li> <li>10-field enrichment \u2014 summary, tags, importance, intent, and more via local LLM</li> <li>Multi-source \u2014 Claude Code, WhatsApp, YouTube, Markdown, Claude Desktop, manual</li> <li>Works everywhere \u2014 Claude Code, Cursor, Zed, VS Code, any MCP client</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>pip install brainlayer\nbrainlayer init              # Interactive setup wizard\nbrainlayer index             # Index your conversations\n</code></pre> <p>Add to Claude Code (<code>~/.claude.json</code>): <pre><code>{\n  \"mcpServers\": {\n    \"brainlayer\": {\n      \"command\": \"brainlayer-mcp\"\n    }\n  }\n}\n</code></pre></p> <p>Your agent now has persistent memory. Ask it:</p> <ul> <li>\"What approach did I use for auth last month?\" \u2192 <code>brainlayer_think</code></li> <li>\"Show me everything about this file\" \u2192 <code>brainlayer_recall</code></li> <li>\"What was I working on yesterday?\" \u2192 <code>brainlayer_current_context</code></li> <li>\"Remember this for later\" \u2192 <code>brainlayer_store</code></li> </ul>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph LR\n    A[\"Claude Code / Cursor / Zed\"] --&gt;|MCP| B[\"BrainLayer MCP Server&lt;br/&gt;14 tools\"]\n    B --&gt; C[\"Hybrid Search&lt;br/&gt;semantic + keyword (RRF)\"]\n    C --&gt; D[\"SQLite + sqlite-vec&lt;br/&gt;single .db file\"]\n\n    E[\"Conversations&lt;br/&gt;JSONL / WhatsApp / YouTube\"] --&gt; F[\"Pipeline\"]\n    F --&gt;|extract \u2192 classify \u2192 chunk \u2192 embed| D\n    G[\"Local LLM&lt;br/&gt;Ollama / MLX\"] --&gt;|enrich| D</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start \u2014 full setup guide</li> <li>MCP Tools Reference \u2014 all 14 tools documented</li> <li>Configuration \u2014 environment variables and options</li> <li>Architecture \u2014 how it works under the hood</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>BrainLayer is a Python package with no external service dependencies. Everything runs locally.</p>"},{"location":"architecture/#overview","title":"Overview","text":"<pre><code>graph TD\n    subgraph Sources\n        A1[\"Claude Code&lt;br/&gt;~/.claude/projects/*.jsonl\"]\n        A2[\"WhatsApp&lt;br/&gt;exported .txt\"]\n        A3[\"YouTube&lt;br/&gt;transcripts\"]\n        A4[\"Markdown&lt;br/&gt;.md files\"]\n        A5[\"Claude Desktop&lt;br/&gt;JSON export\"]\n        A6[\"Manual&lt;br/&gt;brainlayer_store MCP tool\"]\n    end\n\n    subgraph Pipeline\n        B1[\"Extract&lt;br/&gt;parse conversations\"]\n        B2[\"Classify&lt;br/&gt;content type + value\"]\n        B3[\"Chunk&lt;br/&gt;AST-aware splitting\"]\n        B4[\"Embed&lt;br/&gt;bge-large-en-v1.5\"]\n    end\n\n    subgraph Storage\n        C[\"SQLite + sqlite-vec&lt;br/&gt;~/.local/share/brainlayer/brainlayer.db\"]\n    end\n\n    subgraph Post-Processing\n        D1[\"Chunk Enrichment&lt;br/&gt;10-field LLM metadata\"]\n        D2[\"Session Enrichment&lt;br/&gt;decisions, learnings\"]\n        D3[\"Brain Graph&lt;br/&gt;HDBSCAN clustering\"]\n    end\n\n    subgraph Interfaces\n        E1[\"MCP Server&lt;br/&gt;brainlayer-mcp (14 tools)\"]\n        E2[\"CLI&lt;br/&gt;brainlayer\"]\n        E3[\"FastAPI Daemon&lt;br/&gt;:8787\"]\n        E4[\"TUI Dashboard\"]\n    end\n\n    A1 &amp; A2 &amp; A3 &amp; A4 &amp; A5 --&gt; B1\n    A6 --&gt; C\n    B1 --&gt; B2 --&gt; B3 --&gt; B4 --&gt; C\n    C --&gt; D1 &amp; D2 &amp; D3\n    C --&gt; E1 &amp; E2 &amp; E3 &amp; E4</code></pre>"},{"location":"architecture/#pipeline-stages","title":"Pipeline Stages","text":""},{"location":"architecture/#stage-1-extract","title":"Stage 1: Extract","text":"<p>Parses source files into individual messages with metadata.</p> <ul> <li>Claude Code: JSONL conversation files from <code>~/.claude/projects/</code></li> <li>Content-addressable storage: System prompts are deduplicated via SHA-256 hash</li> <li>Session detection: Groups messages into sessions by ID and temporal proximity</li> </ul>"},{"location":"architecture/#stage-2-classify","title":"Stage 2: Classify","text":"<p>Each message is classified by content type and value:</p> Type Value Action <code>ai_code</code> HIGH Preserve verbatim <code>stack_trace</code> HIGH Preserve exact (never split) <code>user_message</code> HIGH Preserve <code>assistant_text</code> MEDIUM Preserve <code>file_read</code> MEDIUM Context-dependent <code>git_diff</code> MEDIUM Extract changed entities <code>build_log</code> LOW Summarize <code>dir_listing</code> LOW Structure only <code>noise</code> SKIP Filter out"},{"location":"architecture/#stage-3-chunk","title":"Stage 3: Chunk","text":"<p>Splits content into indexable chunks (~500 tokens):</p> <ul> <li>AST-aware chunking with tree-sitter for code</li> <li>Sentence-boundary splitting for prose</li> <li>Stack traces are never split</li> <li>10-20% overlap between chunks for context continuity</li> </ul>"},{"location":"architecture/#stage-4-embed","title":"Stage 4: Embed","text":"<p>Generates vector embeddings:</p> <ul> <li>Model: <code>bge-large-en-v1.5</code> via sentence-transformers</li> <li>Dimensions: 1024</li> <li>Acceleration: MPS on Apple Silicon, CPU elsewhere</li> <li>Load time: ~8s (vs ~30s with Ollama embeddings)</li> </ul>"},{"location":"architecture/#stage-5-index","title":"Stage 5: Index","text":"<p>Stores chunks with metadata in SQLite + sqlite-vec:</p> <ul> <li>WAL mode for concurrent reads</li> <li><code>busy_timeout = 5000ms</code> for multi-process safety</li> <li>Metadata: project, content_type, source_file, char_count, created_at, source</li> </ul>"},{"location":"architecture/#search","title":"Search","text":"<p>BrainLayer uses hybrid search \u2014 combining semantic similarity with keyword matching.</p>"},{"location":"architecture/#semantic-search","title":"Semantic Search","text":"<p>Vector similarity search using sqlite-vec. The query is embedded with the same bge-large model, then matched against stored chunk embeddings.</p>"},{"location":"architecture/#keyword-search","title":"Keyword Search","text":"<p>FTS5 full-text search on chunk content. Catches exact matches that semantic search might miss (specific function names, error codes, etc.).</p>"},{"location":"architecture/#reciprocal-rank-fusion-rrf","title":"Reciprocal Rank Fusion (RRF)","text":"<p>Both result sets are merged using RRF: <code>score = \u03a3 1/(k + rank_i)</code> where k=60. This produces better results than either search alone.</p>"},{"location":"architecture/#storage","title":"Storage","text":"<p>Everything lives in a single SQLite database file:</p> <pre><code>~/.local/share/brainlayer/brainlayer.db  (~1.4GB for 260K+ chunks)\n</code></pre> <p>Key tables:</p> <ul> <li>chunks \u2014 content, embeddings, metadata, enrichment fields</li> <li>chunks_vec \u2014 sqlite-vec virtual table for vector search</li> <li>chunks_fts \u2014 FTS5 virtual table for keyword search</li> <li>session_context \u2014 session metadata (project, branch, plan)</li> <li>session_enrichments \u2014 session-level analysis results</li> <li>prompts \u2014 deduplicated system prompts (SHA-256 keyed)</li> </ul>"},{"location":"architecture/#concurrency","title":"Concurrency","text":"<p>Multiple processes can safely access the database:</p> <ul> <li>WAL mode allows concurrent readers</li> <li>busy_timeout = 5000ms waits for write locks</li> <li>Retry logic with backoff on <code>SQLITE_BUSY</code></li> <li>Thread-local connections in parallel enrichment workers</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>All BrainLayer configuration is via environment variables. No config files needed.</p>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"configuration/#core","title":"Core","text":"Variable Default Description <code>BRAINLAYER_DB</code> <code>~/.local/share/brainlayer/brainlayer.db</code> Database file path. Set to override the default location."},{"location":"configuration/#enrichment","title":"Enrichment","text":"Variable Default Description <code>BRAINLAYER_ENRICH_BACKEND</code> auto-detect LLM backend: <code>mlx</code> or <code>ollama</code>. Auto-detects Apple Silicon \u2192 MLX, else Ollama. <code>BRAINLAYER_ENRICH_MODEL</code> <code>glm-4.7-flash</code> Ollama model name for enrichment <code>BRAINLAYER_MLX_MODEL</code> <code>mlx-community/Qwen2.5-Coder-14B-Instruct-4bit</code> MLX model identifier <code>BRAINLAYER_OLLAMA_URL</code> <code>http://127.0.0.1:11434/api/generate</code> Ollama API endpoint <code>BRAINLAYER_MLX_URL</code> <code>http://127.0.0.1:8080/v1/chat/completions</code> MLX server endpoint <code>BRAINLAYER_STALL_TIMEOUT</code> <code>300</code> Seconds before killing a stuck enrichment chunk <code>BRAINLAYER_HEARTBEAT_INTERVAL</code> <code>25</code> Log progress every N chunks during enrichment"},{"location":"configuration/#privacy","title":"Privacy","text":"Variable Default Description <code>BRAINLAYER_SANITIZE_EXTRA_NAMES</code> (empty) Comma-separated names to redact from indexed content <code>BRAINLAYER_SANITIZE_USE_SPACY</code> <code>true</code> Use spaCy NER for PII detection during indexing"},{"location":"configuration/#database-location","title":"Database Location","text":"<p>The database path is resolved in this order:</p> <ol> <li><code>BRAINLAYER_DB</code> environment variable (highest priority)</li> <li>Legacy path <code>~/.local/share/zikaron/zikaron.db</code> (if it exists \u2014 for migration)</li> <li>Canonical path <code>~/.local/share/brainlayer/brainlayer.db</code> (default for fresh installs)</li> </ol>"},{"location":"configuration/#data-sources","title":"Data Sources","text":"<p>BrainLayer reads from these locations by default:</p> Source Location Claude Code conversations <code>~/.claude/projects/</code> Deduplicated system prompts <code>~/.local/share/brainlayer/prompts/</code> Daemon socket <code>/tmp/brainlayer.sock</code> Enrichment lock <code>/tmp/brainlayer-enrichment.lock</code>"},{"location":"configuration/#scheduled-tasks-macos","title":"Scheduled Tasks (macOS)","text":"<p>BrainLayer includes launchd plist templates for automated operation:</p> Service Schedule Description <code>com.brainlayer.index</code> Every 30 minutes Incremental indexing of new conversations <code>com.brainlayer.enrich</code> Every hour Enrich up to 500 new chunks per run <p>Install with:</p> <pre><code>brainlayer init  # Includes launchd setup option\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>See the CONTRIBUTING.md in the repository root for full contribution guidelines, including:</p> <ul> <li>Development setup</li> <li>Project structure</li> <li>Running tests</li> <li>Pull request process</li> <li>Key patterns and conventions</li> <li>How to add an MCP tool</li> </ul>"},{"location":"data-locations/","title":"BrainLayer Data Locations","text":"<p>Single source of truth for where all data lives, where it moved from, and the archive strategy.</p>"},{"location":"data-locations/#active-data","title":"Active Data","text":"What Path Size Notes Main database <code>~/.local/share/zikaron/zikaron.db</code> ~3.8 GB 268K+ chunks, sqlite-vec + FTS5 knowledge.db <code>~/.local/share/zikaron/knowledge.db</code> symlink Points to zikaron.db Current sessions <code>~/.claude/projects/{encoded-path}/*.jsonl</code> ~805 files Claude Code session transcripts Archived sessions <code>~/.claude-archive/{project-id}/archive-{timestamp}/</code> 1.2 GB Moved by session-archiver"},{"location":"data-locations/#path-resolution","title":"Path Resolution","text":"<p>BrainLayer resolves the database path in this order (see <code>src/brainlayer/paths.py</code>):</p> <ol> <li><code>BRAINLAYER_DB</code> env var \u2014 explicit override</li> <li><code>~/.local/share/zikaron/zikaron.db</code> \u2014 legacy path (if exists, use it)</li> <li><code>~/.local/share/brainlayer/brainlayer.db</code> \u2014 canonical path (for fresh installs)</li> </ol>"},{"location":"data-locations/#why-the-legacy-path","title":"Why the legacy path?","text":"<p>The project was originally called \"Zikaron\" and all data lives at the legacy path. Renaming the 3.8 GB database is risky and unnecessary \u2014 the code resolves it automatically. When users install BrainLayer fresh (no existing data), it uses the canonical path.</p>"},{"location":"data-locations/#session-archiver","title":"Session Archiver","text":"<p>Service: <code>com.brainlayer.session-archiver</code> (launchd, runs daily at 4am)</p>"},{"location":"data-locations/#how-it-works","title":"How it works:","text":"<ol> <li>Scans <code>~/.claude/projects/</code> for all session JSONL files</li> <li>Keeps last 7 days of active sessions per project</li> <li>Moves older sessions to <code>~/.claude-archive/{project-id}/archive-{timestamp}/</code></li> <li>Writes <code>manifest.json</code> per batch (UUIDs, timestamps, sizes)</li> <li>After BrainLayer indexes the archived sessions, the archiver cleans up verified copies</li> </ol>"},{"location":"data-locations/#archive-structure","title":"Archive structure:","text":"<pre><code>~/.claude-archive/\n  my-project/\n    archive-2026-02-09T02-00-05/\n      {uuid}.jsonl           # Archived session transcript\n      {uuid}/                # Optional: subagent files\n      manifest.json          # Batch metadata\n    archive-2026-02-10T02-00-05/\n      ...\n  domica/\n    ...\n  songscript/\n    ...\n</code></pre>"},{"location":"data-locations/#manifest-format","title":"Manifest format:","text":"<pre><code>{\n  \"archivedAt\": \"2026-02-09T02:00:05.123Z\",\n  \"projectId\": \"my-project\",\n  \"originalPath\": \"/Users/username/Gits/my-project\",\n  \"sessions\": [\n    {\n      \"uuid\": \"abc123...\",\n      \"originalMtime\": \"2026-02-07T15:30:00.000Z\",\n      \"size\": 524288,\n      \"hasSubdir\": true,\n      \"firstMessageTimestamp\": \"2026-02-07T15:28:42.123Z\",\n      \"gitBranch\": \"feature/some-branch\"\n    }\n  ],\n  \"metadata\": {\n    \"archiver_version\": \"1.1.0\",\n    \"sessions_kept\": 7,\n    \"total_archived\": 12,\n    \"total_size_bytes\": 6291456\n  }\n}\n</code></pre>"},{"location":"data-locations/#backups-manual","title":"Backups (Manual)","text":"<p>Before any bulk operation, back up the database:</p> <pre><code># WAL-safe copy\nsqlite3 ~/.local/share/brainlayer/brainlayer.db \"VACUUM INTO '/path/to/backup/brainlayer-$(date +%Y%m%d).db'\"\n</code></pre> <p>Store backups in your preferred location (iCloud, external drive, etc.).</p>"},{"location":"data-locations/#historical-data-migrations","title":"Historical: Data Migrations","text":""},{"location":"data-locations/#repo-path-change-jan-feb-2026","title":"Repo path change (Jan-Feb 2026)","text":"<p>Repos moved from <code>~/Desktop/Gits/</code> to <code>~/Gits/</code>. This means: - Old chunks reference <code>~/.claude/projects/-Users-username-Desktop-Gits-{repo}/</code> - New chunks reference <code>~/.claude/projects/-Users-username-Gits-{repo}/</code> - The old JSONL session files at the Desktop paths no longer exist</p>"},{"location":"data-locations/#session-archiver-setup-feb-9-2026","title":"Session archiver setup (Feb 9, 2026)","text":"<p>Before the archiver was set up, old sessions were manually deleted. ~160K chunks reference sessions that no longer exist anywhere. These chunks are still searchable \u2014 they just don't have <code>created_at</code> timestamps.</p>"},{"location":"data-locations/#brainlayer-extraction-feb-19-2026","title":"BrainLayer extraction (Feb 19, 2026)","text":"<p>Extracted to standalone repository. Code moved, data stayed at <code>~/.local/share/zikaron/zikaron.db</code>. <code>paths.py</code> handles the legacy path transparently.</p>"},{"location":"data-locations/#vertex-ai-batch-enrichment-feb-17-18-2026","title":"Vertex AI Batch Enrichment (Feb 17-18, 2026)","text":"<ul> <li>153,825 chunks submitted to Vertex AI batch prediction</li> <li>Results imported Feb 18 at 08:00 (135,865 chunks enriched)</li> <li>Job tracking: <code>scripts/backfill_data/vertex_jobs.json</code></li> <li>Predictions stored in: <code>scripts/backfill_data/predictions/</code></li> </ul>"},{"location":"data-locations/#coverage-stats-as-of-feb-19-2026","title":"Coverage Stats (as of Feb 19, 2026)","text":"Metric Count Percentage Total chunks 268,864 100% Have <code>created_at</code> 107,935 40.1% Missing <code>created_at</code> 160,929 59.9% Enriched 144,146 53.6% Enrichable but not enriched 22,974 8.5% Too small to enrich (&lt;50 chars) 101,744 37.8% <p>The 160K chunks without <code>created_at</code> are from pre-archiver sessions whose JSONL files were deleted. The chunks themselves are fully indexed and searchable \u2014 date filtering just won't apply to them (they'll always be included in unfiltered searches).</p>"},{"location":"embedding-setup/","title":"Embedding Setup for Style Analysis","text":"<p>StyleDistance is the best model for style analysis. It clusters by how you write (formality, emoji, punctuation, phrasing)\u2014not by topic. General embeddings (Qwen3, bge-m3) cluster by content; they're worse for style.</p>"},{"location":"embedding-setup/#usage","title":"Usage","text":"<pre><code>brainlayer analyze-evolution --use-embeddings\n</code></pre>"},{"location":"embedding-setup/#setup","title":"Setup","text":"<ul> <li>Install: <code>sentence-transformers</code> (included in brainlayer deps)</li> <li>First run: Downloads <code>StyleDistance/mstyledistance</code> to <code>~/.cache/huggingface/</code> (~500MB)</li> <li>Auth: None for public models. If rate limited: <code>huggingface-cli login</code> (same as ml-training-pipeline)</li> <li>Env: <code>HF_TOKEN</code> used automatically if set</li> </ul>"},{"location":"embedding-setup/#hardware","title":"Hardware","text":"<ul> <li>M1 Pro 32GB: Runs comfortably</li> <li>Storage: ~500MB model</li> <li>Embedding time: ~30\u201360 min per 100K messages</li> </ul>"},{"location":"enrichment-runbook/","title":"Enrichment Runbook","text":"<p>How to run, monitor, and troubleshoot BrainLayer's enrichment pipeline.</p>"},{"location":"enrichment-runbook/#what-enrichment-does","title":"What Enrichment Does","text":"<p>Every chunk in BrainLayer starts as raw text \u2014 conversation snippets, code, error logs. Enrichment passes each chunk through a local LLM to add 10 structured metadata fields: a summary, topic tags, importance score, intent, key symbols, and more.</p> <p>This metadata powers better search (filter by importance, intent, or tags), the brain graph (cluster by topic), and the dashboard analytics.</p>"},{"location":"enrichment-runbook/#quick-start","title":"Quick Start","text":"<pre><code># Make sure Ollama is running\nollama serve  # if not already running\n\n# Run a batch (50 chunks)\ncd /path/to/brainlayer\nsource .venv/bin/activate\nbrainlayer enrich\n\n# Check progress\nbrainlayer enrich --stats\n</code></pre>"},{"location":"enrichment-runbook/#daily-ongoing-enrichment","title":"Daily / Ongoing Enrichment","text":"<p>The <code>auto-enrich.sh</code> script handles this. Set it up with cron or launchd:</p> <pre><code># Run every 6 hours \u2014 skips if queue is small\n./scripts/auto-enrich.sh --threshold 500 --max-hours 3\n</code></pre> <p>What it does: 1. Checks how many chunks are unenriched 2. Skips if below threshold (default: 500) 3. Alerts via Telegram if queue &gt; 5,000 (you're falling behind) 4. Starts the right backend (Ollama or MLX) 5. Runs enrichment with a time cap 6. Reports results via Telegram</p>"},{"location":"enrichment-runbook/#choosing-a-backend","title":"Choosing a Backend","text":"Ollama MLX Setup <code>ollama pull glm4</code> <code>pip install mlx-lm</code> + download model Speed ~1s/chunk (short content) 21-87% faster Memory ~4GB VRAM ~8GB RAM (14B-4bit model) Parallel Usually 1 worker 2-3 workers work well Env var <code>BRAINLAYER_ENRICH_BACKEND=ollama</code> (default) <code>BRAINLAYER_ENRICH_BACKEND=mlx</code> <p>To switch, just set the env var. Both produce the same 10-field JSON output.</p>"},{"location":"enrichment-runbook/#cloud-backfill-gemini-batch-api","title":"Cloud Backfill (Gemini Batch API)","text":"<p>For the initial bulk run (251K chunks), local LLM would take weeks. Instead, use Gemini 2.5 Flash-Lite Batch API:</p>"},{"location":"enrichment-runbook/#cost","title":"Cost","text":"<ul> <li>~$16 total (251K chunks)</li> <li>Gemini Flash-Lite: $0.075/MTok input, $0.30/MTok output (batch gets 50% discount)</li> </ul>"},{"location":"enrichment-runbook/#how-to-run","title":"How to Run","text":"<pre><code>cd /path/to/brainlayer\nsource .venv/bin/activate\n\n# Set your Gemini API key\nexport GOOGLE_API_KEY=your-key-here\n\n# Run backfill (processes ~100K chunks per batch)\npython3 scripts/cloud_backfill.py\n\n# Resume if interrupted\npython3 scripts/cloud_backfill.py --resume\n</code></pre> <p>The script: 1. Exports unenriched chunks from SQLite 2. Uploads to Gemini Batch API in batches of 100K 3. Polls for completion (~30 min per batch) 4. Downloads results and imports back to SQLite 5. Logs token usage and cost</p>"},{"location":"enrichment-runbook/#safety","title":"Safety","text":"<ul> <li>Only targets <code>WHERE enriched_at IS NULL</code> \u2014 never overwrites existing enrichments</li> <li>Validates a 100-chunk sample before full run</li> <li>Generates cost log for budget tracking</li> </ul>"},{"location":"enrichment-runbook/#10-field-schema","title":"10-Field Schema","text":"<p>Each enriched chunk gets these fields:</p> <pre><code>{\n  \"summary\": \"Debugging Telegram bot message drops under high load\",\n  \"tags\": \"telegram, debugging, performance, grammy\",\n  \"importance\": 7,\n  \"intent\": \"debugging\",\n  \"primary_symbols\": \"TelegramBot, handleMessage, grammy\",\n  \"resolved_query\": \"Why does the Telegram bot drop messages during peak hours?\",\n  \"epistemic_level\": \"substantiated\",\n  \"version_scope\": \"grammy 1.32, Railway deployment\",\n  \"debt_impact\": \"resolution\",\n  \"external_deps\": \"grammy, Railway\"\n}\n</code></pre>"},{"location":"enrichment-runbook/#field-details","title":"Field Details","text":"<ul> <li>importance (1-10): Directory listings get a 2, architectural decisions get an 8-9</li> <li>intent: One of <code>debugging</code>, <code>designing</code>, <code>implementing</code>, <code>configuring</code>, <code>discussing</code>, <code>deciding</code>, <code>reviewing</code></li> <li>epistemic_level: <code>hypothesis</code> (guessing), <code>substantiated</code> (evidence-backed), <code>validated</code> (tested/confirmed)</li> <li>debt_impact: <code>introduction</code> (new tech debt), <code>resolution</code> (fixing debt), <code>none</code> (neutral)</li> </ul>"},{"location":"enrichment-runbook/#troubleshooting","title":"Troubleshooting","text":""},{"location":"enrichment-runbook/#enrichment-hangs-or-is-very-slow","title":"Enrichment hangs or is very slow","text":"<ol> <li>Check Ollama thinking mode: <code>\"think\": false</code> must be set in the API call. Without it, GLM-4.7 adds 350+ reasoning tokens per chunk (20s vs 1s).</li> <li>Check DB locks: <code>lsof ~/.local/share/brainlayer/brainlayer.db</code> \u2014 if daemon + MCP + enrichment are all running, the <code>busy_timeout</code> should handle it, but check the logs.</li> <li>Stale lock file: <code>rm /tmp/brainlayer-enrichment.lock</code> if enrichment died and left a lock.</li> </ol>"},{"location":"enrichment-runbook/#db-locked-errors","title":"DB locked errors","text":"<p>The pipeline has <code>busy_timeout = 5000ms</code> + 3-attempt retry. If you still see lock errors: 1. Check who has the DB open: <code>lsof ~/.local/share/brainlayer/brainlayer.db</code> 2. Restart the daemon: <code>brainlayer serve --http 8787</code> (it reconnects cleanly) 3. Make sure only one enrichment process runs at a time</p>"},{"location":"enrichment-runbook/#enrichment-produces-bad-json","title":"Enrichment produces bad JSON","text":"<p>The LLM sometimes returns malformed JSON. The parser tries to extract JSON from the response using brace-matching. If it fails, the chunk is skipped (counted as \"failed\" in batch stats). Failed chunks can be retried on the next run.</p>"},{"location":"enrichment-runbook/#backup-and-recovery","title":"Backup and Recovery","text":"<p>Before any bulk operation, back up the database: <pre><code># WAL-safe copy using SQLite VACUUM INTO\nmkdir -p ~/.local/share/brainlayer/backups\nsqlite3 ~/.local/share/brainlayer/brainlayer.db \\\n  \"VACUUM INTO '$HOME/.local/share/brainlayer/backups/brainlayer-$(date +%Y%m%d-%H%M).db'\"\n</code></pre></p> <p>To restore from backup: <pre><code># Stop daemon and any enrichment\npkill -f \"brainlayer serve\" || true\nrm /tmp/brainlayer-enrichment.lock 2&gt;/dev/null || true\n\n# Copy backup over current DB\ncp ~/.local/share/brainlayer/backups/brainlayer-YYYYMMDD-HHMM.db \\\n   ~/.local/share/brainlayer/brainlayer.db\n\n# Restart daemon\nbrainlayer serve --http 8787\n</code></pre></p>"},{"location":"enrichment-runbook/#queue-keeps-growing","title":"Queue keeps growing","text":"<p>New Claude Code sessions add chunks constantly. If the queue grows faster than enrichment processes it: 1. Increase batch frequency (cron every 4 hours instead of 6) 2. Use MLX + parallel workers: <code>--parallel 3</code> with MLX backend 3. Run a cloud backfill to catch up</p>"},{"location":"enrichment/","title":"Enrichment","text":"<p>BrainLayer enriches indexed chunks with structured metadata using a local LLM. Think of it as a librarian cataloging every conversation snippet.</p>"},{"location":"enrichment/#chunk-enrichment","title":"Chunk Enrichment","text":"<p>Each chunk gets 10 metadata fields:</p> Field Description Example <code>summary</code> 1-2 sentence gist \"Debugging Telegram bot message drops under load\" <code>tags</code> Topic tags (comma-separated) \"telegram, debugging, performance\" <code>importance</code> Relevance score 1-10 8 (architectural decision) vs 2 (directory listing) <code>intent</code> What was happening <code>debugging</code>, <code>designing</code>, <code>implementing</code>, <code>configuring</code>, <code>deciding</code>, <code>reviewing</code> <code>primary_symbols</code> Key code entities \"TelegramBot, handleMessage, grammy\" <code>resolved_query</code> Question this answers (HyDE-style) \"How does the Telegram bot handle rate limiting?\" <code>epistemic_level</code> How proven is this <code>hypothesis</code>, <code>substantiated</code>, <code>validated</code> <code>version_scope</code> System state context \"grammy 1.32, Node 22\" <code>debt_impact</code> Technical debt signal <code>introduction</code>, <code>resolution</code>, <code>none</code> <code>external_deps</code> Libraries/APIs mentioned \"grammy, Supabase, Railway\""},{"location":"enrichment/#running-enrichment","title":"Running Enrichment","text":"<pre><code># Basic (50 chunks at a time)\nbrainlayer enrich\n\n# Larger batches\nbrainlayer enrich --batch-size=100\n\n# Process up to 5000 chunks\nbrainlayer enrich --max=5000\n\n# With parallel workers\nbrainlayer enrich --parallel=3\n</code></pre>"},{"location":"enrichment/#source-aware-thresholds","title":"Source-Aware Thresholds","text":"<p>Not all chunks are worth enriching. BrainLayer automatically skips chunks that are too short:</p> Source Minimum Length Reason Claude Code 50 characters Code context needs substance WhatsApp / Telegram 15 characters Short messages can still be meaningful <p>Skipped chunks are tagged as <code>skipped:too_short</code> and excluded from enrichment stats.</p>"},{"location":"enrichment/#session-enrichment","title":"Session Enrichment","text":"<p>Session-level analysis extracts structured insights from entire conversations:</p> <pre><code>brainlayer enrich-sessions\nbrainlayer enrich-sessions --project my-project --since 2026-01-01\nbrainlayer enrich-sessions --stats   # Show progress\n</code></pre> <p>Session enrichment extracts:</p> <ul> <li>Summary \u2014 what the session was about</li> <li>Decisions \u2014 architectural and implementation choices made</li> <li>Corrections \u2014 mistakes caught and fixed</li> <li>Learnings \u2014 new knowledge gained</li> <li>Patterns \u2014 recurring approaches identified</li> <li>Quality scores \u2014 code quality, communication quality</li> </ul>"},{"location":"enrichment/#llm-backends","title":"LLM Backends","text":"<p>Two local backends are supported:</p> Backend Best for Speed How to start MLX Apple Silicon (M1/M2/M3) 21-87% faster <code>python3 -m mlx_lm.server --model mlx-community/Qwen2.5-Coder-14B-Instruct-4bit --port 8080</code> Ollama Any platform ~1s/chunk (short), ~13s (long) <code>ollama serve</code> + <code>ollama pull glm4</code> <p>Backend is auto-detected: Apple Silicon defaults to MLX, everything else to Ollama. Override with:</p> <pre><code>BRAINLAYER_ENRICH_BACKEND=mlx brainlayer enrich\nBRAINLAYER_ENRICH_BACKEND=ollama brainlayer enrich\n</code></pre>"},{"location":"enrichment/#performance-tips","title":"Performance Tips","text":"<ul> <li>Set <code>\"think\": false</code> in Ollama API calls \u2014 GLM-4.7 defaults to thinking mode, adding 350+ tokens and 20s delay for no benefit</li> <li>Use <code>PYTHONUNBUFFERED=1</code> for log visibility in background processes</li> <li>MLX parallel workers: each gets its own DB connection (thread-local)</li> </ul>"},{"location":"enrichment/#stall-detection","title":"Stall Detection","text":"<p>If a chunk takes too long (default: 5 minutes), it's automatically killed and skipped:</p> <pre><code>BRAINLAYER_STALL_TIMEOUT=300 brainlayer enrich  # 5 min default\n</code></pre> <p>Progress is logged every N chunks:</p> <pre><code>BRAINLAYER_HEARTBEAT_INTERVAL=25 brainlayer enrich  # Log every 25 chunks\n</code></pre>"},{"location":"local-models-guide/","title":"Local LLM Models Guide","text":"<p>Last updated: 2026-01-26 Hardware: MacBook Pro M1 Pro, 32GB RAM</p>"},{"location":"local-models-guide/#quick-reference-models-for-your-mac-32gb-ram","title":"Quick Reference: Models for Your Mac (32GB RAM)","text":"Model Size Speed Best For <code>qwen3-coder</code> 19GB Fast (MoE, 3.3B active) Agentic coding, best overall <code>qwen2.5-coder:32b</code> 20GB ~15-20 tok/s High quality code gen <code>qwen2.5-coder:14b</code> 9GB ~50 tok/s Fast coding tasks <code>devstral</code> 14GB ~30 tok/s Codebase navigation, file ops"},{"location":"local-models-guide/#model-details","title":"Model Details","text":""},{"location":"local-models-guide/#qwen3-coder-recommended","title":"qwen3-coder (RECOMMENDED)","text":"<pre><code>ollama pull qwen3-coder\n</code></pre> <ul> <li>Architecture: MoE (Mixture of Experts)</li> <li>Parameters: 30.5B total, only 3.3B activated</li> <li>Size: ~19GB</li> <li>RAM needed: 32GB (perfect for your Mac)</li> <li>Why it's good: Smarter than dense models, runs fast due to MoE</li> <li>Best for: Agentic coding, multi-step tasks, code analysis</li> </ul>"},{"location":"local-models-guide/#qwen25-coder32b","title":"qwen2.5-coder:32b","text":"<pre><code>ollama pull qwen2.5-coder:32b\n</code></pre> <ul> <li>Architecture: Dense transformer</li> <li>Parameters: 32.5B</li> <li>Size: ~20GB</li> <li>RAM needed: 24-32GB</li> <li>Benchmark: Best open-source on EvalPlus, LiveCodeBench, BigCodeBench</li> <li>Best for: High quality code generation, complex problems</li> </ul>"},{"location":"local-models-guide/#qwen25-coder14b","title":"qwen2.5-coder:14b","text":"<pre><code>ollama pull qwen2.5-coder:14b\n</code></pre> <ul> <li>Parameters: 14B</li> <li>Size: ~9GB</li> <li>Speed: ~50+ tok/s on M1 Pro</li> <li>Best for: Fast iterations, simpler tasks, when speed matters</li> </ul>"},{"location":"local-models-guide/#devstral","title":"devstral","text":"<pre><code>ollama pull devstral\n</code></pre> <ul> <li>By: Mistral AI</li> <li>Size: ~14GB</li> <li>Specialty: File system operations, code navigation, large codebases</li> <li>Best for: Codebase exploration, documentation audits, multi-file edits</li> </ul>"},{"location":"local-models-guide/#benchmark-comparison-swe-bench-verified","title":"Benchmark Comparison (SWE-Bench Verified)","text":"Model Score Type Claude 4 Sonnet 72.7% Closed-source Claude 4 Opus 72.5% Closed-source Qwen3-Coder 69.6% Open-source DeepSeek-V3 ~68% Open-source <p>Takeaway: Open-source is only ~3% behind Claude now.</p>"},{"location":"local-models-guide/#models-that-wont-fit-32gb-ram","title":"Models That WON'T Fit (32GB RAM)","text":"Model Size Issue qwen3-coder:480b 163-368GB Way too big deepseek-v3 400GB+ Won't fit Any 70B+ dense model 40GB+ Will swap, very slow"},{"location":"local-models-guide/#usage-patterns","title":"Usage Patterns","text":""},{"location":"local-models-guide/#for-code-researchdocumentation","title":"For Code Research/Documentation","text":"<pre><code>ollama run qwen3-coder\n# or\nollama run devstral\n</code></pre>"},{"location":"local-models-guide/#for-fast-code-generation","title":"For Fast Code Generation","text":"<pre><code>ollama run qwen2.5-coder:14b\n</code></pre>"},{"location":"local-models-guide/#for-highest-quality-slower","title":"For Highest Quality (Slower)","text":"<pre><code>ollama run qwen2.5-coder:32b\n</code></pre>"},{"location":"local-models-guide/#running-with-context","title":"Running with Context","text":""},{"location":"local-models-guide/#pipe-a-file","title":"Pipe a file","text":"<pre><code>cat README.md | ollama run qwen3-coder \"Analyze this README and suggest improvements\"\n</code></pre>"},{"location":"local-models-guide/#interactive-session-with-system-prompt","title":"Interactive session with system prompt","text":"<pre><code>ollama run qwen3-coder --system \"You are a senior software architect auditing a codebase.\"\n</code></pre>"},{"location":"local-models-guide/#free-cloud-alternatives-when-local-wont-cut-it","title":"Free Cloud Alternatives (When Local Won't Cut It)","text":"Service Models Limit Gemini CLI Gemini 2.5 Pro 1000 req/day Groq Llama 3.3 70B Very fast, free tier Together.ai All major models $25 free credit OpenRouter Everything Pay-per-use"},{"location":"local-models-guide/#check-whats-installed","title":"Check What's Installed","text":"<pre><code>ollama list\n</code></pre>"},{"location":"local-models-guide/#check-available-space","title":"Check Available Space","text":"<pre><code>df -h /\ndu -sh ~/.ollama/models/\n</code></pre>"},{"location":"local-models-guide/#sources","title":"Sources","text":"<ul> <li>Ollama qwen2.5-coder</li> <li>Ollama qwen3-coder</li> <li>Best Open Source LLMs for Coding 2026</li> <li>Qwen AI Coding Review</li> </ul>"},{"location":"mcp-config/","title":"MCP Configuration for BrainLayer","text":"<p>Add this to <code>~/.claude/settings.json</code> under <code>mcpServers</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"brainlayer\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"brainlayer.mcp\"],\n      \"cwd\": \"/path/to/brainlayer\"\n    }\n  }\n}\n</code></pre> <p>Or if you have brainlayer installed globally:</p> <pre><code>{\n  \"mcpServers\": {\n    \"brainlayer\": {\n      \"command\": \"brainlayer-mcp\",\n      \"args\": []\n    }\n  }\n}\n</code></pre>"},{"location":"mcp-config/#testing-the-mcp-server","title":"Testing the MCP Server","text":"<ol> <li> <p>Start the server manually to test:    <pre><code>cd /path/to/brainlayer\nsource .venv/bin/activate\npython -m brainlayer.mcp\n</code></pre></p> </li> <li> <p>In Claude Code, the tools should appear:</p> </li> <li><code>brainlayer_search</code> - Search past conversations</li> <li><code>brainlayer_stats</code> - Knowledge base statistics</li> <li><code>brainlayer_list_projects</code> - List indexed projects</li> </ol>"},{"location":"mcp-tools/","title":"MCP Tools Reference","text":"<p>BrainLayer exposes 14 MCP tools, organized into two categories.</p>"},{"location":"mcp-tools/#intelligence-layer","title":"Intelligence Layer","text":"<p>These tools go beyond raw search \u2014 they understand intent and context.</p>"},{"location":"mcp-tools/#brainlayer_think","title":"brainlayer_think","text":"<p>Given your current task context, retrieves relevant past decisions, patterns, and bugs. Groups results by intent category.</p> Parameter Type Required Description <code>context</code> string Yes What you're currently working on <code>project</code> string No Filter to a specific project <code>max_results</code> integer No Maximum results (default: 10) <p>Returns: Markdown with results grouped by category (decisions, bugs, patterns, implementations).</p> <p>Use when: Starting a new task, making architectural decisions, or debugging.</p>"},{"location":"mcp-tools/#brainlayer_recall","title":"brainlayer_recall","text":"<p>File-based or topic-based recall. \"What happened with this file?\" or \"What do I know about deployment?\"</p> Parameter Type Required Description <code>file_path</code> string No* File to recall history for <code>topic</code> string No* Topic to recall knowledge about <code>project</code> string No Filter to a specific project <code>max_results</code> integer No Maximum results (default: 10) <p>*At least one of <code>file_path</code> or <code>topic</code> is required.</p> <p>Returns: Markdown with relevant chunks, session context, and file interactions.</p> <p>Use when: Investigating a file's history, or gathering knowledge about a specific topic.</p>"},{"location":"mcp-tools/#brainlayer_current_context","title":"brainlayer_current_context","text":"<p>Lightweight \u2014 what projects, branches, files, and active plan were you working on recently? No embedding needed.</p> Parameter Type Required Description <code>hours</code> integer No How many hours back to look (default: 24) <p>Returns: Structured summary of recent activity.</p> <p>Use when: Starting a conversation, to understand current state.</p>"},{"location":"mcp-tools/#brainlayer_sessions","title":"brainlayer_sessions","text":"<p>Browse recent sessions by project and date range.</p> Parameter Type Required Description <code>project</code> string No Filter to a specific project <code>days</code> integer No How many days back (default: 7, max: 365) <code>limit</code> integer No Maximum sessions (default: 20, max: 100) <p>Returns: Markdown list of sessions with ID, project, branch, plan, and start time.</p>"},{"location":"mcp-tools/#brainlayer_session_summary","title":"brainlayer_session_summary","text":"<p>Session-level analysis: decisions made, corrections, learnings, quality scores.</p> Parameter Type Required Description <code>session_id</code> string Yes Session ID to summarize <p>Returns: Markdown with decisions, corrections, learnings, patterns, and quality metrics.</p> <p>Use when: Reviewing what happened in a specific session.</p>"},{"location":"mcp-tools/#brainlayer_store","title":"brainlayer_store","text":"<p>Persist a memory (idea, decision, learning, mistake, etc.) for future retrieval.</p> Parameter Type Required Description <code>content</code> string Yes The memory content to store <code>type</code> string Yes Memory type: idea, mistake, decision, learning, todo, bookmark, note, journal <code>project</code> string No Project to scope the memory <code>tags</code> array[string] No Tags for categorization <code>importance</code> integer No Importance score 1-10 <p>Returns: Chunk ID and related existing memories.</p> <p>Use when: An agent discovers something worth remembering for future sessions.</p>"},{"location":"mcp-tools/#search-context","title":"Search &amp; Context","text":"<p>Core search and retrieval tools.</p>"},{"location":"mcp-tools/#brainlayer_search","title":"brainlayer_search","text":"<p>Hybrid semantic + keyword search with filters.</p> Parameter Type Required Description <code>query</code> string Yes Search query <code>project</code> string No Filter by project <code>content_type</code> string No Filter: <code>ai_code</code>, <code>stack_trace</code>, <code>user_message</code>, <code>assistant_text</code>, <code>file_read</code>, <code>git_diff</code> <code>num_results</code> integer No Results to return (default: 5, max: 100) <code>source</code> string No Filter: <code>claude_code</code>, <code>whatsapp</code>, <code>youtube</code>, <code>all</code> <code>tag</code> string No Filter by enrichment tag <code>intent</code> string No Filter: <code>debugging</code>, <code>designing</code>, <code>configuring</code>, <code>discussing</code>, <code>deciding</code>, <code>implementing</code>, <code>reviewing</code> <code>importance_min</code> integer No Minimum importance score (1-10) <p>Returns: Matched chunks with content, metadata, similarity scores.</p>"},{"location":"mcp-tools/#brainlayer_context","title":"brainlayer_context","text":"<p>Get surrounding conversation chunks for a search result.</p> Parameter Type Required Description <code>chunk_id</code> string Yes Chunk ID from a search result <code>before</code> integer No Chunks before (default: 3, max: 50) <code>after</code> integer No Chunks after (default: 3, max: 50) <p>Returns: The target chunk plus surrounding conversation context.</p>"},{"location":"mcp-tools/#brainlayer_file_timeline","title":"brainlayer_file_timeline","text":"<p>Full interaction history of a file across all sessions.</p> Parameter Type Required Description <code>file_path</code> string Yes File path to look up <code>project</code> string No Filter by project <code>limit</code> integer No Maximum entries (default: 50) <p>Returns: Chronological timeline of all interactions with the file.</p>"},{"location":"mcp-tools/#brainlayer_operations","title":"brainlayer_operations","text":"<p>Logical operation groups \u2014 read/edit/test cycles within a session.</p> Parameter Type Required Description <code>session_id</code> string Yes Session to analyze <p>Returns: Grouped operations showing the read\u2192edit\u2192test workflow patterns.</p>"},{"location":"mcp-tools/#brainlayer_regression","title":"brainlayer_regression","text":"<p>What changed since a file last worked? Diff-based regression analysis.</p> Parameter Type Required Description <code>file_path</code> string Yes File to analyze <code>project</code> string No Filter by project <p>Returns: Changes since the file's last known-good state.</p>"},{"location":"mcp-tools/#brainlayer_plan_links","title":"brainlayer_plan_links","text":"<p>Connect sessions to implementation plans and phases.</p> Parameter Type Required Description <code>plan_name</code> string No Filter by plan name <code>session_id</code> string No Filter by session <code>project</code> string No Filter by project <p>Returns: Session-to-plan linkage with phase information.</p>"},{"location":"mcp-tools/#brainlayer_stats","title":"brainlayer_stats","text":"<p>Knowledge base statistics.</p> <p>Returns: Total chunks, projects, content types, enrichment progress, and source breakdown.</p>"},{"location":"mcp-tools/#brainlayer_list_projects","title":"brainlayer_list_projects","text":"<p>List all indexed projects.</p> <p>Returns: Project names with chunk counts.</p>"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"quickstart/#installation","title":"Installation","text":"<pre><code>pip install brainlayer\n</code></pre>"},{"location":"quickstart/#optional-extras","title":"Optional extras","text":"<pre><code>pip install \"brainlayer[brain]\"     # Brain graph visualization (HDBSCAN + UMAP)\npip install \"brainlayer[cloud]\"     # Cloud backfill (Gemini Batch API)\npip install \"brainlayer[youtube]\"   # YouTube transcript indexing\npip install \"brainlayer[ast]\"       # AST-aware code chunking (tree-sitter)\n</code></pre>"},{"location":"quickstart/#setup","title":"Setup","text":"<p>Run the interactive wizard:</p> <pre><code>brainlayer init\n</code></pre> <p>This will:</p> <ol> <li>Check for Claude Code conversations in <code>~/.claude/projects/</code></li> <li>Detect your hardware (Apple Silicon \u2192 MLX, otherwise Ollama)</li> <li>Configure your LLM backend for enrichment</li> <li>Create the database at <code>~/.local/share/brainlayer/brainlayer.db</code></li> </ol>"},{"location":"quickstart/#index-your-conversations","title":"Index Your Conversations","text":"<pre><code>brainlayer index\n</code></pre> <p>This parses your Claude Code conversations (JSONL files), classifies content, chunks it with sentence boundaries, generates embeddings (bge-large-en-v1.5), and stores everything in the SQLite database.</p>"},{"location":"quickstart/#connect-to-your-editor","title":"Connect to Your Editor","text":""},{"location":"quickstart/#claude-code","title":"Claude Code","text":"<p>Add to <code>~/.claude.json</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"brainlayer\": {\n      \"command\": \"brainlayer-mcp\"\n    }\n  }\n}\n</code></pre>"},{"location":"quickstart/#cursor","title":"Cursor","text":"<p>Add in Cursor's MCP settings:</p> <pre><code>{\n  \"mcpServers\": {\n    \"brainlayer\": {\n      \"command\": \"brainlayer-mcp\"\n    }\n  }\n}\n</code></pre>"},{"location":"quickstart/#zed","title":"Zed","text":"<p>Add to <code>settings.json</code>:</p> <pre><code>{\n  \"context_servers\": {\n    \"brainlayer\": {\n      \"command\": { \"path\": \"brainlayer-mcp\" }\n    }\n  }\n}\n</code></pre>"},{"location":"quickstart/#vs-code","title":"VS Code","text":"<p>Add to <code>.vscode/mcp.json</code>:</p> <pre><code>{\n  \"servers\": {\n    \"brainlayer\": {\n      \"command\": \"brainlayer-mcp\"\n    }\n  }\n}\n</code></pre>"},{"location":"quickstart/#enrich-optional","title":"Enrich (Optional)","text":"<p>Add structured metadata to your indexed content using a local LLM:</p> <pre><code>brainlayer enrich\n</code></pre> <p>This adds summary, tags, importance scores, intent classification, and more to each chunk. See Enrichment for details.</p>"},{"location":"quickstart/#verify","title":"Verify","text":"<pre><code>brainlayer stats              # Check your knowledge base\nbrainlayer search \"auth\"      # Test a search\n</code></pre>"},{"location":"quickstart/#cli-reference","title":"CLI Reference","text":"<pre><code>brainlayer init               # Interactive setup wizard\nbrainlayer index              # Index new conversations\nbrainlayer search \"query\"     # Semantic + keyword search\nbrainlayer enrich             # Run LLM enrichment on new chunks\nbrainlayer enrich-sessions    # Session-level analysis\nbrainlayer stats              # Database statistics\nbrainlayer brain-export       # Generate brain graph JSON\nbrainlayer export-obsidian    # Export to Obsidian vault\nbrainlayer dashboard          # Interactive TUI dashboard\n</code></pre>"},{"location":"showcase-claude-collab-discovery/","title":"Showcase: BrainLayer Finds Claude Collaboration Protocol","text":"<p>Real example from 2026-02-02 demonstrating semantic search capabilities</p>"},{"location":"showcase-claude-collab-discovery/#the-query","title":"The Query","text":"<p>User had 3 Claude sessions running in parallel for a monorepo consolidation. Needed to find the \"collaborative Claudes\" pattern from past conversations.</p> <pre><code>brainlayer search \"collaborative claudes parallel sessions coordination\"\n</code></pre>"},{"location":"showcase-claude-collab-discovery/#what-brainlayer-found","title":"What BrainLayer Found","text":"<p>15 results in ~2 seconds, including the exact file and pattern:</p> <pre><code>\u2500\u2500\u2500 Result 1 \u2500\u2500\u2500 (score: 0.715)\nclaude-golem \u00b7 assistant_text\n\nCommand: cat &gt;&gt; ~/.claude/claude-collab.md &lt;&lt; 'EOF'\n## From: golem-session @ 2026-01-26 02:00\n**Closing collab**\nUpdated MP-128 with the agreed split. Proceeding independently.\n...\n\n\u2500\u2500\u2500 Result 2 \u2500\u2500\u2500 (score: 0.700)\nclaude-golem \u00b7 assistant_text\n\n## Inter-Claude Collaboration\n\n### The Problem\nYou had two Claude sessions running:\n1. **This session (golem)** - working on brainlayer active learning (MP-128)\n2. **Another session (farther-steps)** - working on the farther-steps queuing system\n\nBoth features needed to integrate, but Claude sessions are isolated...\n\n### The Solution: Shared File as Async Chat\nWe created `~/.claude/claude-collab.md` as a communication channel...\n</code></pre>"},{"location":"showcase-claude-collab-discovery/#the-discovery","title":"The Discovery","text":"<p>BrainLayer surfaced <code>~/.claude/claude-collab.md</code> - a shared async communication protocol between Claude sessions:</p> <pre><code># Claude Session Collaboration\n\n&gt; Async communication between Claude sessions. Append your responses, don't overwrite.\n&gt; Check for updates: `cat ~/.claude/claude-collab.md`\n&gt; After reading, append your response at the bottom.\n\n---\n\n## From: golem-session @ 2026-01-26 01:35\n\n**Topic:** Integrating BrainLayer Active Learning with Farther-Steps\n\nHey farther-steps Claude! I'm working on MP-128...\n\n---\n\n## From: farther-steps-session @ 2026-01-26 11:45\n\n**Re:** Integrating BrainLayer Active Learning with Farther-Steps\n\nHey! Just finished documenting farther-steps...\n</code></pre>"},{"location":"showcase-claude-collab-discovery/#why-this-matters","title":"Why This Matters","text":"<ol> <li>Knowledge wasn't lost - A pattern created weeks ago was instantly retrievable</li> <li>Semantic search works - Query didn't need exact terms like \"claude-collab.md\"</li> <li>Conversation context preserved - Found the explanation AND the actual protocol</li> <li>Enabled reuse - The same pattern was immediately applied to the new 3-Claude coordination</li> </ol>"},{"location":"showcase-claude-collab-discovery/#the-pattern","title":"The Pattern","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Claude A   \u2502     \u2502  Claude B   \u2502     \u2502  Claude C   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                   \u2502                   \u2502\n       \u25bc                   \u25bc                   \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502         ~/.claude/claude-collab.md            \u2502\n   \u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500    \u2502\n   \u2502  - Each session APPENDS (never overwrites)    \u2502\n   \u2502  - Timestamp and identify your session        \u2502\n   \u2502  - Ask questions, propose designs             \u2502\n   \u2502  - Close collab when done                     \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"showcase-claude-collab-discovery/#conclusion","title":"Conclusion","text":"<p>This is exactly what BrainLayer was built for: turning months of Claude Code conversations into a searchable knowledge base. Without it, this pattern would have been lost in the thousands of conversation turns.</p> <p>Documented by the third Claude session during monorepo consolidation project</p>"},{"location":"archive/chat-based-analysis-design/","title":"Chat-Based Analysis Design","text":""},{"location":"archive/chat-based-analysis-design/#overview","title":"Overview","text":"<p>Extend the longitudinal analysis to group by chat first, then optionally by time. Add relationship tags so the model understands who you're speaking with, not just when and what.</p>"},{"location":"archive/chat-based-analysis-design/#1-data-what-we-have","title":"1. Data: What We Have","text":""},{"location":"archive/chat-based-analysis-design/#whatsapp","title":"WhatsApp","text":"<ul> <li>Per chat: <code>ZCONTACTJID</code> (e.g. <code>972501234567@s.whatsapp.net</code>)</li> <li>Display name: <code>ZPARTNERNAME</code> from <code>ZWACHATSESSION</code> (e.g. \"Mom\", \"Dad\", \"Friend A\")</li> <li>Join: Messages have <code>ZCHATSESSION</code> \u2192 links to <code>ZWACHATSESSION</code></li> </ul>"},{"location":"archive/chat-based-analysis-design/#claude","title":"Claude","text":"<ul> <li>Per conversation: <code>conv.name</code> (e.g. \"Instagram comment draft\")</li> <li>No contact JID; we have conversation title</li> </ul>"},{"location":"archive/chat-based-analysis-design/#gemini-google-takeout","title":"Gemini (Google Takeout)","text":"<ul> <li>TBD \u2014 need to inspect export format</li> </ul>"},{"location":"archive/chat-based-analysis-design/#2-changes-to-extraction","title":"2. Changes to Extraction","text":""},{"location":"archive/chat-based-analysis-design/#21-unifiedmessage-schema-add-fields","title":"2.1 UnifiedMessage schema (add fields)","text":"<pre><code>@dataclass\nclass UnifiedMessage:\n    timestamp: datetime\n    source: str\n    language: str\n    text: str\n    is_from_user: bool\n    context: Optional[str]       # existing: conversation name\n    chat_id: Optional[str]       # NEW: stable ID (JID for WA, conv_id for Claude)\n    contact_name: Optional[str]  # NEW: \"Mom\", \"Dad\", \"Instagram draft\"\n    # tag filled later from user config\n</code></pre>"},{"location":"archive/chat-based-analysis-design/#22-whatsapp-join-with-zwachatsession","title":"2.2 WhatsApp: join with ZWACHATSESSION","text":"<ul> <li>Join <code>ZWAMESSAGE</code> \u2194 <code>ZWACHATSESSION</code> on <code>ZCHATSESSION</code></li> <li>Populate <code>chat_id</code> = <code>ZCONTACTJID</code>, <code>contact_name</code> = <code>ZPARTNERNAME</code></li> </ul>"},{"location":"archive/chat-based-analysis-design/#23-fallback-for-unknown-contacts","title":"2.3 Fallback for unknown contacts","text":"<p>If <code>ZPARTNERNAME</code> is a phone number or empty: - Option A: Use as-is (<code>+1 (555) 012-3456</code>) - Option B: User provides a mapping file or searches for a unique sentence to identify the chat</p>"},{"location":"archive/chat-based-analysis-design/#3-user-tagging-relationship-labels","title":"3. User Tagging (Relationship Labels)","text":""},{"location":"archive/chat-based-analysis-design/#config-file-configbrainlayerchat-tagsyaml-or-similar","title":"Config file: <code>~/.config/brainlayer/chat-tags.yaml</code> (or similar)","text":"<pre><code># Map contact_name or chat_id to relationship tag\ntags:\n  - contact: \"Mom\"\n    tag: family\n  - contact: \"Dad\"\n    tag: family\n  - contact: \"Friend A\"\n    tag: friends\n  - contact: \"Jane Smith\"\n    tag: co-workers\n  # Or by JID if name is ambiguous\n  - jid: \"972501234567@s.whatsapp.net\"\n    tag: family\n</code></pre>"},{"location":"archive/chat-based-analysis-design/#workflow-for-user","title":"Workflow for user","text":"<ol> <li>We generate <code>chats.csv</code>: <code>chat_id, contact_name, message_count</code></li> <li>User adds tags (manually or via script)</li> <li>Re-run analysis with tags</li> </ol>"},{"location":"archive/chat-based-analysis-design/#search-for-a-sentence-fallback","title":"\"Search for a sentence\" fallback","text":"<ul> <li>For chats with no clear name: user searches their WhatsApp for a unique phrase from that chat</li> <li>We could output: \"Chat 972501234567 has 500 msgs, contact_name: +972 50-123-4567. Search for a unique message to identify.\"</li> <li>User finds it, tells us \"that's John from work\" \u2192 we add to tags</li> </ul>"},{"location":"archive/chat-based-analysis-design/#4-batching-strategy","title":"4. Batching Strategy","text":""},{"location":"archive/chat-based-analysis-design/#option-a-chat-first-then-time-within-chat","title":"Option A: Chat-first, then time within chat","text":"<pre><code>Chat: Mom (family)         \u2192 2024-H1, 2024-H2, 2025-H1, ...\nChat: Dad (family)         \u2192 2024-H1, 2024-H2, ...\nChat: Jane (co-workers)    \u2192 2024-H1, 2024-H2, ...\n</code></pre>"},{"location":"archive/chat-based-analysis-design/#option-b-tag-first-then-time","title":"Option B: Tag-first, then time","text":"<pre><code>Tag: family    \u2192 2024-H1, 2024-H2, 2025-H1, ... (all family chats combined)\nTag: friends   \u2192 2024-H1, 2024-H2, ...\nTag: co-workers \u2192 2024-H1, 2024-H2, ...\n</code></pre>"},{"location":"archive/chat-based-analysis-design/#option-c-both-recommended","title":"Option C: Both (recommended)","text":"<ul> <li>Primary: Tag + time (e.g. \"family, 2024-H2\")</li> <li>Secondary: Per-chat analysis for top N chats (optional)</li> </ul>"},{"location":"archive/chat-based-analysis-design/#5-prompt-enrichment","title":"5. Prompt Enrichment","text":"<p>Current:</p> <p>\"Analyze 250 messages from 2024-H2. Language: HEBREW.\"</p> <p>New:</p> <p>\"Analyze 250 messages from 2024-H2. - Relationship context: ~60% family (Mom, Dad), ~40% friends (Friend A, Friend B) - Contact names in messages: Mom, Dad, Friend A, Friend B - Language: HEBREW. Consider how they may adapt tone by relationship.\"</p>"},{"location":"archive/chat-based-analysis-design/#6-implementation-order","title":"6. Implementation Order","text":"Step Task Output 1 Add <code>chat_id</code>, <code>contact_name</code> to WhatsApp extraction (join ZWACHATSESSION) Messages with contact names 2 Add same fields to Claude extraction (use conv name as contact_name) Unified schema 3 Create <code>chats.csv</code> / <code>list-chats</code> CLI command User sees who we have 4 Add <code>chat-tags.yaml</code> config + loader Tagged messages 5 Implement tag+time batching New batch structure 6 Update analysis prompts with relationship context Richer LLM input 7 Run new analysis Improved style guide 8 Add Gemini/Google Takeout extractor One more source"},{"location":"archive/chat-based-analysis-design/#7-gemini-google-takeout","title":"7. Gemini / Google Takeout","text":"<ul> <li>User has downloaded Takeout</li> <li>Need to: locate the archive, inspect structure, add <code>extract_gemini()</code> or <code>load_gemini_messages()</code> to unified_timeline</li> <li>Likely format: JSON in <code>Takeout/My Activity/Gemini</code> or similar</li> <li>Do this after chat-based analysis is working</li> </ul>"},{"location":"archive/chat-based-analysis-design/#8-open-questions","title":"8. Open Questions","text":"<ol> <li>Default for untagged chats: Skip? Use \"unknown\"? Or \"unlabeled\" and still include?</li> <li>Group chats: Include with tag \"group\" or exclude? (Currently we exclude.)</li> <li>Claude conversations: Many are \"Instagram draft\" style \u2014 tag as \"instagram_ai\" or similar?</li> </ol>"},{"location":"archive/communication-analysis/","title":"Communication Pattern Analysis","text":"<p>Extract your writing style from WhatsApp and Claude chats to generate personalized rules for AI assistants.</p>"},{"location":"archive/communication-analysis/#overview","title":"Overview","text":"<p>This feature analyzes your communication patterns from:</p> <ol> <li>WhatsApp messages - Your real-world texting style</li> <li>Claude chat transcripts - How Claude responds to you effectively</li> </ol> <p>It then generates Cursor rules that help AI assistants: - Match your writing tone and style - Ask clarifying questions you typically need - Structure responses the way you prefer</p>"},{"location":"archive/communication-analysis/#quick-start","title":"Quick Start","text":"<pre><code># Analyze WhatsApp messages (most recent 1000)\nbrainlayer analyze-style\n\n# Analyze more messages\nbrainlayer analyze-style --whatsapp-limit 5000\n\n# Include Claude chat export\nbrainlayer analyze-style --claude-export ~/Downloads/claude-export.json\n\n# Custom output location\nbrainlayer analyze-style --output ~/.cursor/rules/my-style.md\n\n# Export full analysis as JSON\nbrainlayer analyze-style --export-json\n</code></pre>"},{"location":"archive/communication-analysis/#data-sources","title":"Data Sources","text":""},{"location":"archive/communication-analysis/#1-whatsapp-messages","title":"1. WhatsApp Messages","text":"<p>Location: <code>~/Library/Group Containers/group.net.whatsapp.WhatsApp.shared/ChatStorage.sqlite</code></p> <p>The tool automatically reads from WhatsApp's local database (macOS only). It extracts: - Your sent messages (to understand your writing style) - Received messages (for context and conversation patterns) - Excludes group chats by default (use personal 1-on-1 conversations)</p> <p>Privacy: All analysis happens locally. No data is sent anywhere.</p>"},{"location":"archive/communication-analysis/#2-claude-chat-transcripts","title":"2. Claude Chat Transcripts","text":"<p>Getting your Claude chat data:</p>"},{"location":"archive/communication-analysis/#option-a-manual-export-recommended","title":"Option A: Manual Export (Recommended)","text":"<ol> <li>Go to claude.ai</li> <li>Click Settings \u2192 Data &amp; Privacy</li> <li>Click \"Export my data\"</li> <li>Download the export file</li> <li>Run: <code>brainlayer analyze-style --claude-export ~/Downloads/claude-export.json</code></li> </ol>"},{"location":"archive/communication-analysis/#option-b-browser-export-advanced","title":"Option B: Browser Export (Advanced)","text":"<p>If Claude doesn't offer export, you can manually copy conversations:</p> <ol> <li>Open a Claude conversation</li> <li>Copy the entire conversation text</li> <li>Save to a JSON file with this format:</li> </ol> <pre><code>{\n  \"conversations\": [\n    {\n      \"id\": \"conv-1\",\n      \"messages\": [\n        {\n          \"role\": \"user\",\n          \"content\": \"Your message here\",\n          \"timestamp\": \"2026-01-30T12:00:00Z\"\n        },\n        {\n          \"role\": \"assistant\",\n          \"content\": \"Claude's response here\",\n          \"timestamp\": \"2026-01-30T12:00:05Z\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"archive/communication-analysis/#option-c-indexeddb-extraction-experimental","title":"Option C: IndexedDB Extraction (Experimental)","text":"<p>Claude desktop app stores chats in IndexedDB (LevelDB format). This is complex to extract:</p> <pre><code># Install plyvel for LevelDB access\npip install plyvel\n\n# Then use the extraction module\npython -c \"\nfrom brainlayer.pipeline.extract_claude_desktop import extract_claude_chats\nfor chat in extract_claude_chats(method='plyvel'):\n    print(chat)\n\"\n</code></pre> <p>Note: IndexedDB extraction is experimental and may not work reliably.</p>"},{"location":"archive/communication-analysis/#what-gets-analyzed","title":"What Gets Analyzed","text":""},{"location":"archive/communication-analysis/#writing-style-metrics","title":"Writing Style Metrics","text":"<ul> <li>Message length: Average characters per message</li> <li>Formality score: 0 (very casual) to 1 (very formal)</li> <li>Sentence structure: Sentences per message</li> <li>Punctuation patterns: Exclamation marks, questions</li> <li>Emoji usage: Frequency of emoji use</li> <li>Common phrases: Your frequently used words</li> <li>Greeting patterns: How you start conversations</li> </ul>"},{"location":"archive/communication-analysis/#claude-response-patterns","title":"Claude Response Patterns","text":"<ul> <li>Structure preferences: Bullet points, numbered lists, code blocks</li> <li>Tone: Encouraging vs neutral</li> <li>Explanation style: Examples, analogies, step-by-step</li> <li>Clarifying questions: Common questions Claude asks you</li> </ul>"},{"location":"archive/communication-analysis/#generated-rules","title":"Generated Rules","text":"<p>The tool generates a Cursor rule file with:</p>"},{"location":"archive/communication-analysis/#1-your-communication-profile","title":"1. Your Communication Profile","text":"<pre><code>## User's Writing Style\n\n**Tone**: casual and conversational\n**Verbosity**: concise and to-the-point\n**Average message length**: 87 characters\n**Formality score**: 0.32 (0=informal, 1=formal)\n\n### Preferences\n\n- Use emojis occasionally\n- Exclamation usage: 0.45 per message\n- Question usage: 0.23 per message\n</code></pre>"},{"location":"archive/communication-analysis/#2-response-guidelines","title":"2. Response Guidelines","text":"<pre><code>## Response Style Guidelines\n\nWhen helping this user with text interactions:\n\n1. **Match their tone**: Write in a casual and conversational style\n2. **Match their verbosity**: Keep responses concise and to-the-point\n3. **Structure**: \n   - Use bullet points for lists\n   - Provide concrete examples\n</code></pre>"},{"location":"archive/communication-analysis/#3-clarifying-questions","title":"3. Clarifying Questions","text":"<pre><code>## Common Clarifying Questions\n\nWhen the user's request is ambiguous, consider asking:\n\n- clarify what you mean by \"integrate\"?\n- provide more details about the expected behavior?\n- would you like me to update the existing code or create new?\n</code></pre>"},{"location":"archive/communication-analysis/#4-desktop-app-integration","title":"4. Desktop App Integration","text":"<pre><code>## Desktop App Integration\n\nWhen helping draft messages or responses:\n\n1. **Analyze the context**: What is the user trying to communicate?\n2. **Match their style**: Use the tone and verbosity patterns above\n3. **Ask clarifying questions**: If unclear, ask specific questions before drafting\n4. **Provide options**: Offer 2-3 variations (formal, casual, concise)\n</code></pre>"},{"location":"archive/communication-analysis/#use-cases","title":"Use Cases","text":""},{"location":"archive/communication-analysis/#1-emailmessage-drafting","title":"1. Email/Message Drafting","text":"<p>When you need help writing emails or messages, the AI will: - Match your natural writing style - Suggest appropriate tone for the context - Ask clarifying questions before drafting</p>"},{"location":"archive/communication-analysis/#2-text-response-assistance","title":"2. Text Response Assistance","text":"<p>For quick text responses: - Generate replies in your voice - Match your typical message length - Use your emoji/punctuation patterns</p>"},{"location":"archive/communication-analysis/#3-professional-communication","title":"3. Professional Communication","text":"<p>For work emails: - Adjust formality based on your baseline - Maintain your communication patterns - Suggest appropriate structure</p>"},{"location":"archive/communication-analysis/#privacy-security","title":"Privacy &amp; Security","text":"<ul> <li>All local: Analysis happens on your machine</li> <li>No uploads: Data never leaves your computer</li> <li>Read-only: WhatsApp database is accessed in read-only mode</li> <li>Encrypted data: WhatsApp messages are already encrypted; we just read the local copy</li> <li>No storage: Original messages aren't stored, only aggregate patterns</li> </ul>"},{"location":"archive/communication-analysis/#troubleshooting","title":"Troubleshooting","text":""},{"location":"archive/communication-analysis/#whatsapp-database-not-found","title":"WhatsApp Database Not Found","text":"<p>Error: <code>WhatsApp database not found at: ~/Library/Group Containers/...</code></p> <p>Solutions: 1. Make sure WhatsApp desktop is installed 2. Open WhatsApp at least once to create the database 3. Check if you're using WhatsApp Business (different path)</p>"},{"location":"archive/communication-analysis/#permission-denied","title":"Permission Denied","text":"<p>Error: <code>Permission denied</code> when accessing WhatsApp database</p> <p>Solutions: 1. Grant Terminal/Cursor full disk access in System Preferences 2. macOS: System Preferences \u2192 Security &amp; Privacy \u2192 Privacy \u2192 Full Disk Access 3. Add Terminal.app or Cursor.app to the list</p>"},{"location":"archive/communication-analysis/#no-messages-extracted","title":"No Messages Extracted","text":"<p>Error: Analysis runs but finds 0 messages</p> <p>Solutions: 1. Check if WhatsApp has messages (open the app) 2. Try increasing the limit: <code>--whatsapp-limit 10000</code> 3. Check if messages are in group chats (excluded by default) 4. Use <code>--include-groups</code> flag if you want group messages</p>"},{"location":"archive/communication-analysis/#claude-export-format-issues","title":"Claude Export Format Issues","text":"<p>Error: Failed to parse Claude export</p> <p>Solutions: 1. Verify the JSON format matches the expected structure 2. Check for syntax errors in the JSON file 3. Try the manual conversation copy method instead</p>"},{"location":"archive/communication-analysis/#advanced-usage","title":"Advanced Usage","text":""},{"location":"archive/communication-analysis/#analyze-specific-conversations","title":"Analyze Specific Conversations","text":"<pre><code>from brainlayer.pipeline.extract_whatsapp import extract_whatsapp_messages\n\n# Get only your messages\nmy_messages = list(extract_whatsapp_messages(\n    only_from_me=True,\n    limit=1000\n))\n\n# Include group chats\nall_messages = list(extract_whatsapp_messages(\n    exclude_groups=False,\n    limit=5000\n))\n</code></pre>"},{"location":"archive/communication-analysis/#custom-analysis","title":"Custom Analysis","text":"<pre><code>from brainlayer.pipeline.analyze_communication import CommunicationAnalyzer\n\nanalyzer = CommunicationAnalyzer()\n\n# Add your own message data\nanalyzer.user_messages.append({\n    'text': 'Your message here',\n    'source': 'custom',\n    'timestamp': None\n})\n\n# Generate analysis\nstyle = analyzer.analyze_writing_style()\nprint(style)\n</code></pre>"},{"location":"archive/communication-analysis/#export-analysis-data","title":"Export Analysis Data","text":"<pre><code># Export full analysis as JSON for further processing\nbrainlayer analyze-style --export-json\n\n# Output: ~/.cursor/rules/communication-style.json\n</code></pre> <p>The JSON contains: - Raw metrics - Sample messages - Clarifying questions - All analysis data</p>"},{"location":"archive/communication-analysis/#integration-with-cursor","title":"Integration with Cursor","text":"<p>The generated rules are automatically saved to <code>~/.cursor/rules/communication-style.md</code>.</p> <p>Cursor will automatically load these rules and apply them when: - You ask for help drafting messages - You request text/email assistance - You use the AI for communication tasks</p> <p>You can also reference the rules explicitly:</p> <pre><code>@communication-style.md Help me write a response to this email\n</code></pre>"},{"location":"archive/communication-analysis/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>[ ] Support for other messaging platforms (Telegram, Signal, iMessage)</li> <li>[ ] Slack workspace analysis</li> <li>[ ] Email analysis (Gmail, Outlook)</li> <li>[ ] Conversation threading detection</li> <li>[ ] Sentiment analysis</li> <li>[ ] Time-based patterns (morning vs evening style)</li> <li>[ ] Contact-specific styles (formal with boss, casual with friends)</li> </ul>"},{"location":"archive/communication-analysis/#examples","title":"Examples","text":""},{"location":"archive/communication-analysis/#example-1-casual-communicator","title":"Example 1: Casual Communicator","text":"<p>Input: 1000 WhatsApp messages, casual style</p> <p>Generated Rule: <pre><code>**Tone**: casual and conversational\n**Verbosity**: concise and to-the-point\n**Emoji rate**: 1.2 per message\n\nWhen drafting messages, use:\n- Short sentences\n- Casual language (\"gonna\", \"wanna\")\n- Emojis for emphasis\n- Minimal punctuation\n</code></pre></p>"},{"location":"archive/communication-analysis/#example-2-professional-communicator","title":"Example 2: Professional Communicator","text":"<p>Input: 1000 WhatsApp messages, formal style</p> <p>Generated Rule: <pre><code>**Tone**: professional and formal\n**Verbosity**: detailed and thorough\n**Emoji rate**: 0.1 per message\n\nWhen drafting messages, use:\n- Complete sentences\n- Formal language\n- Proper punctuation\n- Structured paragraphs\n</code></pre></p>"},{"location":"archive/communication-analysis/#contributing","title":"Contributing","text":"<p>Have ideas for improving communication analysis? Open an issue or PR!</p> <p>Areas for contribution: - Additional messaging platforms - Better pattern detection - More sophisticated style analysis - UI for reviewing/editing generated rules</p>"},{"location":"archive/communication-analysis/#license","title":"License","text":"<p>Same as brainlayer project (see main README).</p>"},{"location":"archive/hierarchical-clustering-deep-research/","title":"Hierarchical knowledge clustering for BrainLayer's personal knowledge graph","text":"<p>Recursive Leiden community detection on a Faiss-built KNN graph is the optimal algorithm for BrainLayer's 245K chunks, running entirely on an M1 Pro in under 30 minutes at ~3\u20134 GB peak memory. This approach delivers a clean 3-level hierarchy with direct control over cluster counts, supports incremental updates, and leverages the user's existing Leiden experience. The biggest risk isn't the clustering itself but the Hebrew embeddings: bge-large-en-v1.5 produces near-random vectors for Hebrew text, meaning 16K WhatsApp chunks need re-embedding with BGE-M3 before clustering can work cross-lingually.</p> <p>This report provides a complete implementation blueprint: algorithm with parameters, memory plan, Python code outline, SQLite schema, incremental strategy, search integration, labeling pipeline, content automation hooks, visualization approach, migration plan, and risk assessment.</p>"},{"location":"archive/hierarchical-clustering-deep-research/#1-recursive-leiden-wins-on-every-constraint-that-matters","title":"1. Recursive Leiden wins on every constraint that matters","text":"<p>HDBSCAN is infeasible at 245K \u00d7 1024 dimensions without dimensionality reduction. The generic algorithm (the only option at 1024 dims, since ball trees degrade past ~50 dims) needs the full pairwise distance matrix: 245K\u00b2 \u00d7 8 bytes \u2248 480 GB. Even with UMAP reducing to 50 dimensions first, HDBSCAN still requires ~6\u201315 GB and takes 1\u20132 hours\u2014and critically, its condensed tree doesn't give direct control over target cluster counts. You'd need to binary-search over <code>min_cluster_size</code> or <code>cut_distance</code> to approximate 30\u201350 / 300\u2013500 / 3000\u20135000 clusters.</p> <p>Faiss K-Means is fast (~5 minutes) but assumes spherical clusters, which poorly fits semantic embedding spaces where clusters have irregular shapes. It also produces wildly uneven cluster sizes\u2014some Voronoi cells getting 50K points while others get 100.</p> <p>Recursive Leiden on a KNN graph is the clear winner for four reasons:</p> <ul> <li>Memory: ~3\u20134 GB peak (embeddings + Faiss index + igraph + partitions), easily fits 32 GB</li> <li>Speed: 15\u201330 minutes total on M1 Pro</li> <li>Hierarchy control: Resolution parameter directly tunes cluster count; recursive application guarantees perfect nesting</li> <li>Familiarity: The user already runs Leiden at 3 resolutions on session-level aggregates in <code>brain_graph.py</code></li> </ul> <p>The pipeline: L2-normalize all 245K embeddings \u2192 build a k=30 KNN graph via Faiss IndexFlatIP (5\u201315 minutes, ~2 GB) \u2192 convert to igraph \u2192 run Leiden recursively. Level 0 uses resolution ~0.005 for 30\u201350 clusters, then each Level 0 subgraph runs at ~0.05 for Level 1, then each Level 1 subgraph at ~0.5 for Level 2. These resolution values are starting points requiring binary-search tuning on the actual data, but the Leiden call itself takes only 1\u20133 minutes per level.</p> Criterion Leiden HDBSCAN (w/ UMAP) Faiss K-Means Birch Memory (32 GB Mac) ~3\u20134 GB ~8\u201315 GB ~2 GB ~10 GB Time 15\u201330 min 1\u20132 hrs 5 min 30 min 3-level hierarchy Resolution control Condensed tree (indirect) Forced nesting Not natural Cluster quality Excellent Excellent Spherical assumption CF tree limits Incremental updates Add nodes/edges Must re-fit Re-assign only partial_fit <p>Optional RunPod validation (~$0.50): Run UMAP + HDBSCAN on an RTX 3090 instance (125 GB RAM, ~$0.25/hr) to get an independent clustering for comparison. Use HDBSCAN's condensed tree to identify noise points that Leiden may have forced into clusters. This is a useful quality check, not a requirement.</p>"},{"location":"archive/hierarchical-clustering-deep-research/#2-memory-and-compute-plan-everything-runs-locally","title":"2. Memory and compute plan: everything runs locally","text":"<p>The entire pipeline fits comfortably on the M1 Pro with 32 GB unified memory. Before starting, kill Ollama, any running Claude sessions, and heavy processes to free ~20\u201324 GB for the pipeline.</p> <p>Step-by-step execution:</p> Step Operation Memory Time Machine 1 Extract embeddings from sqlite-vec (batched 10K reads) ~1 GB (float32 matrix) 2\u20133 min Local 2 L2-normalize embeddings (in-place) +0 MB Seconds Local 3 Build Faiss IndexFlatIP, search k=30 ~2 GB (index + results) 5\u201315 min Local 4 Construct igraph from KNN edges ~400 MB 1\u20132 min Local 5 Recursive Leiden at 3 levels ~200 MB overhead 5\u201310 min Local 6 Compute centroids per cluster ~50 MB 1 min Local 7 Write hierarchy to SQLite Minimal 1\u20132 min Local 8 c-TF-IDF labeling (all clusters) ~500 MB 2\u20135 min Local 9 LLM labeling (top 2 levels, ~300\u2013550 clusters) 19 GB (Ollama) 20\u201337 min Local Peak Steps 1\u20135 concurrent ~3.6 GB Total ~45\u201380 min <p>Steps 1\u20137 run without Ollama. After the hierarchy is built, unload the Faiss index and igraph (freeing ~2.5 GB), then start Ollama for LLM labeling. The two phases never compete for memory.</p> <p>If RunPod is used for UMAP + HDBSCAN validation: provision an RTX 3090 community instance ($0.25/hr), upload the ~1 GB embedding matrix via SCP, run UMAP (1024\u219250, ~30 min) + HDBSCAN (~30 min), download results. Total cost: ~$0.50, well within the $5\u201320 budget.</p>"},{"location":"archive/hierarchical-clustering-deep-research/#3-python-code-outline-with-key-functions-and-data-flow","title":"3. Python code outline with key functions and data flow","text":"<pre><code># === PHASE 1: EXTRACT &amp; PREPARE ===\n\ndef extract_embeddings(db_path: str, batch_size: int = 10_000) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Read all embeddings from sqlite-vec into numpy arrays.\n    Returns (embeddings: float32[N, 1024], chunk_ids: int64[N])\"\"\"\n    conn = apsw.Connection(db_path)\n    all_embeddings, all_ids = [], []\n    offset = 0\n    while True:\n        rows = conn.execute(\n            \"SELECT chunk_id, embedding FROM vec_chunks LIMIT ? OFFSET ?\",\n            (batch_size, offset)\n        ).fetchall()\n        if not rows:\n            break\n        for cid, blob in rows:\n            all_ids.append(cid)\n            all_embeddings.append(np.frombuffer(blob, dtype=np.float32))\n        offset += batch_size\n    return np.vstack(all_embeddings), np.array(all_ids, dtype=np.int64)\n\n\ndef build_knn_graph(embeddings: np.ndarray, k: int = 30) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Build k-NN graph using Faiss. Returns (indices[N, k], distances[N, k]).\"\"\"\n    import faiss\n    n, d = embeddings.shape\n    faiss.normalize_L2(embeddings)  # in-place for cosine similarity via IP\n    index = faiss.IndexFlatIP(d)\n    index.add(embeddings)\n    distances, indices = index.search(embeddings, k + 1)\n    return indices[:, 1:], distances[:, 1:]  # drop self-match\n\n\ndef knn_to_igraph(indices: np.ndarray, distances: np.ndarray, n: int) -&gt; ig.Graph:\n    \"\"\"Convert KNN results to weighted undirected igraph.\"\"\"\n    import igraph as ig\n    k = indices.shape[1]\n    sources = np.repeat(np.arange(n), k)\n    targets = indices.flatten()\n    weights = distances.flatten()\n    edges = list(zip(sources.tolist(), targets.tolist()))\n    g = ig.Graph(n=n, edges=edges, directed=True)\n    g.es['weight'] = weights.tolist()\n    g.to_undirected(mode='collapse', combine_edges={'weight': 'max'})\n    return g\n\n\n# === PHASE 2: RECURSIVE LEIDEN CLUSTERING ===\n\ndef find_resolution_for_target(graph: ig.Graph, target_clusters: int,\n                                lo: float = 0.0001, hi: float = 5.0,\n                                tolerance: int = 5, max_iter: int = 20) -&gt; float:\n    \"\"\"Binary search for Leiden resolution parameter yielding target cluster count.\"\"\"\n    import leidenalg\n    for _ in range(max_iter):\n        mid = (lo + hi) / 2\n        part = leidenalg.find_partition(\n            graph, leidenalg.RBConfigurationVertexPartition,\n            weights='weight', resolution_parameter=mid, seed=42\n        )\n        n_clusters = len(set(part.membership))\n        if abs(n_clusters - target_clusters) &lt;= tolerance:\n            return mid\n        if n_clusters &lt; target_clusters:\n            lo = mid\n        else:\n            hi = mid\n    return mid\n\n\ndef recursive_leiden(graph: ig.Graph, node_indices: np.ndarray,\n                     level_targets: list[int]) -&gt; dict:\n    \"\"\"Run Leiden recursively for guaranteed nested hierarchy.\n    level_targets: [40, 10, 10] means 40 top clusters, ~10 sub per top, ~10 per mid.\n    Returns: {cluster_id: {level, parent_id, member_indices, children}}\"\"\"\n    import leidenalg\n    hierarchy = {}\n    cluster_counter = [0]\n\n    def _cluster_level(subgraph, parent_indices, parent_id, level):\n        if level &gt;= len(level_targets):\n            return\n        target = level_targets[level]\n        res = find_resolution_for_target(subgraph, target)\n        part = leidenalg.find_partition(\n            subgraph, leidenalg.RBConfigurationVertexPartition,\n            weights='weight', resolution_parameter=res, seed=42\n        )\n        communities = {}\n        for local_idx, comm in enumerate(part.membership):\n            communities.setdefault(comm, []).append(local_idx)\n\n        for comm_id, local_members in communities.items():\n            cid = cluster_counter[0]\n            cluster_counter[0] += 1\n            global_members = parent_indices[local_members]\n            hierarchy[cid] = {\n                'level': level, 'parent_id': parent_id,\n                'member_indices': global_members, 'children': []\n            }\n            if parent_id is not None:\n                hierarchy[parent_id]['children'].append(cid)\n            # Recurse into subgraph\n            if level + 1 &lt; len(level_targets) and len(local_members) &gt; 5:\n                sub = subgraph.subgraph(local_members)\n                _cluster_level(sub, global_members, cid, level + 1)\n\n    _cluster_level(graph, node_indices, None, 0)\n    return hierarchy\n\n\n# === PHASE 3: CENTROID COMPUTATION ===\n\ndef compute_centroids(hierarchy: dict, embeddings: np.ndarray) -&gt; dict[int, np.ndarray]:\n    \"\"\"Compute mean embedding for each cluster.\"\"\"\n    centroids = {}\n    for cid, info in hierarchy.items():\n        member_embs = embeddings[info['member_indices']]\n        centroids[cid] = member_embs.mean(axis=0).astype(np.float32)\n    return centroids\n\n\n# === PHASE 4: INCREMENTAL ASSIGNMENT ===\n\ndef assign_new_chunk(embedding: np.ndarray, db_conn,\n                     level_order: list[int] = [0, 1, 2]) -&gt; dict[int, int]:\n    \"\"\"Top-down nearest-centroid assignment for a single new chunk.\"\"\"\n    assignments = {}\n    parent_id = None\n    for level in level_order:\n        if parent_id is None:\n            query = \"\"\"SELECT cluster_id, distance FROM vec_cluster_centroids\n                       WHERE centroid_embedding MATCH ? AND k=1 AND level=?\"\"\"\n            row = db_conn.execute(query, [embedding.tobytes(), level]).fetchone()\n        else:\n            query = \"\"\"SELECT cluster_id, distance FROM vec_cluster_centroids\n                       WHERE centroid_embedding MATCH ? AND k=1\n                       AND level=? AND parent_id=?\"\"\"\n            row = db_conn.execute(query, [embedding.tobytes(), level, parent_id]).fetchone()\n        assignments[level] = row[0]\n        parent_id = row[0]\n    return assignments\n\n\ndef update_centroid_incremental(cluster_id: int, new_embedding: np.ndarray,\n                                 db_conn):\n    \"\"\"Update centroid as running mean: new = (old * n + new_emb) / (n + 1).\"\"\"\n    row = db_conn.execute(\n        \"SELECT centroid_embedding, chunk_count FROM vec_cluster_centroids WHERE cluster_id=?\",\n        [cluster_id]\n    ).fetchone()\n    old_centroid = np.frombuffer(row[0], dtype=np.float32)\n    n = row[1]\n    updated = ((old_centroid * n) + new_embedding) / (n + 1)\n    updated = updated.astype(np.float32)\n    db_conn.execute(\n        \"UPDATE vec_cluster_centroids SET centroid_embedding=?, chunk_count=? WHERE cluster_id=?\",\n        [updated.tobytes(), n + 1, cluster_id]\n    )\n</code></pre> <p>Data flow: <code>extract_embeddings</code> \u2192 <code>build_knn_graph</code> \u2192 <code>knn_to_igraph</code> \u2192 <code>recursive_leiden</code> \u2192 <code>compute_centroids</code> \u2192 write to SQLite tables + <code>vec_cluster_centroids</code>. New chunks enter via <code>assign_new_chunk</code> \u2192 <code>update_centroid_incremental</code>.</p>"},{"location":"archive/hierarchical-clustering-deep-research/#4-sqlite-schema-exact-ddl-for-hierarchy-tables","title":"4. SQLite schema: exact DDL for hierarchy tables","text":"<pre><code>-- ============================================================\n-- CLUSTER HIERARCHY (adjacency list + materialized path)\n-- ============================================================\nCREATE TABLE clusters (\n    id            INTEGER PRIMARY KEY,\n    level         INTEGER NOT NULL CHECK(level IN (0, 1, 2)),\n    parent_id     INTEGER REFERENCES clusters(id) ON DELETE SET NULL,\n    path          TEXT NOT NULL,               -- \"/0003/0051/0551\" (zero-padded)\n    label         TEXT,                        -- LLM-generated natural language label\n    ctfidf_label  TEXT,                        -- c-TF-IDF keyword label (always present)\n    chunk_count   INTEGER NOT NULL DEFAULT 0,\n    avg_intra_dist   REAL,                     -- mean cosine dist to centroid\n    silhouette_score REAL,                     -- cluster quality metric\n    created_at    TEXT NOT NULL DEFAULT (datetime('now')),\n    updated_at    TEXT NOT NULL DEFAULT (datetime('now'))\n);\nCREATE INDEX idx_clusters_level ON clusters(level);\nCREATE INDEX idx_clusters_parent ON clusters(parent_id);\nCREATE INDEX idx_clusters_path ON clusters(path);\n\n-- ============================================================\n-- CHUNK-TO-CLUSTER MAPPING (normalized, survives re-clustering)\n-- ============================================================\nCREATE TABLE chunk_clusters (\n    chunk_id         INTEGER NOT NULL,          -- FK to chunks table\n    cluster_id       INTEGER NOT NULL REFERENCES clusters(id) ON DELETE CASCADE,\n    level            INTEGER NOT NULL,           -- denormalized for speed\n    dist_to_centroid REAL,\n    assignment_method TEXT DEFAULT 'initial',    -- 'initial' | 'incremental' | 'recluster'\n    assigned_at      TEXT NOT NULL DEFAULT (datetime('now')),\n    PRIMARY KEY (chunk_id, level)\n);\nCREATE INDEX idx_cc_cluster ON chunk_clusters(cluster_id);\nCREATE INDEX idx_cc_chunk ON chunk_clusters(chunk_id);\n\n-- ============================================================\n-- CLUSTER CENTROIDS (sqlite-vec, separate from chunk embeddings)\n-- ============================================================\nCREATE VIRTUAL TABLE vec_cluster_centroids USING vec0(\n    cluster_id         INTEGER PRIMARY KEY,\n    centroid_embedding float[1024],\n    level              INTEGER partition key,    -- fast level-filtered KNN\n    parent_id          INTEGER,                  -- metadata: filter within parent\n    chunk_count        INTEGER,                  -- metadata: cluster size\n    +label             TEXT,                     -- auxiliary: human-readable\n    +path              TEXT                      -- auxiliary: materialized path\n);\n\n-- ============================================================\n-- CLUSTERING AUDIT LOG\n-- ============================================================\nCREATE TABLE clustering_runs (\n    id                 INTEGER PRIMARY KEY AUTOINCREMENT,\n    started_at         TEXT NOT NULL DEFAULT (datetime('now')),\n    completed_at       TEXT,\n    algorithm          TEXT NOT NULL,\n    parameters_json    TEXT,\n    total_chunks       INTEGER,\n    num_clusters_l0    INTEGER,\n    num_clusters_l1    INTEGER,\n    num_clusters_l2    INTEGER,\n    avg_silhouette     REAL,\n    status             TEXT DEFAULT 'running'\n);\n</code></pre> <p>Design rationale: The <code>chunk_clusters</code> mapping table is preferred over adding <code>cluster_l0/l1/l2</code> columns to the chunks table because it cleanly separates concerns\u2014the chunks table stays immutable during re-clustering, and you can atomically swap cluster assignments in a transaction. The materialized <code>path</code> column (zero-padded: <code>\"/0003/0051/0551\"</code>) enables fast subtree queries (<code>WHERE path LIKE '/0003/%'</code>) while <code>parent_id</code> handles direct parent lookups. The <code>vec_cluster_centroids</code> table uses <code>level</code> as a partition key, meaning <code>WHERE level = 0</code> only searches the ~40 centroids at Level 0\u2014instant even by brute force.</p>"},{"location":"archive/hierarchical-clustering-deep-research/#5-incremental-update-strategy","title":"5. Incremental update strategy","text":"<p>Daily new chunk assignment uses top-down nearest-centroid routing. For each new chunk: find nearest Level 0 centroid \u2192 find nearest Level 1 centroid within that parent \u2192 find nearest Level 2 centroid within that parent. This requires 5,550 distance computations total (50 + 500 + 5000), which takes milliseconds. After assignment, update the leaf centroid incrementally: <code>new_centroid = (old \u00d7 n + embedding) / (n + 1)</code>.</p> <p>Quality safeguard: Also run KNN voting (find 10 nearest existing chunks, check their cluster assignments). If KNN vote disagrees with centroid assignment, flag the chunk in a <code>staging</code> table for manual review or next re-clustering batch.</p> <p>Cluster health monitoring runs as a daily cron job checking three conditions:</p> <ul> <li>Size split: Leaf cluster exceeds 3\u00d7 the average leaf size (~147 chunks at initial scale) \u2192 run 2-means on that cluster</li> <li>Variance split: Cluster's average intra-distance exceeds 2\u00d7 the global average \u2192 BIC test for 1 vs 2 clusters</li> <li>Merge detection: Two sibling clusters whose centroids are closer than cosine distance 0.1 \u2192 merge and relabel</li> </ul> <p>Re-clustering schedule at ~750 chunks/day growth:</p> Action Frequency Trigger Centroid assignment + update Per-chunk (real-time) Every new chunk ingestion Split/merge health check Daily Cron job Local re-clustering (affected clusters) Weekly &gt;5% of leaf clusters flagged Full hierarchical re-clustering Every 4\u20136 weeks Silhouette score drops &gt;10% from baseline <p>At 750 chunks/day, the dataset grows ~3% per week. Empirically, incremental centroid assignment degrades gracefully for 4\u20136 weeks before requiring a full rebuild. Track a sampled silhouette score (5K random chunks, weekly) as the quality metric that triggers re-clustering.</p>"},{"location":"archive/hierarchical-clustering-deep-research/#6-search-integration-concrete-changes-to-the-existing-pipeline","title":"6. Search integration: concrete changes to the existing pipeline","text":"<p>The current BM25 (FTS5) + semantic (sqlite-vec cosine) \u2192 rerank pipeline should adopt a search-first with cluster expansion strategy. This preserves existing search quality while adding cluster context.</p> <p>Modified search flow:</p> <ol> <li>Existing search \u2014 BM25 + semantic retrieval produces top-K candidates (unchanged)</li> <li>Cluster annotation \u2014 For each result, look up its cluster path via <code>chunk_clusters</code> JOIN <code>clusters</code>. Attach the path as metadata: <code>\"deployment / railway / dockerfile-optimization\"</code></li> <li>Sibling expansion \u2014 For the top-3 results, retrieve 5 highest-cohesion siblings from the same leaf cluster (chunks closest to the cluster centroid, excluding already-returned results). This adds semantically related chunks the user didn't directly query for</li> <li>Cluster-boosted reranking \u2014 When multiple results share a leaf cluster, boost their collective relevance (cluster coherence as a ranking signal). Replace the static importance score with a dynamic cluster relevance score: <code>0.3 \u00d7 log(cluster_size) + 0.3 \u00d7 cohesion + 0.2 \u00d7 (1 - dist_to_centroid) + 0.2 \u00d7 freshness</code></li> </ol> <p>Cluster-first search (find nearest cluster \u2192 search within) is useful as an alternative mode for browsing, not as the default search path. The cluster hypothesis holds well for semantic embeddings, but boundary documents between clusters get missed. Offer it as <code>?mode=browse</code> for when users want to explore a topic rather than find a specific answer.</p> <p>RAPTOR-style enhancement (future phase): Generate LLM summaries for each cluster and store them as searchable nodes. Queries can then match leaf chunks for factual retrieval OR cluster summaries for thematic retrieval\u2014different abstraction levels serve different query types. The RAPTOR paper showed 20% absolute accuracy improvement on the QuALITY benchmark using this approach.</p> <pre><code>-- Cluster annotation query (fast, indexed)\nSELECT c.path, c.label, c.chunk_count\nFROM chunk_clusters cc\nJOIN clusters c ON cc.cluster_id = c.id\nWHERE cc.chunk_id = ? AND cc.level = 2;\n\n-- Sibling expansion query\nSELECT cc2.chunk_id, cc2.dist_to_centroid\nFROM chunk_clusters cc1\nJOIN chunk_clusters cc2 ON cc1.cluster_id = cc2.cluster_id\nWHERE cc1.chunk_id = ? AND cc1.level = 2\n  AND cc2.chunk_id != ?\nORDER BY cc2.dist_to_centroid ASC\nLIMIT 5;\n</code></pre>"},{"location":"archive/hierarchical-clustering-deep-research/#7-labeling-5000-clusters-efficiently-with-a-tiered-hybrid-approach","title":"7. Labeling 5,000 clusters efficiently with a tiered hybrid approach","text":"<p>A pure LLM approach for all 5,000 clusters takes 5.5 hours at 4 seconds/call\u2014feasible overnight but unnecessarily expensive for leaf clusters. The optimal strategy is tiered:</p> <p>Tier 1: c-TF-IDF keywords for all 5,000 clusters (2\u20135 minutes, no LLM). Treat each cluster's chunks as a single document, compute class-based TF-IDF, extract top-5 keywords. Use BERTopic's enhancements: <code>bm25_weighting=True</code> and <code>reduce_frequent_words=True</code> to improve keyword quality. This produces labels like <code>\"docker_railway_deploy_config_dockerfile\"</code>.</p> <p>Tier 2: LLM hybrid labeling for Level 0 + Level 1 (~300\u2013550 clusters, 20\u201337 minutes). For each cluster, pass the c-TF-IDF keywords + 5 representative chunks (nearest to centroid) + any existing enrichment tags to GLM-4.7-Flash with this prompt:</p> <pre><code>I have a topic described by these keywords: {ctfidf_keywords}\nEnrichment tags found in this cluster: {enrichment_tags}\n\nRepresentative content samples:\n{5_closest_to_centroid_chunks, truncated_to_200_chars_each}\n\nProvide a short descriptive label (2-5 words) for this topic.\ntopic:\n</code></pre> <p>Parse everything after <code>topic:</code> as the label. Research shows that including key terms plus representative snippets produces the best labels\u2014better than either alone, and better than human naming in blind evaluations.</p> <p>Labels should be hierarchical paths: <code>\"Deployment / Railway / Dockerfile Config\"</code>. Level 0 labels are the broad domain, Level 1 the sub-area, Level 2 uses c-TF-IDF keywords formatted as a short phrase. This supports breadcrumb navigation in the UI and filter-by-level in search.</p> <p>Parallelizing Ollama: Set <code>OLLAMA_NUM_PARALLEL=2</code> to halve labeling time. Each parallel slot increases VRAM usage proportionally, but with a single 19 GB model and no other workloads, 2 slots should fit in 32 GB. This brings Level 0+1 labeling down to ~10\u201318 minutes. For the full 5,000 clusters, overnight batch at 2 parallel slots takes ~2.75 hours.</p> <p>Existing enrichment data (tags, intents) provides a powerful quality signal. Feed cluster-level tag distributions into the LLM prompt for better labels. Additionally, if any chunks already have manually assigned topic labels, use BERTopic's semi-supervised mode (pass labels as the <code>y</code> parameter) to steer the UMAP dimensionality reduction toward known categories.</p>"},{"location":"archive/hierarchical-clustering-deep-research/#8-content-automation-mcp-tools-n8n-hooks-and-topic-suggestions","title":"8. Content automation: MCP tools, n8n hooks, and topic suggestions","text":"<p>Five MCP tools expose the cluster hierarchy to AI agents in the content pipeline:</p> <ul> <li><code>get_topic_clusters(level, parent_id?, min_chunks?)</code> \u2014 Browse hierarchy. Level 0 returns ~40 top topics with labels and chunk counts. Pass <code>parent_id</code> to drill down.</li> <li><code>get_cluster_details(cluster_id, include_chunks?)</code> \u2014 Full cluster info: label, children, cohesion score, representative chunks. The content pipeline's Claude agent uses this to understand what a topic actually covers.</li> <li><code>find_relevant_clusters(query, limit?)</code> \u2014 Semantic search over cluster centroids via <code>vec_cluster_centroids</code>. Returns ranked clusters. Powers \"find me everything related to Railway deployment.\"</li> <li><code>get_expert_topics(min_chunks?, sort_by?)</code> \u2014 Returns clusters where the user has demonstrable expertise, sorted by a composite score.</li> <li><code>suggest_content_topics(type?, limit?)</code> \u2014 Analyzes the knowledge graph to surface content opportunities across four categories.</li> </ul> <p>Content suggestion categories the <code>suggest_content_topics</code> tool should implement:</p> <ul> <li>Authority content: Clusters with &gt;50 chunks, high cohesion (silhouette &gt;0.5), multiple source types \u2192 \"You have deep expertise in TypeScript monorepo patterns\u2014write a technical guide\"</li> <li>Timely content: Clusters with spiking chunk velocity (&gt;5\u00d7 their 30-day average) \u2192 \"Railway deployment activity surged this week\u2014share your setup\"</li> <li>Unique angles: Chunks bridging two distant clusters (high betweenness centrality) \u2192 \"You've connected Telegram bots with real-estate scraping\u2014that's a unique perspective\"</li> <li>Content gaps: Clusters with high query frequency but low chunk count \u2192 topics the user searches for but hasn't documented</li> </ul> <p>Expertise scoring formula: <pre><code>expertise = 0.30 \u00d7 log(chunk_count)/log(max_count)  // depth\n          + 0.20 \u00d7 silhouette_score                   // focus\n          + 0.20 \u00d7 source_diversity/max_diversity      // breadth\n          + 0.15 \u00d7 temporal_span/365                   // sustained\n          + 0.15 \u00d7 exp(-0.01 \u00d7 days_since_last)        // fresh\n</code></pre></p> <p>n8n integration: Wrap the cluster SQLite queries in a lightweight FastAPI service (5 endpoints matching the MCP tools above). n8n's HTTP Request node calls these endpoints. Example workflow: Schedule Trigger (Monday 9am) \u2192 HTTP Request (<code>GET /api/suggest-content?type=all</code>) \u2192 Code Node (format suggestions) \u2192 Slack Node (post to #content-ideas). For Remotion/ComfyUI pipeline triggers, n8n Webhook nodes receive cluster data and fan out to the appropriate rendering pipeline based on content type.</p>"},{"location":"archive/hierarchical-clustering-deep-research/#9-visualization-plotly-treemap-plus-threejs-hull-overlays","title":"9. Visualization: Plotly treemap plus Three.js hull overlays","text":"<p>Start with a Plotly.js zoomable treemap \u2014 it handles 5,000 nodes natively, requires ~50 lines of code, and provides built-in click-to-zoom with a pathbar breadcrumb. The treemap is the highest insight-per-effort visualization: it fills 100% of screen space, rectangle sizes immediately communicate cluster importance, and drilling down reveals the topic hierarchy intuitively.</p> <pre><code>// Plotly treemap data: three flat arrays from your clusters table\nPlotly.newPlot('treemap', [{\n  type: 'treemap',\n  labels: clusterLabels,      // [\"All\", \"Deployment\", \"Railway\", ...]\n  parents: clusterParents,    // [\"\", \"All\", \"Deployment\", ...]\n  values: chunkCounts,        // [245817, 5000, 1200, ...]\n  textinfo: 'label+value',\n  branchvalues: 'total'\n}], { margin: { t: 30, l: 0, r: 0, b: 0 } });\n</code></pre> <p>Add a sunburst chart as an alternative view \u2014 same data structure, different visual. Sunbursts work especially well for exactly 3 levels: the inner ring shows broad domains, middle ring shows sub-topics, outer ring shows leaf clusters. Toggle between treemap and sunburst with a single button.</p> <p>Integrate with the existing Three.js 3D brain graph by adding semi-transparent convex hull overlays per cluster. Use Three.js's built-in <code>ConvexGeometry</code> to compute hulls from member chunk 3D positions, then render with <code>MeshBasicMaterial({ transparent: true, opacity: 0.15 })</code>. For performance, only render hulls at the current zoom level\u2014Level 0 hulls (~40 meshes) at default zoom, Level 1 hulls (~500) when zoomed into a cluster. UMAP 3D coordinates from the existing brain graph can be reused directly; compute cluster-level positions as the mean of member coordinates.</p> <p>Linked views: Clicking a cluster in the treemap sidebar flies the 3D camera to that cluster's centroid and renders its hull. Hovering a chunk in the 3D graph highlights its cluster path in the treemap. This dual-view approach gives structural overview (treemap) plus spatial exploration (3D graph).</p>"},{"location":"archive/hierarchical-clustering-deep-research/#10-migration-plan-flat-enrichment-to-hierarchical-step-by-step","title":"10. Migration plan: flat enrichment to hierarchical, step by step","text":"<p>Phase 1 \u2014 Hebrew re-embedding (Day 1, ~2\u20134 hours)</p> <p>bge-large-en-v1.5 produces near-random vectors for Hebrew because its BERT tokenizer has virtually zero Hebrew tokens\u2014characters decompose to <code>[UNK]</code> with no learned semantics. The 16K WhatsApp Hebrew embeddings are effectively noise in the current vector space.</p> <p>The practical fix: Re-embed at minimum all 16K WhatsApp chunks with BGE-M3 (1024 dims, 100+ languages, MIRACL SOTA). On a RunPod T4 instance ($0.20/hr), this takes ~10 minutes. Ideally, re-embed the full 245K with BGE-M3 in a single RunPod session (~1\u20132 hours on an A100, ~$2). BGE-M3 matches bge-large-en-v1.5 on English while adding real Hebrew support and handling short-to-long text via its multi-granularity architecture.</p> <p>Critical: Do not mix embeddings from different models in the same vector index. Either re-embed everything with BGE-M3, or maintain two separate <code>vec0</code> tables and cluster each independently, linking cross-source via temporal/entity overlap.</p> <p>Phase 2 \u2014 Initial clustering (Day 1\u20132, ~1 hour)</p> <ol> <li>Kill Ollama and heavy processes</li> <li>Run <code>extract_embeddings()</code> \u2192 <code>build_knn_graph()</code> \u2192 <code>knn_to_igraph()</code> \u2192 <code>recursive_leiden()</code></li> <li>Run <code>compute_centroids()</code></li> <li>Execute the DDL to create <code>clusters</code>, <code>chunk_clusters</code>, <code>vec_cluster_centroids</code> tables</li> <li>Populate all three tables from the clustering results</li> <li>Verify: <code>SELECT level, COUNT(*) FROM clusters GROUP BY level</code> should return ~40/~400/~4000</li> </ol> <p>Phase 3 \u2014 Labeling (Day 2, ~1 hour)</p> <ol> <li>Generate c-TF-IDF labels for all clusters (BERTopic vectorizer, 2\u20135 minutes)</li> <li>Start Ollama with GLM-4.7-Flash, <code>OLLAMA_NUM_PARALLEL=2</code></li> <li>LLM-label Level 0 + Level 1 clusters (~300\u2013550 calls, ~15\u201330 minutes)</li> <li>Write all labels to <code>clusters.label</code> and <code>clusters.ctfidf_label</code></li> </ol> <p>Phase 4 \u2014 Search integration (Day 3)</p> <ol> <li>Modify the search reranker to JOIN <code>chunk_clusters</code> and annotate results with cluster paths</li> <li>Add sibling expansion: for top-3 results, query 5 siblings from the same leaf cluster</li> <li>Replace static importance score with cluster-derived relevance score</li> </ol> <p>Phase 5 \u2014 Visualization + MCP tools (Week 2)</p> <ol> <li>Add Plotly treemap to Ops Dashboard (generates from <code>clusters</code> table)</li> <li>Implement the 5 MCP tools as FastAPI endpoints</li> <li>Add convex hull overlays to existing Three.js brain graph</li> </ol> <p>Phase 6 \u2014 Content automation (Week 3+)</p> <ol> <li>Build expertise scoring query</li> <li>Implement content suggestion categories</li> <li>Create n8n workflows connecting to FastAPI endpoints</li> <li>Wire into Remotion/ComfyUI pipeline selection logic</li> </ol>"},{"location":"archive/hierarchical-clustering-deep-research/#11-risk-assessment-and-what-could-go-wrong","title":"11. Risk assessment and what could go wrong","text":"<p>Hebrew embedding quality is the highest-impact risk. bge-large-en-v1.5 fundamentally cannot process Hebrew\u2014this isn't a \"degraded quality\" situation, it's near-random output. If Hebrew WhatsApp chunks aren't re-embedded with a multilingual model, they'll cluster randomly, corrupt any cross-source topic detection, and make the 16K WhatsApp chunks essentially invisible to the knowledge graph. Mitigation: Re-embed with BGE-M3. If full re-embedding is too costly, at minimum re-embed WhatsApp chunks and keep them in a separate vector index.</p> <p>Short WhatsApp messages (5\u201320 words) produce noisier embeddings than long code blocks (200+ tokens). Even with BGE-M3, expect WhatsApp clusters to be looser with wider variance. Mitigation: Use HDBSCAN-style <code>min_cluster_size</code> thresholds during Leiden subclustering of WhatsApp-heavy subgraphs. Accept that some WhatsApp messages will land in catch-all clusters\u2014this is inherent to short-text clustering, not a system failure.</p> <p>Leiden resolution tuning is empirical, not deterministic. The resolution values (0.005 / 0.05 / 0.5) are starting estimates. The actual values for your data's graph structure could differ by 10\u00d7. Mitigation: The binary-search <code>find_resolution_for_target()</code> function automates this, but verify cluster quality manually for the first run. Inspect the 5 largest and 5 smallest clusters at each level.</p> <p>Centroid drift during incremental updates is a slow-acting risk. The running-mean centroid update (<code>(old \u00d7 n + new) / (n + 1)</code>) biases the centroid toward the temporal distribution of new chunks, which may not represent the cluster's semantic center. After 4\u20136 weeks of 750 chunks/day, cluster boundaries may no longer reflect the actual data distribution. Mitigation: Weekly silhouette sampling detects this early. Re-compute true centroids (mean of all member embeddings) monthly rather than relying solely on the running mean.</p> <p>sqlite-vec partition key filtering in v0.1.6 works for the centroid table's ~5,500 rows but may behave unexpectedly for complex compound filters (<code>level AND parent_id</code>). The partition key creates separate internal indices per level, but metadata filtering happens post-KNN-search. With only ~50 centroids per Level 0 partition, this is fine\u2014but test the <code>AND parent_id = ?</code> filter carefully. Mitigation: If compound filtering fails, fall back to fetching top-K per level and filtering in Python.</p> <p>228K code chunks dominate the embedding space. With 93% of chunks being code conversations, the Level 0 clusters will likely all be code-related, with WhatsApp/YouTube squeezed into 1\u20132 clusters. Mitigation: Consider source-weighted sampling during KNN graph construction (e.g., upweight WhatsApp edges) or run a preliminary source-aware split before unified clustering. Alternatively, accept the imbalance and use the existing <code>content_type</code> metadata to surface non-code clusters in the UI regardless of their relative size.</p> <p>SSD space is tight at 28 GB free. The clustering pipeline temporarily needs ~1 GB for the embedding matrix + ~200 MB for intermediate arrays. The new SQLite tables add ~50 MB (clusters) + ~25 MB (centroid vec0) + ~30 MB (chunk_clusters mapping). Total new storage: ~100\u2013150 MB permanent. This is negligible, but if a full BGE-M3 re-embedding is done, the new vec0 table replaces the old one (same 1024 dims, ~1 GB). No net SSD impact from re-embedding.</p> <p>Code-switching in WhatsApp messages (Hebrew words mixed with English technical terms like \"deploy,\" \"push,\" \"build\") is actually a partial advantage: the English terms provide semantic signal even with the English-only model. But for pure-Hebrew messages (social, personal), the signal is zero. BGE-M3 handles code-switching natively via its XLM-RoBERTa backbone, which was trained on multilingual web data including mixed-language text.</p>"},{"location":"archive/research-chat-based-analysis/","title":"Deep Research: Chat-Based Style Analysis","text":"<p>For: extraction design, user tagging, fallback strategies, prompt enrichment</p>"},{"location":"archive/research-chat-based-analysis/#1-should-we-group-by-chat-relationship-linguistic-psych-research","title":"1. Should We Group by Chat / Relationship? (Linguistic &amp; Psych Research)","text":""},{"location":"archive/research-chat-based-analysis/#verdict-yes-strong-evidence","title":"Verdict: Yes \u2014 strong evidence","text":"<p>Register &amp; addressee effects (linguistics): - Register = way of speaking tied to situation and audience - Style = formality level, shaped by addressee - Addressee influence: familiarity, age, social context all change formality, vocabulary, and structure - Source: Cambridge Handbook of English Corpus Linguistics, Annual Review of Linguistics</p> <p>Communication accommodation theory (digital messaging): - In IM, people converge on message length, timing, and style - Effects differ by friends vs strangers and task vs social context - Style accommodation improves rapport and impressions - Source: \"Communication Accommodation in Instant Messaging\" (Sage), \"Linguistic Style Accommodation Shapes Impression Formation...\" (2017)</p> <p>Friends vs strangers: - Friends: more exploratory, varied topics - Strangers: more convergent, similar patterns - Source: Nature 2024 hyperscanning study</p> <p>Implications for our design: - Grouping by relationship (family, friends, co-workers) is supported by research - Batching only by time loses important variation - Including relationship context in prompts should improve style modeling</p>"},{"location":"archive/research-chat-based-analysis/#2-user-tagging-how-much-what-friction","title":"2. User Tagging: How Much, What Friction?","text":""},{"location":"archive/research-chat-based-analysis/#verdict-minimize-manual-tagging-use-programmatic-llm-assisted-where-possible","title":"Verdict: Minimize manual tagging; use programmatic + LLM-assisted where possible","text":"<p>Annotation burden research: - Programmatic labeling (rules, not per-item clicks): 10\u2013100x faster than manual - Selective labeling: user confirms uncertain cases, not every item \u2014 10x cost reduction - LLM-assisted baselines: model proposes labels, user corrects \u2014 2.8x faster, ~45% better than hand labeling - Source: Snorkel AI, Google \"Selective Labeling\" (2024), FreeAL / ActiveLLM</p> <p>What to do: 1. Do: Provide a chat list (contact, message count) so the user can tag in bulk 2. Consider: LLM pre-labeling \u2014 e.g. infer \"family\" from names like \"Mom\", \"Dad\", \"Aunt Lisa\" 3. Do: Use templates \u2014 e.g. tag by regex on contact name 4. Don\u2019t: Require per-message tagging</p> <p>Recommendation: - Generate <code>chats.csv</code> with contact_name, chat_id, message_count - User edits a YAML/CSV mapping: <code>Mom \u2192 family</code>, <code>Dad \u2192 family</code>, etc. - Optional: LLM suggests tags for top N contacts; user reviews and fixes - Untagged chats: either \"unlabeled\" (included but flagged) or excluded \u2014 make it configurable</p>"},{"location":"archive/research-chat-based-analysis/#3-fallback-for-unknown-contacts","title":"3. Fallback for Unknown Contacts","text":""},{"location":"archive/research-chat-based-analysis/#verdict-multi-step-fallback-avoid-blocking-on-full-identification","title":"Verdict: Multi-step fallback; avoid blocking on full identification","text":"<p>Options, in order of preference:</p> Approach Pros Cons 1. Use contact_name when available Already in DB (ZPARTNERNAME) Sometimes phone number or empty 2. \"Search for sentence\" User can disambiguate Extra step, may be skipped 3. Include as \"unlabeled\" No user effort Dilutes relationship signal 4. Exclude untagged Clean labels Loses data <p>Recommendation: - Primary: Use contact_name from ZWACHATSESSION (we have Mom, Dad, etc.) - Fallback A: If name is only a phone number, keep as-is and let user tag in YAML - Fallback B: Emit a report: \"Chats needing tags: [list with sample message]\" \u2014 user can search for a phrase to identify - Default for untagged: Include in analysis as \"unlabeled\" so we don\u2019t discard data; optionally allow exclusion in config</p>"},{"location":"archive/research-chat-based-analysis/#4-prompt-enrichment-what-context-helps","title":"4. Prompt Enrichment: What Context Helps?","text":""},{"location":"archive/research-chat-based-analysis/#verdict-include-relationship-mix-contact-names-and-temporal-context","title":"Verdict: Include relationship mix, contact names, and temporal context","text":"<p>Context engineering (LLMs): - Context = selection, compression, and ordering of information - Metadata helps build query-specific context - Source: \"Everything is Context\" (arxiv 2025), Readme_AI</p> <p>What to add to prompts:</p> Context Format Rationale Relationship mix \"~60% family, ~40% friends\" Register/addressee effects Contact names \"Mom, Dad, Friend A, Jane\" Grounds examples, reduces hallucination Temporal \"2024-H2, mostly evening/weekend\" Style varies by time of day Source \"WhatsApp 1:1\" Medium affects formality <p>Don\u2019t overload: - Keep prompts within context limits - Prefer summary stats over raw dumps</p> <p>Example enriched prompt: <pre><code>Analyze 250 messages from 2024-H2.\nRelationship context: ~55% family (Mom, Dad), ~30% friends (Friend A, Friend B), ~15% unlabeled.\nContact names you may reference: Mom, Dad, Friend A, Friend B, Jane Smith.\nLanguage: HEBREW.\nConsider how tone may shift by relationship (family vs friends vs work).\n</code></pre></p>"},{"location":"archive/research-chat-based-analysis/#5-extraction-technical-notes","title":"5. Extraction: Technical Notes","text":""},{"location":"archive/research-chat-based-analysis/#whatsapp","title":"WhatsApp","text":"<ul> <li>Chat ID: ZTOJID (e.g. 972501234567@s.whatsapp.net)</li> <li>Display name: ZPARTNERNAME from ZWACHATSESSION (join on ZCHATSESSION)</li> <li>Confirmed: Mom, Dad, Friend A, Jane Smith, etc. are available</li> </ul>"},{"location":"archive/research-chat-based-analysis/#google-takeout-gemini","title":"Google Takeout / Gemini","text":"<ul> <li>Location: <code>Takeout/My Activity/Gemini Apps/</code></li> <li>Format: HTML (MyActivity.html, ~1.3MB) plus JSON-like and binary files</li> <li>Gemini folder: Only gems/scheduled actions (tiny); main data is in My Activity</li> <li>Parser: <code>google-takeout-parser</code> supports My Activity; may need a Gemini-specific handler</li> <li>Note: My Activity schema uses <code>header</code>, <code>title</code>, <code>time</code>, <code>products</code>; exact structure for Gemini turns needs inspection</li> </ul>"},{"location":"archive/research-chat-based-analysis/#6-recommendations-summary","title":"6. Recommendations Summary","text":"Area Do Don\u2019t Extraction Join with ZWACHATSESSION for contact names; add chat_id, contact_name to schema Rely on ZPUSHNAME from messages (often encoded) Tagging Bulk YAML/CSV mapping; optional LLM suggestions Per-message or per-chat manual clicks Fallback Use contact_name; report \"needs tags\"; include unlabeled by default Block analysis on full tagging Prompts Add relationship mix, contact names, optional time-of-day Dump full metadata into every prompt Batching Tag + time (e.g. family-2024-H2) Time-only batching"},{"location":"archive/research-chat-based-analysis/#7-open-questions","title":"7. Open Questions","text":"<ol> <li>LLM pre-tagging: Use local model to propose family/friends/work from names? Worth the complexity?</li> <li>Group chats: Include with tag \"group\" or keep excluding?</li> <li>Gemini HTML: Parsing strategy \u2014 use google-takeout-parser or custom HTML parser?</li> <li>Claude conversations: Tag \"instagram_ai\", \"general\", etc. by conversation name patterns?</li> </ol>"},{"location":"archive/research-prompt-embeddings/","title":"Deep Research Prompt: Embeddings &amp; Hybrid Pipelines for Communication Style Analysis","text":"<p>Context: This is a follow-up to the BrainLayer chat-based style analysis research. We now have a pipeline that loads messages from WhatsApp, Claude, Gemini; groups them by chat + time; tags relationships (family, friends, co-workers); and runs a generative LLM (qwen3-coder-64k via Ollama) to analyze style per batch. We are not currently using embeddings for style analysis\u2014only nomic-embed-text exists for the knowledge-indexing pipeline.</p> <p>Research questions:</p> <ol> <li>Heavier/smarter embedding models (local/Ollama, 2024\u20132026)</li> <li>What are the best open embedding models that run locally (e.g. via Ollama)?</li> <li>How does nomic-embed-text compare to alternatives (bge-m3, mxbai-embed-large, GritLM, jina-embeddings-v3, multilingual variants, etc.)?</li> <li>Tradeoffs: quality vs. speed vs. VRAM vs. multilingual (Hebrew + English).</li> <li> <p>Which models are recommended for semantic search over personal communication (short, informal messages)?</p> </li> <li> <p>Hybrid embedding + generative pipeline</p> </li> <li>How do RAG-style pipelines combine embeddings with generative LLMs? (e.g. embed \u2192 retrieve \u2192 feed to LLM.)</li> <li>For style analysis specifically: has anyone used embeddings to improve sampling or retrieval before LLM analysis?</li> <li>Pattern: embed messages \u2192 cluster or retrieve similar ones \u2192 use clusters/exemplars as context for a generative model. Papers, implementations?</li> <li> <p>Pattern: embed style exemplars, then at inference embed a new situation and retrieve \"similar past messages\" for few-shot drafting. Is this used anywhere?</p> </li> <li> <p>Concrete recommendations</p> </li> <li>For a longitudinal communication-style analysis pipeline (batches by time + relationship, ~50k\u2013200k messages): should we add embeddings? For what role?</li> <li>If yes: which model, and what flow (pre-sampling, retrieval-augmented analysis, or both)?</li> <li>For future \"draft in my style\" features: embedding-based retrieval vs. pure generative. What does the literature suggest?</li> </ol> <p>Output format: - A comparison table of embedding models (quality, speed, multilingual, local feasibility). - 1\u20132 concrete hybrid pipeline designs (with citations) that fit our use case. - Clear recommendation: add embeddings or not, which model, and how to integrate with the current flow.</p> <p>Reference: The prior BrainLayer research (chat-based analysis, relationship tagging, longitudinal batching). Current setup: Ollama, nomic-embed-text for indexing, qwen3-coder-64k for style analysis.</p> <p>Hardware &amp; system constraints (constrain all recommendations to what can run on this machine):</p> Spec Value CPU Apple M1 Pro RAM 32 GB Unified memory / VRAM Shared with CPU (no discrete GPU) Storage free 30 GiB OS macOS Inference stack Ollama (local) <p>Models must fit in 32 GB shared memory alongside the OS and any running apps. Storage for vector DB and model weights is limited\u2014factor in embedding model size, ChromaDB index size, and headroom.</p>"},{"location":"research/launch-readiness-audit/","title":"BrainLayer launch readiness audit","text":"<p>BrainLayer enters a booming MCP ecosystem with a strong technical differentiator but needs strategic packaging to stand out. The MCP protocol now powers 97 million monthly SDK downloads across 10,000+ servers, backed by the Linux Foundation's Agentic AI Foundation with Anthropic, Google, Microsoft, AWS, and OpenAI as members. BrainLayer's 12-tool suite with hybrid semantic+keyword search and LLM enrichment offers a genuine leap over the official MCP memory server (basic text matching on a JSON file) and a compelling open-source alternative to mem0's cloud-centric model. The window is open: memory is the fastest-growing MCP category, the official MCP Registry launched just months ago, and no dominant open-source MCP-native memory server has yet emerged.</p> <p>This audit covers the full launch surface \u2014 ecosystem requirements, competitive positioning, documentation, branding, distribution, and a prioritized checklist with effort estimates.</p>"},{"location":"research/launch-readiness-audit/#the-mcp-ecosystem-demands-more-than-working-code","title":"The MCP ecosystem demands more than working code","text":"<p>The MCP specification (latest: 2025-11-25) has matured rapidly. Production-ready servers in 2026 must implement several features beyond basic tool definitions.</p> <p>Tool annotations are now expected, not optional. Clients like VS Code and Claude Desktop use <code>readOnlyHint</code> to skip confirmation dialogs, and <code>destructiveHint</code> and <code>idempotentHint</code> signal safety properties to agent orchestrators. BrainLayer's read tools should set <code>readOnlyHint: true</code>; write tools need <code>destructiveHint</code> and <code>idempotentHint</code> configured accurately. The spec also supports <code>openWorldHint</code> for tools interacting with external services (relevant for LLM enrichment calls).</p> <p>Structured content with <code>outputSchema</code> (added June 2025) lets tools return typed JSON alongside traditional text blocks. This is increasingly important as agents chain MCP tool outputs into downstream processing. BrainLayer should define output schemas for search results, memory stats, and context retrieval tools.</p> <p>The Official MCP Registry at <code>registry.modelcontextprotocol.io</code> is the single most important listing. Launched September 2025, it validates namespace ownership (GitHub username or DNS) and pings servers for uptime. Publishing requires a <code>server.json</code> metadata file and the <code>mcp-publisher</code> CLI. There is no formal certification program from Anthropic or third parties \u2014 the registry listing is the closest official stamp of legitimacy.</p> <p>Beyond the official registry, the directory landscape is fragmented but active. MCP.so lists 17,749 servers. PulseMCP tracks 8,610+ with weekly visitor estimates. awesome-mcp-servers on GitHub has 79.9K stars and accepts PRs. Smithery hosts 2,200+ with automated install flows. Glama.ai manually reviews submissions across 5,867+ servers. BrainLayer should submit to all of these \u2014 each directory serves a different discovery audience.</p> <p>The top MCP servers share five patterns: single clear purpose, minimal configuration (zero-config or one API key), excellent documentation with JSON config examples, multi-transport support (stdio + Streamable HTTP), and active development cadence. Context7 (46K stars) succeeds by solving one universal pain point \u2014 outdated LLM training data. Playwright MCP (1.3M weekly visitors on PulseMCP) benefits from Microsoft backing and zero-config browser automation. Supabase MCP wraps a complete platform behind clear tool boundaries. BrainLayer should learn from these: lead with the single pain point (AI agents forget everything) rather than listing all 12 tools upfront.</p>"},{"location":"research/launch-readiness-audit/#how-brainlayer-should-position-against-mem0-and-the-basic-memory-mcp","title":"How BrainLayer should position against mem0 and the basic memory MCP","text":"<p>Mem0 is BrainLayer's most visible competitor, with ~46,900 GitHub stars, $24M in funding, 14 million PyPI downloads, and an AWS partnership as the exclusive memory provider for the Strands Agent SDK. Their hybrid architecture combines vector, key-value, and graph databases with LLM-powered memory extraction. They offer both open-source self-hosting and a managed cloud starting at $19/month.</p> <p>But mem0 has exploitable weaknesses. It requires an external LLM API key (default: OpenAI's gpt-4.1-nano) for basic operation, adding cost and latency. Its free tier caps at 10,000 memories and 1,000 API calls/month. The Letta team publicly challenged mem0's LOCOMO benchmark claims without receiving a response. And critically, mem0 added MCP support as an afterthought via a separate <code>mem0-mcp</code> repository \u2014 it was not designed as an MCP server from the ground up.</p> <p>BrainLayer's differentiation should center on three angles:</p> <p>\"MCP-native, not MCP-bolted-on.\" BrainLayer IS an MCP server. Every design decision serves the MCP workflow. Mem0 is a memory platform that also exposes MCP tools. This distinction matters for developers who want memory that integrates seamlessly with Claude Code, Cursor, or VS Code without configuring separate services.</p> <p>\"No API keys required for core operation.\" If BrainLayer can run fully locally without mandatory third-party LLM calls, this is a powerful differentiator. Position against mem0's OpenAI dependency: \"Full persistent memory without sending your data to OpenAI.\"</p> <p>\"Drop-in upgrade from the official MCP memory server.\" The official <code>@modelcontextprotocol/server-memory</code> stores entities, relations, and observations in a single JSON file with simple text matching. It has 9 CRUD tools and zero intelligence \u2014 no embeddings, no semantic search, no enrichment, no scalability. This is BrainLayer's most compelling comparison:</p> Capability Official server-memory BrainLayer Search type Text matching only Semantic + keyword hybrid Embeddings None Full vector embeddings LLM enrichment None Auto-extraction, summarization Scale Single JSON file 268K+ chunks Multi-source indexing No Yes Tool count 9 (basic CRUD) 12 purpose-built tools <p>The aitmpl.com \"memory-integration\" template (4,703 downloads, #2 most-downloaded MCP on the Claude Code Templates marketplace with ~20.9K GitHub stars) is likely a thin wrapper around similar basic key-value storage. BrainLayer should submit to aitmpl.com and position explicitly as the production-grade alternative. The submission process is via GitHub PR to <code>davila7/claude-code-templates</code>.</p> <p>The broader AI memory landscape includes Zep/Graphiti (temporal knowledge graphs, 20K stars for Graphiti, but community edition deprecated \u2014 now cloud-only), Letta/MemGPT (OS-inspired memory tiers, agent self-editing), and LangMem (framework-locked to LangGraph). The trend is clear: graph memory, temporal awareness, and cross-client memory sharing are becoming table stakes. BrainLayer's roadmap should account for these expectations.</p>"},{"location":"research/launch-readiness-audit/#github-repo-and-pypi-listing-must-sell-in-seconds","title":"GitHub repo and PyPI listing must sell in seconds","text":"<p>The \"above the fold\" content \u2014 what appears without scrolling \u2014 determines 80% of first impressions. Analysis of successful AI tool repos reveals a consistent structure:</p> <p>The README should open with a branded banner image, 1-2 rows of badges (CI status, PyPI version, Python versions, license, downloads), a one-line tagline, and a 15-30 second demo GIF created with VHS (Charmbracelet's scriptable terminal recorder). VHS generates reproducible GIFs from <code>.tape</code> files and integrates with GitHub Actions for auto-regeneration. Below the fold: a \"Why BrainLayer?\" section with 2-3 pain\u2192solution points, a 3-5 line quickstart, a feature comparison table (\u2705/\u274c format against mem0 and official server-memory), and an architecture diagram.</p> <p>Essential badges for BrainLayer: CI status (GitHub Actions), PyPI version, Python version support (3.10-3.13), license, monthly downloads (via pypistats), and code coverage (Codecov, aim for 80%+). Keep to two rows maximum.</p> <p>PyPI optimization is straightforward but often neglected. Use <code>pyproject.toml</code> (PEP 621), set <code>long_description_content_type = \"text/markdown\"</code> so the README renders directly on PyPI, include classifiers for <code>Topic :: Scientific/Engineering :: Artificial Intelligence</code> and <code>Typing :: Typed</code>, and populate all project URLs (Homepage, Documentation, Repository, Bug Tracker, Changelog, Discord). Publish via Trusted Publishers (GitHub Actions OIDC integration \u2014 no API tokens needed) using <code>pypa/gh-action-pypi-publish</code>. PyPI has no trending feature \u2014 discovery happens on GitHub, not PyPI. The package name <code>brainlayer</code> should be memorable and keyword-rich in metadata.</p> <p>CI/CD should include more than tests. The standard 2025 Python stack: - Test matrix across Python 3.10-3.13 on Ubuntu + macOS - Ruff for linting + formatting (replaces Black, isort, Flake8 \u2014 10-100x faster) - Mypy or Pyright for type checking - CodeQL for security scanning (free for open-source, catches vulnerabilities in Python) - Dependabot with weekly schedule and auto-merge for patch updates - Gitleaks pre-commit hook to prevent secret leaks - Automated release workflow: git tag \u2192 tests \u2192 build \u2192 PyPI publish \u2192 GitHub Release with auto-generated notes</p> <p>SBOM generation via <code>cyclonedx-bom</code> signals enterprise readiness but is not critical for initial launch.</p>"},{"location":"research/launch-readiness-audit/#mkdocs-material-is-the-clear-documentation-choice","title":"MkDocs Material is the clear documentation choice","text":"<p>Every major Python AI project in 2025-2026 uses MkDocs with Material theme \u2014 FastAPI, Pydantic, LlamaIndex, UV, Ruff, Polars. It installs via pip (no Node.js), configures through a single <code>mkdocs.yml</code>, auto-generates API references via the <code>mkdocstrings</code> plugin, and supports versioning through the <code>mike</code> plugin. Sphinx remains powerful but has a steeper learning curve. Docusaurus and Astro Starlight require Node.js \u2014 a friction point for a Python project's contributor base.</p> <p>Recommended documentation architecture for BrainLayer's 12 tools + CLI:</p> <pre><code>Getting Started (installation, quickstart, editor setup with tabs)\n\u251c\u2500\u2500 Claude Code | Cursor | VS Code Copilot | Zed (tabbed content)\nConcepts (how persistent memory works, MCP basics)\nTool Reference (auto-generated schemas + hand-written examples)\n\u251c\u2500\u2500 Memory tools | Context tools | Management tools\nCLI Reference (auto-generated from Click/Typer)\nPython API Reference (auto-generated via mkdocstrings)\nGuides (configuration, advanced usage, multi-agent setups)\nIntegrations (LangChain, CrewAI, LlamaIndex)\nContributing + Changelog\n</code></pre> <p>Tool documentation should be hybrid: auto-generate the reference table (name, description, input/output schema) from MCP <code>tools/list</code> metadata, then hand-write usage examples, prompt suggestions, and use-case narratives around each auto-generated entry. The editor setup page should use MkDocs Material's content tabs feature to show configuration for Claude Code, Cursor, VS Code, and Zed side by side.</p> <p>Start with docs-as-landing-page (the FastAPI/Pydantic model) rather than building a separate marketing site. MkDocs Material supports custom hero sections via template overrides. Split to a separate landing page only when you need marketing content, pricing pages, or a significant non-developer audience. This is the approach that FastAPI used to reach 70K+ stars \u2014 the documentation IS the product website.</p>"},{"location":"research/launch-readiness-audit/#visual-identity-and-social-assets-need-to-be-ready-on-day-one","title":"Visual identity and social assets need to be ready on day one","text":"<p>Successful open-source AI projects converge on clean geometric SVG logos with gradient accents \u2014 typically purple/blue/teal palettes that convey intelligence and trust. LangChain uses a playful emoji-based identity (\ud83e\udd9c\ud83d\udd17), ChromaDB uses a prismatic gradient, mem0 uses a minimalist purple wordmark, and FastAPI uses a lightning bolt in a hexagon. BrainLayer should target a layered brain/neural motif in SVG format, with monochrome variants for dark and light backgrounds. Use Figma for design (free tier) and avoid AI-generated logos for final branding \u2014 the U.S. Copyright Office has ruled AI-generated images cannot be copyrighted.</p> <p>Social media asset dimensions (create these before launch):</p> Asset Dimensions Purpose GitHub social preview 1280 \u00d7 640 px (PNG) Repo link previews everywhere Open Graph / Twitter / LinkedIn 1200 \u00d7 630 px (PNG) Universal social card <p>These two images cover all platforms. Center critical content (logo, name, tagline) to account for edge cropping. Use dark backgrounds \u2014 most developer audiences browse in dark mode. Test with Facebook Sharing Debugger and Twitter Card Validator before launch.</p> <p>Demo strategy: VHS-scripted GIF for the GitHub README (auto-regenerated in CI), asciinema player embeds for docs pages (lightweight, text-selectable), and short MP4 screen captures (&lt;60 seconds) for Twitter/LinkedIn showing an agent conversation with persistent memory across sessions.</p>"},{"location":"research/launch-readiness-audit/#distribution-requires-simultaneous-multi-channel-activation","title":"Distribution requires simultaneous multi-channel activation","text":"<p>The launch sequence matters as much as the launch content. GitHub Trending's algorithm rewards star velocity relative to historical average \u2014 a new repo getting 30-50 stars in 1-2 hours has a strong chance of trending. The key is driving traffic from at least two external sources simultaneously (e.g., Hacker News + Reddit, or Twitter + Dev.to) to avoid single-source detection.</p> <p>MCP-specific distribution (submit to ALL before launch day): - Official MCP Registry (<code>registry.modelcontextprotocol.io</code>) \u2014 publish via <code>mcp-publisher</code> CLI - awesome-mcp-servers PR (79.9K stars, the canonical list) - aitmpl.com / Claude Code Templates PR - PulseMCP, mcp.so, Glama.ai, Smithery, mcpservers.org, mcpserverfinder.com</p> <p>Launch day execution: - Product Hunt at 12:01 AM PT \u2014 recruit an experienced Hunter (Flo Merian is active for dev tools), prepare maker comment under 800 characters, 4-6 image gallery showing workflow. Weekends can work well for dev tools due to less competition. - Show HN at 8-9 AM ET \u2014 title format: <code>Show HN: BrainLayer \u2013 Open-source persistent memory for AI agents</code>. Link to GitHub repo, not a website. Add a detailed first comment with backstory and technical details. Respond to every comment. - Twitter thread with demo GIF \u2014 hook tweet (\"AI agents have amnesia. I built an open-source fix. \ud83e\uddf5\"), followed by problem \u2192 solution \u2192 demo \u2192 features \u2192 GitHub link with star CTA. Tag Simon Willison, swyx, Alex Albert. - Reddit to r/ClaudeAI and r/Python (stagger by 1-2 hours). Frame as sharing something useful, not self-promotion. Tutorial-style posts outperform announcements.</p> <p>Post-launch week: Cross-post blog to Dev.to and Medium with canonical URLs. Submit to There's An AI For That (2.5M subscribers), FutureTools, Ben's Bites, and the PulseMCP newsletter. Write a tutorial post: \"How to Give Your AI Agent Persistent Memory in 5 Minutes.\" If Hacker News post underperforms, email <code>hn@ycombinator.com</code> for the second-chance pool.</p> <p>MCP-specific communities to engage: Official MCP Discord, r/ClaudeAI, the MCP X Community, Glama.ai Discord, and GitHub Discussions on <code>modelcontextprotocol/registry</code>.</p>"},{"location":"research/launch-readiness-audit/#feature-roadmap-what-ships-before-launch-vs-after","title":"Feature roadmap: what ships before launch vs. after","text":"<p>Based on patterns from mem0 (launched with simple API, added graph memory later), ChromaDB (launched with 4-function API, added cloud later), and LangChain (shipped fast, iterated weekly), the principle is clear: launch with a polished core, ship everything else fast afterward.</p> <p>Before launch (non-negotiable): - Public docs website (g) \u2014 every successful open-source AI tool has docs at launch. MkDocs Material can be deployed in a day. - Security audit of dependencies (e) \u2014 a single CVE in a dependency will torpedo credibility on Hacker News. Run <code>pip-audit</code>, CodeQL, and Dependabot before going public. - CLI UX refresh with rich library (d) \u2014 first impression from <code>pip install brainlayer</code> must be polished. Rich provides progress bars, tables, syntax highlighting with minimal effort.</p> <p>Before launch (strongly recommended): - brainlayer_store write-side MCP tool (b) \u2014 a memory system without an obvious write path through MCP will confuse users on day one. Even a basic version ships the complete read+write story. - LLM backend flexibility (f) \u2014 if launch requires an OpenAI API key, the \"no API keys required\" positioning collapses. Support at least OpenAI + Anthropic + one local option (Ollama) before launch.</p> <p>After launch (v1.1): - Session-level enrichment (a) \u2014 valuable but adds complexity. Ship after gathering user feedback on enrichment patterns. - Document archiving (c) \u2014 a power-user feature that can wait for the post-launch roadmap.</p>"},{"location":"research/launch-readiness-audit/#the-structured-launch-checklist","title":"The structured launch checklist","text":""},{"location":"research/launch-readiness-audit/#must-have-before-launch","title":"MUST HAVE before launch","text":"Item Why it matters Effort Tools/services Public docs site on MkDocs Material Every top AI tool launches with docs. No docs = no trust. 8-12 hours MkDocs Material, mkdocstrings, GitHub Pages Security audit of all dependencies One CVE kills HN credibility. 88% of MCP servers require credentials. 4-6 hours pip-audit, CodeQL, Safety, Dependabot Tool annotations on all 12 MCP tools Spec-required for modern MCP compliance. VS Code skips confirmations based on readOnlyHint. 2-3 hours MCP spec reference Professional README with badges, demo GIF, comparison table Above-the-fold content determines 80% of first impressions. 6-8 hours VHS (demo GIF), Shields.io (badges), Mermaid (diagrams) PyPI listing with full metadata Install via <code>pip install brainlayer</code> must work flawlessly with clean listing. 2-3 hours pyproject.toml, Trusted Publishers, twine CI/CD pipeline (tests, linting, security scanning) Green CI badge signals reliability. Automated publishing prevents release errors. 4-6 hours GitHub Actions, Ruff, Mypy, CodeQL, Codecov GitHub social preview image (1280\u00d7640) Every link share displays this image. No image = amateur signal. 2-3 hours Figma or Canva Open Graph image (1200\u00d7630) Twitter/LinkedIn/Discord link previews. 1-2 hours Same design, different crop Official MCP Registry submission The canonical listing. Backed by Anthropic, GitHub, Microsoft. 2-3 hours mcp-publisher CLI, server.json Editor setup configs for Claude Code, Cursor, VS Code Users need copy-pasteable JSON to start. Blocks adoption without it. 3-4 hours Test across all editors CLI UX polish with Rich library (d) First <code>pip install</code> \u2192 first command must feel polished. 6-8 hours Rich, Typer or Click LICENSE file (Apache 2.0 or MIT) Non-negotiable for enterprise adoption and awesome-list submissions. 0.5 hours Choose Apache 2.0 for maximum enterprise friendliness CONTRIBUTING.md and CODE_OF_CONDUCT.md Signals a healthy project. Required by many awesome-lists. 1-2 hours Templates from GitHub"},{"location":"research/launch-readiness-audit/#should-have-significantly-improves-launch-impact","title":"SHOULD HAVE \u2014 significantly improves launch impact","text":"Item Why it matters Effort Tools/services brainlayer_store write-side MCP tool (b) Completes the read+write story. Users expect to write memories through MCP. 8-16 hours Internal development LLM backend flexibility (f) \u2014 OpenAI + Anthropic + Ollama \"No vendor lock-in\" positioning requires multi-backend support at launch. 12-20 hours LiteLLM or custom adapter layer Logo \u2014 clean geometric SVG with variants Every link, social share, and directory listing displays a logo. No logo = forgettable. 4-8 hours (Figma DIY) or $200-500 (Fiverr/designer) Figma, Inkscape Comparison table in README (vs mem0, vs official server-memory) Developers deciding between options. Tables make the decision instant. 2-3 hours Markdown table Architecture diagram (Mermaid or SVG) Shows how BrainLayer fits between agents and storage. Builds understanding fast. 2-3 hours Mermaid, Excalidraw, or Figma awesome-mcp-servers PR submission 79.9K stars = massive visibility. PR process is straightforward. 1-2 hours GitHub PR following CONTRIBUTING.md Submit to 5+ MCP directories Each directory serves a different audience segment. 3-4 hours PulseMCP, mcp.so, Glama, Smithery, mcpservers.org aitmpl.com / Claude Code Templates submission 500K+ npm downloads, 20.9K stars. Direct competitor channel to memory-integration. 2-3 hours GitHub PR Discord server Every successful AI tool (mem0, ChromaDB, LangChain, Haystack) launched with Discord. 2-3 hours Discord (free) Launch blog post \u2014 tutorial format \"How to give AI agents persistent memory in 5 minutes\" outperforms announcements. 4-6 hours Hashnode (custom domain) or own site Pre-commit hooks (Ruff, Mypy, Gitleaks) Contributor experience + security hygiene. 1-2 hours pre-commit framework Output schemas (<code>structuredContent</code>) for key tools Modern MCP spec feature. Enables typed downstream processing of results. 4-6 hours MCP SDK"},{"location":"research/launch-readiness-audit/#nice-to-have-can-come-in-v11","title":"NICE TO HAVE \u2014 can come in v1.1","text":"Item Why it matters Effort Tools/services Session-level enrichment (a) Adds intelligence to memory management but increases scope. Ship after user feedback. 16-24 hours Internal development Migration guide from official server-memory Warmest audience = developers already using the basic MCP memory server. 4-6 hours Docs page with step-by-step Versioned docs via mike plugin Important once you have multiple releases. Not needed at v1.0. 2-3 hours mike MkDocs plugin Integration examples (LangChain, CrewAI, LlamaIndex) Expands BrainLayer's distribution through framework ecosystems. 8-12 hours Per-framework code examples SBOM generation Enterprise credibility signal. Executive Order compliance. 2-3 hours cyclonedx-bom in CI Automated changelog generation Professional release hygiene. 2-3 hours python-semantic-release or release-drafter YouTube demo video (2-3 min) Long-form discoverability. Embeddable in blog posts and directories. 4-6 hours OBS Studio, simple editing Streamable HTTP transport (in addition to stdio) Required for remote deployment and some IDE integrations. 8-12 hours MCP SDK HTTP transport"},{"location":"research/launch-readiness-audit/#post-launch-roadmap","title":"POST-LAUNCH roadmap","text":"Item Why it matters Effort Tools/services Document archiving (c) Power-user feature for managing memory lifecycle. 12-20 hours Internal development Graph memory support Market trend: mem0, Zep, Letta all have graph-based memory. Becoming table stakes. 40-60 hours Neo4j or custom graph layer Temporal awareness (bi-temporal model) Zep/Graphiti pioneered this. Tracks when events occurred vs. when ingested. 30-40 hours Internal development Cross-client memory sharing Users expect memory to travel across Cursor \u2192 Claude \u2192 VS Code. 20-30 hours Shared storage backend Separate landing page (brainlayer.dev) Needed when non-developer audience grows or pricing/cloud tier is added. 16-24 hours Astro or Next.js Framework integrations as first-class packages Distribution through LangChain/CrewAI/LlamaIndex ecosystems. 20-30 hours Per-framework packages Benchmarks (search quality, latency, memory accuracy) Credibility through data. Mem0 published benchmarks; BrainLayer should too. 12-16 hours LOCOMO benchmark, custom eval suite Multi-user/multi-agent scoping Enterprise requirement. Mem0 has user_id/agent_id/run_id scoping. 20-30 hours Internal development Product Hunt launch (with prepared assets) 500K+ possible reach. Save for when the product is polished and demo-ready. 8-12 hours prep Product Hunt, recruit a Hunter"},{"location":"research/launch-readiness-audit/#competitive-positioning-the-three-sentence-pitch","title":"Competitive positioning: the three-sentence pitch","text":"<p>For developer audiences: \"The official MCP memory server stores notes in a JSON file. Mem0 requires an OpenAI API key and a cloud account. BrainLayer gives your AI agents real memory \u2014 hybrid semantic search, LLM enrichment, 268K+ chunks \u2014 fully open source, fully local, fully MCP-native.\"</p> <p>This pitch works because it acknowledges the two alternatives developers already know, states their limitations factually, and positions BrainLayer as the solution without superlatives. Lead with this framing in the README, Hacker News post, and Twitter launch thread. The comparison table does the rest.</p> <p>The total estimated effort for all MUST HAVE items is 42-61 hours. Adding all SHOULD HAVE items brings the total to roughly 90-130 hours. For a solo developer, this represents 2-3 weeks of focused work. The MUST HAVE list alone produces a credible, professional launch. The SHOULD HAVE list transforms it from credible to compelling.</p>"},{"location":"research/session-enrichment/","title":"Session Enrichment Research","text":"<p>Research collected Feb 2026 for BrainLayer's session-level enrichment feature.</p>"},{"location":"research/session-enrichment/#context","title":"Context","text":"<p>BrainLayer currently has chunk-level enrichment (summary, tags, importance, intent per ~2000-char chunk). Session-level enrichment analyzes full conversations as a unit to extract things invisible from individual chunks: corrections, decisions, patterns, frustrations, learnings.</p>"},{"location":"research/session-enrichment/#research-files","title":"Research Files","text":"File Topic Source prompts.md 5 research prompts used for web AI research Claude Code session-enrichment-architecture.md Schema design, pipeline, metadata fields Web AI research brainstore-write-tools.md Write-side MCP tool (<code>brainlayer_store</code>) design Web AI research conversation-reconstruction.md JSONL fork/branch detection, UUID tree reconstruction Web AI research auto-extracting-learnings.md Mining corrections and patterns from 268K chunks Web AI research cli-ux-patterns.md CLI display design with rich library Web AI research"},{"location":"research/session-enrichment/#key-findings","title":"Key Findings","text":""},{"location":"research/session-enrichment/#session-jsonl-structure","title":"Session JSONL Structure","text":"<ul> <li>Claude Code sessions use <code>uuid</code>/<code>parentUuid</code> forming a tree</li> <li>~92% of messages are parallel tool events (progress + tool_result), not real conversation branches</li> <li>Real conversation = trace <code>parentUuid</code> from last message to root</li> <li>Compaction events marked by \"This session is being continued from a previous conversation\"</li> <li>Real user rewinds (checkpoints) are rare (~5-8 per long session)</li> </ul>"},{"location":"research/session-enrichment/#two-layer-enrichment","title":"Two-Layer Enrichment","text":"<ol> <li>Chunk-level (exists): summary, tags, importance, intent per chunk</li> <li>Session-level (proposed): corrections, decisions, patterns, learnings per conversation</li> </ol>"},{"location":"research/session-enrichment/#brainstore-write-tool","title":"brainStore Write Tool","text":"<ul> <li>Quick MCP tool for any Claude Code session to store ideas, mistakes, decisions, notes</li> <li>Replaces golems' <code>/learn-mistake</code> skill with proper database-backed system</li> <li>Available to ALL Claude Code sessions, not just golems</li> </ul>"},{"location":"research/session-enrichment/#next-steps","title":"Next Steps","text":"<ul> <li>Write formal design doc in <code>brainlayer/docs/session-enrichment-design.md</code></li> <li>Implement session reconstruction algorithm</li> <li>Add <code>brainlayer_store</code> MCP tool</li> <li>Build session enrichment pipeline (Gemini Flash for long sessions)</li> </ul>"},{"location":"research/session-enrichment/auto-extracting-learnings/","title":"Extracting learnings from 268K Claude Code conversation chunks","text":"<p>A five-stage pipeline combining FTS5 keyword filtering, context window assembly, LLM classification, embedding-based clustering, and confidence-scored rule generation delivers the best signal-to-noise ratio for mining actionable rules from your SQLite database. No single approach works alone \u2014 keyword mining catches only 60\u201380% of corrections with high false-positive rates, while full LLM classification of all 268K chunks is needlessly expensive. The hybrid pipeline narrows 268K chunks to ~5\u201315K candidates cheaply via SQL, then spends LLM compute only where it matters. The entire pipeline runs locally on a 32GB laptop in under 8 hours, with no sampling required.</p> <p>This report synthesizes research across correction detection in dialogue systems, embedding models for mixed code/NL content, clustering algorithms for short text, existing tools in this space, and practical SQLite patterns \u2014 all oriented toward auto-generating CLAUDE.md instruction files from conversation history.</p>"},{"location":"research/session-enrichment/auto-extracting-learnings/#the-five-stage-pipeline-that-maximizes-signal-over-noise","title":"The five-stage pipeline that maximizes signal over noise","text":"<p>The core insight from dialogue systems research is that corrections exist on a spectrum from explicit (\"No, use TypeScript\") to implicit (user silently redoes what the AI produced). No single technique captures the full range. The optimal architecture is a progressive funnel:</p> <p>Stage 1 \u2014 SQL/FTS5 pre-filter (268K \u2192 15\u201325K chunks, milliseconds). Use SQLite's FTS5 with porter stemming to identify chunks containing correction signals. This is essentially free computationally \u2014 FTS5 handles 268K rows in sub-millisecond query times after a 1\u20133 second index build. Combine keyword matches with metadata filters: <code>content_type = 'user_message'</code>, <code>importance &gt;= 5</code>, and <code>intent IN ('debugging', 'reviewing', 'deciding', 'configuring')</code>. This stage has roughly 30\u201350% precision but 60\u201380% recall \u2014 it casts a wide net.</p> <p>Stage 2 \u2014 Context window assembly (15\u201325K \u2192 15\u201325K windows, seconds). This is where you solve the \"no RTL\" problem. For each candidate chunk, pull a window of 5 chunks before and 5 after within the same session using <code>ROW_NUMBER() OVER (PARTITION BY source ORDER BY created_at, rowid)</code>. The conversation triplet \u2014 prior assistant response, user correction, subsequent assistant acknowledgment \u2014 provides the semantic context that makes individual chunks interpretable. Without this step, a user message like \"no, RTL\" is noise; with the preceding assistant text showing a left-to-right layout, it becomes a clear correction.</p> <p>Stage 3 \u2014 LLM classification (15\u201325K windows \u2192 3\u20138K learnings, hours). Feed each context window to a local LLM (Mistral 7B or Llama 3.1 8B via Ollama) with a few-shot prompt classifying into six categories: CORRECTION, PREFERENCE, RULE, ARCHITECTURE_DECISION, TOOL_PREFERENCE, or NORMAL. Research shows Mistral 7B achieves \u03ba &gt; 0.8 agreement with humans on dialogue act classification tasks. At ~500 classifications per minute on consumer hardware, 15K windows processes in about 30 minutes. This stage lifts precision to 75\u201390%.</p> <p>Stage 4 \u2014 Embedding and clustering (3\u20138K learnings \u2192 50\u2013200 clusters, minutes). Embed classified learnings using EmbeddingGemma-300M (truncated to 256 dimensions), reduce with UMAP, and cluster with HDBSCAN via BERTopic. This surfaces repeated patterns \u2014 five separate \"use bun not npm\" corrections across different sessions collapse into one high-confidence cluster. Label clusters automatically with c-TF-IDF + KeyBERTInspired representations.</p> <p>Stage 5 \u2014 Confidence scoring and rule generation. Score each cluster: single mentions start at 0.4 confidence, explicit corrections score 0.7, and repeat observations across sessions compound to 0.85\u20130.95. Older learnings decay unless reinforced. Generate CLAUDE.md from clusters exceeding 0.7 confidence, keeping the file under 150 actionable rules.</p>"},{"location":"research/session-enrichment/auto-extracting-learnings/#distinguishing-genuine-corrections-from-normal-conversation","title":"Distinguishing genuine corrections from normal conversation","text":"<p>The correction detection problem has been studied extensively in conversational analysis, starting with Schegloff, Jefferson, and Sacks' foundational work on conversational repair. In AI assistant dialogues, corrections follow a specific sequential pattern called Third Position Repair: the user says something, the AI misinterprets and responds, and the user corrects in their next turn. This triplet structure is your strongest signal.</p> <p>Signal words divide into three reliability tiers. Tier 1 (highest precision): contrastive patterns like \"not X, use Y\", \"X instead of Y\"; identity markers like \"I said\", \"I already told you\"; and explicit negation \"that's wrong\", \"that's incorrect\". Tier 2 (good precision, broad coverage): sentence-initial \"No,\" followed by an instruction, \"Actually,\" as a correction marker, imperative \"Don't\" + verb, and rule-establishment words \"always\", \"never\". Tier 3 (high recall, more noise): standalone \"wrong\", \"I prefer\", \"instead\" in isolation.</p> <p>The critical distinguisher is position + context. A \"no\" at the start of a user turn immediately following an assistant response is far more likely to be a correction than \"no\" embedded within a sentence. False positives cluster around rhetorical negation (\"no problem\"), agreement with negation (\"no, that's fine\"), and discussion of errors in code (\"no such file or directory\"). Your FTS5 queries should combine signal words with the sequential pattern check \u2014 require that the matched chunk follows an <code>assistant_text</code> or <code>ai_code</code> chunk within the same session.</p> <p>For the LLM classification stage, the most effective prompt structure provides 2\u20133 examples per category (10\u201315 total), includes the preceding assistant context, and uses a structured JSON output format. Research from ACL 2025 shows that even 7B parameter models handle this classification task well when given clear few-shot examples, and that placing the most critical examples last in the prompt slightly improves performance.</p>"},{"location":"research/session-enrichment/auto-extracting-learnings/#embedding-and-clustering-268k-chunks-locally","title":"Embedding and clustering 268K chunks locally","text":"<p>EmbeddingGemma-300M is the strongest recommendation for this use case. Released by Google in September 2025, it ranks #1 on MTEB benchmarks among models under 500M parameters specifically on code retrieval tasks \u2014 critical since your chunks mix natural language with code. It supports Matryoshka representations (truncatable to 256/128 dimensions without retraining), runs in under 200MB RAM with int4 quantization, and integrates directly with sentence-transformers. For your 268K chunks at 256 dimensions, total embedding storage is 275MB \u2014 trivial.</p> <p>The alternative is nomic-embed-text-v1.5 if any chunks exceed 2,048 tokens (nomic supports 8,192). For maximum speed on a CPU-only machine, all-MiniLM-L6-v2 embeds at 5\u201314K sentences/second but sacrifices quality.</p> <p>For clustering, BERTopic provides the best end-to-end pipeline: embeddings \u2192 UMAP (n_components=5, not 2) \u2192 HDBSCAN (min_cluster_size=30, min_samples=10) \u2192 c-TF-IDF for topic representation. HDBSCAN is preferred over K-means because it automatically determines cluster count and handles noise points \u2014 many chunks genuinely don't cluster, and forcing them into clusters pollutes results. Expect HDBSCAN to mark 50\u201374% of short text as outliers; use BERTopic's <code>reduce_outliers(strategy=\"embeddings\")</code> to reassign borderline cases.</p> <p>A critical optimization: apply PCA from 768\u219250 dimensions before UMAP for high-dimensional embeddings. This two-stage reduction, recommended by UMAP's creator Leland McInnes, removes noise and dramatically accelerates UMAP on large datasets.</p> <p>Regarding compute requirements: 268K is very manageable \u2014 no sampling needed. The full pipeline on a 32GB laptop peaks at roughly 12\u201317GB RAM. Embedding takes 3\u20136 hours on CPU with EmbeddingGemma (30\u201360 minutes with GPU), UMAP takes 15\u201345 minutes, and HDBSCAN takes 5\u201330 minutes. Total end-to-end: 4\u20138 hours CPU, under 2 hours GPU. On a 16GB machine, use all-MiniLM-L6-v2 at 384 dimensions with <code>low_memory=True</code> flags.</p>"},{"location":"research/session-enrichment/auto-extracting-learnings/#what-existing-tools-get-right-and-where-the-gap-is","title":"What existing tools get right and where the gap is","text":"<p>No existing tool automatically extracts learnings from AI coding conversation history to generate instruction files. This is a genuine gap. The closest systems approach the problem from different angles:</p> <p>Mem0 (raised $24M, October 2025) provides the most relevant extraction architecture. Its two-phase pipeline \u2014 extraction (LLM identifies candidate memories from conversations) then update (compare against existing memories, choose ADD/UPDATE/DELETE/NO-OP) \u2014 maps directly onto the correction mining problem. Its custom instructions feature lets you specify \"extract coding preferences, library choices, tool preferences\" while excluding noise. Mem0 reports 26% higher accuracy than OpenAI's memory and 90% fewer tokens on the LOCOMO benchmark.</p> <p>Zep/Graphiti offers the gold standard for temporal contradiction resolution. Every fact tracks four timestamps: creation time, expiry time, validity start, and validity end. When a new learning contradicts an existing one (detected via LLM comparison against semantically similar entries), the old fact gets its <code>t_invalid</code> set \u2014 new information always wins, but history is preserved. This directly solves your \"user preferences change over time\" problem.</p> <p>LangMem from LangChain provides the clearest model for procedural memory \u2014 rules, style guides, behavioral patterns \u2014 which maps directly to CLAUDE.md generation. Its <code>metaprompt</code> algorithm reflects on conversation feedback and proposes prompt/instruction updates, essentially automating rule refinement.</p> <p>Among coding-specific tools, Pro-Workflow is the most mature learning capture system, with <code>/learn</code>, <code>/learn-rule</code>, and <code>/search</code> commands, correction heatmaps tracking which categories get corrected most, and adaptive quality gates. Agentdex indexes conversations from Cursor, Claude Code, and Codex into LanceDB for semantic search. Claude-mem auto-captures Claude Code session activity and generates CLAUDE.md files with activity timelines. However, all operate on individual session capture \u2014 none batch-processes historical conversation logs to extract cross-session patterns.</p> <p>Claude Code's own memory system (v2.1.32+) now includes auto-memory at <code>~/.claude/projects/&lt;project&gt;/memory/MEMORY.md</code> and session memory extraction, but this captures forward-looking notes, not retrospective pattern mining across hundreds of sessions.</p>"},{"location":"research/session-enrichment/auto-extracting-learnings/#sqlite-patterns-that-make-context-windows-practical","title":"SQLite patterns that make context windows practical","text":"<p>The key query pattern for your database uses <code>ROW_NUMBER</code> with <code>PARTITION BY source</code> to establish chunk ordering within sessions, then a CTE to pull windows around target chunks:</p> <pre><code>WITH numbered AS (\n  SELECT rowid, content, content_type, source, created_at,\n    ROW_NUMBER() OVER (PARTITION BY source ORDER BY created_at, rowid) AS seq\n  FROM chunks\n),\ntargets AS (\n  SELECT rowid, source, seq FROM numbered\n  WHERE content_type = 'user_message' AND (\n    content LIKE 'No,%' OR content LIKE 'Actually%'\n    OR content LIKE '%instead of%' OR content LIKE '%should be%')\n)\nSELECT n.*, t.rowid AS target_id, n.seq - t.seq AS offset\nFROM targets t\nJOIN numbered n ON n.source = t.source\n  AND n.seq BETWEEN t.seq - 5 AND t.seq + 5\nORDER BY t.rowid, n.seq;\n</code></pre> <p>For the diff analysis approach, correlate user requests with nearby git_diff chunks using <code>LEAD</code> window functions to find the sequence <code>user_message \u2192 ai_code \u2192 user_message (correction) \u2192 git_diff</code>. This captures cases where the user's correction led to a code change \u2014 high-signal evidence of a genuine preference.</p> <p>Essential indexes: <code>CREATE INDEX idx_chunks_source_time ON chunks(source, created_at, rowid)</code> is critical for window function performance. Add covering indexes on <code>content_type</code>, <code>importance</code>, and <code>intent</code> for the pre-filter stage. For JSON tags, if you query them frequently, normalize into a <code>chunk_tags</code> join table \u2014 <code>json_each()</code> cannot use indexes and scans each row's JSON on every query.</p> <p>Set SQLite pragmas for read-heavy batch processing: <code>PRAGMA journal_mode=WAL</code> (concurrent reads), <code>PRAGMA cache_size=-64000</code> (64MB cache), <code>PRAGMA mmap_size=268435456</code> (256MB mmap). With these settings, 268K rows is modest \u2014 all queries run in milliseconds to low seconds.</p>"},{"location":"research/session-enrichment/auto-extracting-learnings/#structuring-output-for-automatic-claudemd-generation","title":"Structuring output for automatic CLAUDE.md generation","text":"<p>The learnings table should track seven essential dimensions: <code>rule_text</code> (the actionable instruction), <code>category</code> (correction/preference/rule/architecture/tool_preference), <code>confidence</code> (0.0\u20131.0, compounds with evidence), <code>frequency</code> (observation count), <code>first_seen</code>/<code>last_seen</code> (temporal bounds), <code>evidence_chunks</code> (JSON array of source chunk rowids for auditability), and <code>status</code> (candidate/confirmed/superseded). The <code>superseded_by</code> foreign key handles temporal evolution \u2014 when a user switches from npm to bun, the npm preference gets status='superseded' with a pointer to the bun preference.</p> <p>For CLAUDE.md generation, filter to <code>confidence &gt;= 0.7</code> and <code>status = 'confirmed'</code>, group by category, and cap at 150 rules total \u2014 research from HumanLayer shows frontier models follow roughly 150\u2013200 instructions reliably, and Claude Code's system prompt already consumes ~50 of that budget. Use the <code>.claude/rules/</code> directory with per-topic <code>.mdc</code> files for higher granularity: <code>corrections.mdc</code>, <code>code-style.mdc</code>, <code>testing.mdc</code>, <code>architecture.mdc</code>, each with YAML frontmatter specifying applicable file paths.</p> <p>The confidence scoring formula should weight recency and cross-session repetition heavily: a single correction scores 0.4, an explicit \"always/never\" rule scores 0.7, repetition across 3+ sessions compounds to 0.85\u20130.95, and learnings not reinforced within 90 days decay by 0.1 per month. This naturally surfaces durable preferences while letting situational corrections fade.</p>"},{"location":"research/session-enrichment/auto-extracting-learnings/#conclusion","title":"Conclusion","text":"<p>The most important insight from this research is that context windows are non-negotiable \u2014 individual chunks are nearly uninterpretable for correction detection, but a 5-chunk window centered on a signal word achieves 75\u201390% precision when combined with LLM classification. The second key finding is that the 268K dataset is small enough to process entirely on a developer laptop without sampling, using EmbeddingGemma-300M and BERTopic. Third, the temporal contradiction problem is solved by Zep/Graphiti's four-timestamp model \u2014 adopt this pattern for your learnings table rather than inventing a new approach.</p> <p>The critical gap in existing tooling is the batch retrospective analysis of conversation history. Tools like Mem0 and LangMem extract memories forward (during conversations), while your use case requires mining patterns backward across 800+ historical sessions. The five-stage pipeline described here bridges that gap. Start with Stage 1 (FTS5 keyword filter) and Stage 2 (context windows) \u2014 these two stages alone, implementable in pure SQL in an afternoon, will surface the highest-signal corrections. Add the LLM classification and clustering stages incrementally as you validate the approach on initial results.</p>"},{"location":"research/session-enrichment/brainstore-write-tools/","title":"Brainstore write tools","text":"<p>Persistent Cognition in Agentic Workflows: The BrainLayer Protocol for State-Aware DevelopmentThe evolution of agentic artificial intelligence has moved beyond simple instruction-following towards a paradigm of autonomous goal-pursuit, where agents utilize multi-step reasoning, adaptive tool selection, and sophisticated planning to navigate complex software environments. However, the efficacy of these agents remains significantly constrained by the inherent statelessness of the large language models (LLMs) that drive them. Every session typically begins from a state of total amnesia, requiring the agent to rebuild context from scratch, which not only increases token costs and latency but also degrades the quality of long-term architectural decision-making. The development of BrainLayer as an open-source memory layer, implemented via the Model Context Protocol (MCP), addresses this deficit by providing a standardized interface for persistent storage and retrieval of cognitive artifacts. The introduction of the brainlayer_store write-side tool marks a critical transition from reactive context retrieval to proactive knowledge management, allowing agents to externalize feature ideas, mistake corrections, and architectural decisions into a durable, queryable database.I. Persistent Storage Architecture: SQLite and the Relational FoundationThe BrainLayer architecture is anchored by a Python-based MCP server utilizing the mcp library and the apsw (Another Python SQLite Wrapper) database engine. This combination provides a high-performance, low-level interface to SQLite, which is essential for managing a database that already encompasses 268,000 chunks and approximately 3.8 gigabytes of data. SQLite is particularly well-suited for local agentic memory due to its single-file format, which ensures that a developer\u2019s entire history remains portable and easily manageable within their local environment.Unified vs. Federated Schema DesignsA central architectural question for the brainlayer_store tool is whether new developer-generated artifacts should reside in the existing chunks table or a specialized notes or store table. In many Retrieval-Augmented Generation (RAG) systems, a unified table structure is preferred for simplicity. However, as the volume of information scales, the lack of distinction between static technical documentation (raw code chunks) and dynamic developer insights (decisions, mistakes) can lead to semantic noise during retrieval.Research into structure-augmented generation suggest that \"structurable data\"\u2014unstructured text that contains rich patterns and implicit relationships\u2014benefits from more granular organization. Relational databases enable computational efficiency and explainability, which LLMs struggle to maintain independently. For BrainLayer, a dedicated brain_store table is recommended to isolate high-value developer notes from the voluminous but lower-density code chunks. This separation allows the system to apply different similarity thresholds and priority weighting to user-generated insights, which are often more predictive of future developer needs than the raw code itself.FeatureUnified Table (chunks)Federated Table (brain_store)Data IntegrityHigh risk of name collisions or schema bloat.Clean isolation of metadata specific to developer intent.Search PerformanceSingle index scan, but higher noise floor.Targeted searches on high-relevance developer artifacts.Metadata FilteringRequires complex conditional logic in the WHERE clause.Simplified filtering by project, type, and priority.ScalabilityFaster initial setup but harder to prune stale data.Enables independent decay and consolidation policies.The proposed brain_store table should utilize the vec0 virtual table mechanism from sqlite-vec to handle 1024-dimensional embeddings from the bge-large-en-v1.5 model. This allows for the storage of metadata columns\u2014such as the category of the note and the specific project ID\u2014as part of the virtual table's primary structure, enabling efficient metadata-filtered K-Nearest Neighbor (KNN) queries. Partition keys, such as project_id, should be used to internally shard the vector index, which can improve query speeds by up to 3x by narrowing the search to specific project boundaries before vector comparison begins.II. Temporal Dynamics and the Embedding PipelineThe integration of brainlayer_store necessitates a strategic decision regarding the timing of embedding generation. When a developer or an agent stores a new idea or mistake, the system must generate a 1024-dimensional vector using the sentence-transformers library (specifically the bge-large-en-v1.5 model). This process is computationally intensive and can introduce latencies of 1 to 2 seconds on standard consumer hardware.Write-Time Synchronicity vs. Background ProcessingSynchronous embedding generation (at write-time) offers the benefit of immediate consistency; as soon as the brainlayer_store tool returns a success message, the item is available for semantic search. This is critical for agentic workflows where a model might need to recall a decision it made only moments prior. However, the latency penalty of synchronous calls can disrupt the \"vibe coding\" experience\u2014a term increasingly used to describe the high-velocity, intuitive interaction between developers and AI coding assistants.MetricSynchronous WriteAsynchronous QueueLatency (Perceived)High (1000ms - 2000ms)Low (&lt;100ms)ConsistencyImmediate (Available for search instantly)Eventual (Lag of seconds to minutes)System ReliabilitySimple, but blocks the tool thread.More resilient but requires a background worker.Hardware StrainSpikes in CPU/GPU usage during work.Smoother, controlled resource allocation.To optimize the developer experience, BrainLayer should adopt a hybrid approach. Short text entries (under 200 tokens) can be embedded synchronously to ensure immediate recall, while longer summaries or complex notes are queued for background processing. A separate SQLite table, functioning as a task queue, can store pending embeddings that are processed by a low-priority thread. This ensures the MCP server remains responsive to the primary Claude Code session while maintaining the integrity of the long-term memory store.III. Taxonomy of Developer Intent: Categorizing Cognitive ArtifactsThe efficacy of the BrainLayer memory system is determined not just by its storage capacity, but by its ability to classify information into actionable categories. A naive, append-only log of every interaction quickly becomes technical debt, creating noise that biases future decisions and produces hallucination-like effects. A well-defined taxonomy allows the agent to distinguish between temporary session context and enduring architectural principles.Core Cognitive CategoriesThe categorization of developer artifacts helps map agent memory to the four established types of human memory: working, procedural, semantic, and episodic. By using specialized types in the brainlayer_store tool signature, the system can more effectively guide Claude in deciding what is worth remembering and how it should be retrieved.Feature Ideas: These represent potential future states of the software. They are semantic in nature but carry an \"unfulfilled\" status, meaning they should be surfaced during planning phases rather than implementation phases.Mistakes and Corrections: These serve as episodic logs of what happened when things went wrong. Tracking mistakes is the \"engine of adaptation,\" allowing agents to detect, classify, and recover from failures in reasoning or tool use. Replacing a JSON-based \"/learn-mistake\" skill with a database-backed system allows for more complex queries, such as \"What common mistakes do I make when using the FastAPI library?\".Decisions (ADRs): Architectural Decision Records (ADRs) are the most critical artifacts for managing \"cognitive debt\". When developers move quickly, they often lose the \"theory of the system\"\u2014the shared understanding of why certain choices were made. Storing decisions explicitly ensures that future agents do not attempt to refactor a system in a way that contradicts its foundational design.Learnings: These capture factual knowledge acquired during work, such as \"this API is flaky, retry needed.\" This is procedural knowledge that informs future tool-use strategies.Todos: While often managed in dedicated trackers, agentic todos represent a \"working memory\" of tasks discovered during exploration.Bookmarks: These are resource-based memories that link the agent back to external documentation or specific Git history, reducing the need for the agent to re-fetch identical data repeatedly.IV. Interaction UX: Proactive Recall and Response PatternsThe Model Context Protocol defines tools as model-controlled actions, meaning the language model discovers and invokes them based on its understanding of the user's intent. For the brainlayer_store tool, the interaction should not be a one-way transaction. A well-designed memory system reduces friction by closing the loop between storage and retrieval.Proactive Retrieval PatternsWhen an agent stores a new item, the brainlayer_store tool should return more than just a confirmation of success. It should proactively check for similar existing memories to prevent duplication and encourage consistency. If a developer attempts to store a decision that contradicts an earlier record, the tool should alert the agent: \"You stored Decision X. Note that something similar (but different) was noted 3 days ago: Decision Y\".This proactive assistance is most effective at workflow boundaries\u2014such as after a successful commit or at the conclusion of a planning session. Research indicates that proactive suggestions achieve a 52% engagement rate at these boundaries, whereas mid-task interventions are dismissed 62% of the time as intrusive. By providing context-aware confirmation, BrainLayer enhances \"cognitive alignment,\" allowing the developer to maintain focus while the agent manages the background knowledge base.UX Feedback LoopsTo build trust, memory should never be a \"black box\". The user interface (in this case, the Claude Code terminal) should clearly indicate when a memory has been captured and provide the user with the ability to edit or delete it. This prevents \"AI psychosis,\" where a model builds distorted behaviors from flawed or overgeneralized recollections. Explicit controls over persistent memory, such as the ability to view, edit, or disable specific entries, are essential for long-term reliability.V. Protocol-Level Interface: The MCP Tool SignatureThe technical implementation of brainlayer_store must follow MCP best practices for tool design, emphasizing clarity, input validation, and secure execution. The tool signature must provide Claude with explicit guidance on when and how to use the store, as descriptions are the single highest-leverage factor in tool performance.Tool Signature SpecificationsThe tool name should be concise and descriptive, following either camelCase or snake_case consistently. While \"brainStore\" is functional, \"brainlayer_store\" maintains consistency with the existing naming convention for the server's tools.JSON{   \"name\": \"brainlayer_store\",   \"description\": \"Persistently stores feature ideas, mistake corrections, architectural decisions, and technical learnings. Use this to maintain context across sessions and build a long-term project knowledge base.\",   \"inputSchema\": {     \"type\": \"object\",     \"properties\": {       \"content\": {         \"type\": \"string\",         \"description\": \"The detailed content to be stored (e.g., the decision logic or error description).\"       },       \"type\": {         \"type\": \"string\",         \"enum\": [\"idea\", \"mistake\", \"decision\", \"learning\", \"todo\", \"bookmark\"],         \"description\": \"The category of the memory to facilitate specialized retrieval and decay.\"       },       \"project\": {         \"type\": \"string\",         \"description\": \"The name of the project or workspace to scope this memory.\"       },       \"tags\": {         \"type\": \"array\",         \"items\": { \"type\": \"string\" },         \"description\": \"Optional keywords for tagging and grouping related memories.\"       },       \"priority\": {         \"type\": \"integer\",         \"description\": \"Optional importance score from 1-5 to help rank retrieval.\"       }     },     \"required\": [\"content\", \"type\", \"project\"]   } } Guidance for Model InvocationTo ensure high-quality tool use, the description must explain when the agent should invoke the store. For example, the agent should be instructed to call brainlayer_store immediately after resolving a bug that required a non-obvious fix, or after reaching a consensus with the user on a project requirement. The use of \"input_examples\" in the tool definition can further guide the model in providing well-structured content, ensuring that \"mistakes\" include the symptom, the root cause, and the fix.VI. Synthesized Retrieval: Search Integration and ThresholdsA common failure mode in agentic memory is treating storage as an isolated activity from retrieval. To be useful, items stored via brainlayer_store must be seamlessly integrated into the existing brainlayer_search functionality. This requires a unified retrieval strategy that can query both the raw document chunks and the high-value developer notes.Semantic Similarity and Absolute ThresholdsWhen searching, the system uses cosine similarity to rank documents. However, the absolute value of similarity scores is often less important than the relative order of results. For BGE-large-en-v1.5, scores are typically concentrated in the [0.6, 1.0] interval, meaning a score of 0.5 does not necessarily indicate dissimilarity. BrainLayer should implement a dynamic similarity threshold:High-Confidence Retrieval: Entries with similarity &gt; 0.85 are surfaced as primary context.Exploratory Retrieval: Entries with similarity between 0.70 and 0.85 are used as \"secondary hints\" for the agent to consider.Cross-Table Reranking: Use a cross-encoder or reranker model (like bge-reranker-large) to re-rank the top-100 results retrieved from the vector search to ensure the most relevant developer note is prioritized over a generic code chunk.Metadata-Enhanced SearchThe brainlayer_search tool should be updated to accept filters based on the type and project parameters introduced in brainlayer_store. This allows an agent to ask targeted questions like \"Search all decisions related to the authentication module\" or \"Recall recent mistakes I've made with Docker\". By leveraging metadata columns in the vec0 virtual table, these filtered queries can remain highly performant even as the database grows toward 4GB and beyond.VII. Sovereignty and Trust: Security in Write-Side OperationsExposing a write-enabled tool to an AI agent introduces significant security implications. Unlike read-only tools, which risk data exfiltration, write-side tools risk data integrity and \"memory poisoning\"\u2014where a model is manipulated into storing false or malicious instructions that persist across sessions.Scoping and Multi-TenancyIn a local developer environment, the primary security concern is maintaining boundaries between different projects. A Claude Code session operating in a \"Project A\" repository should not have the ability to write to or modify the memory associated with \"Project B\".Path-Based Scoping: The MCP server should enforce that the project parameter matches the current working directory of the client.Client Consent: High-risk operations\u2014such as deleting a memory or updating a \"Decision\"\u2014must require human-in-the-loop (HITL) consent. The terminal should display the intended change and wait for user approval before persisting it to the database.Authentication: For servers that might be shared across a network, implementing OAuth 2.1 or API-key based authentication is critical to ensure that only authorized users can modify the memory store.Mitigating Memory PoisoningMemory poisoning occurs when an external actor injects unauthorized instructions or \"facts\" into an assistant's memory. This can happen via \"sleeper injections\"\u2014hidden instructions in documents or emails that an agent processes during routine work. Once poisoned, the agent treats these injected preferences as legitimate user instructions, potentially steering future actions toward harmful outcomes.Provenance Tracking: Every entry in the brain_store should include a source field identifying where the information originated (e.g., \"User Prompt\", \"External API\", \"File System\").Semantic Drift Detection: Periodic scans should compare new memories against the historical baseline to identify sudden, subtle shifts in the agent's behavior or \"preferences\" that might indicate corruption.Versioned Snapshots: Maintaining a history of changes to the brain_store allows developers to roll back to a last-known-good state if poisoning is detected.VIII. Algorithmic Synthesis: Nightly Aggregation and ConsolidationA persistent memory store that only grows will eventually become an \"append-only graveyard\" of stale information. To maintain high-quality retrieval, the system must implement a \"strategic forgetting\" or consolidation mechanism. The existing goal of replacing a JSON-based nightly clustering system with a proper database aggregation is a significant step toward \"agentic memory\"\u2014where the system itself decides what to prune or summarize.The Nightly Batch ProcessUsing the local GLM-4.7-Flash model (30B parameters), BrainLayer can perform nightly summarization and clustering of new artifacts. This model\u2019s Mixture-of-Experts (MoE) architecture delivers strong reasoning and coding performance without the heavy compute requirements of dense 30B+ models, making it ideal for local batch processing.Process StepMethodologyGoalParsingClustering algorithm \"Drain\" or template matching.Convert raw notes into structured templates for grouping.ClusteringK-Means or HDBSCAN inside severity/priority buckets.Group related ideas or recurring mistakes into logical themes.SummarizationGLM-4.7-Flash processes cluster exemplars.Condense multiple redundant notes into a single \"Canonical Memory.\"PruningEvaluate \"decay\" based on age and low similarity scores.Move low-priority, stale artifacts to an archive to reduce noise.From Log to Skill: Memory ConsolidationConsolidation transforms memory from a log of events into a \"skill\" for the agent. Instead of retrieving ten individual notes about a specific flaky API, the nightly batch process summarizes these into a single procedural rule: \"API X is flaky; always implement a 3-retry exponential backoff strategy\". By deduplicating on \"fingerprints\" and batching aggressively, the system minimizes the cost and latency of embedding generation while maximizing the clarity of the agent\u2019s context window.IX. Implementation Roadmap: Enhancing Claude Code SessionsThe integration of brainlayer_store completes the cognitive loop for Claude Code, transforming it from a powerful but transient tool into a \"senior developer\" who remembers the project's entire history. By moving away from ephemeral markdown files or JSON logs and into a structured SQLite environment, BrainLayer provides the foundation for more reliable, trust-based AI collaboration.Strategic Takeaways for DevelopmentThe move to a write-side memory tool is a prerequisite for enterprise-scale agentic systems. Success in this domain depends on:Architecture: Prioritize a federated schema with dedicated metadata for developer intent to minimize semantic noise.Performance: Implement a hybrid-async embedding pipeline to maintain terminal responsiveness while ensuring long-term consistency.Governance: Enforce strict project-based scoping and provenance tracking to protect the integrity of the knowledge base from memory poisoning attacks.Intelligence: Use local MoE models like GLM-4.7-Flash to autonomously consolidate and prune the memory store, preventing the accumulation of cognitive debt.By implementing brainlayer_store with these considerations, the BrainLayer project creates a durable cognitive framework that scales with the complexity of modern software systems, ensuring that every session builds upon the last rather than starting from zero.</p>"},{"location":"research/session-enrichment/cli-ux-patterns/","title":"CLI UX patterns for knowledge search tools","text":"<p>The best CLI search tools converge on six proven display patterns, three levels of progressive disclosure, and a composable architecture that chains simple interactive elements rather than building monolithic TUI applications. This research synthesizes patterns from ripgrep, fzf, GitHub CLI, kubectl, and dozens of knowledge management tools to provide a concrete blueprint for building a terminal-based knowledge search interface. The Python <code>rich</code> library combined with <code>click</code>/<code>typer</code>, <code>plotext</code>, and <code>iterfzf</code> forms the ideal stack. What follows is a comprehensive pattern guide with ASCII mockups, code examples, library recommendations, and prioritization frameworks.</p>"},{"location":"research/session-enrichment/cli-ux-patterns/#six-foundational-display-patterns-power-every-great-cli-search-tool","title":"Six foundational display patterns power every great CLI search tool","text":"<p>Research across ripgrep, fzf, ag, gh, bat, fd, jq, httpie, and kubectl reveals six primary layout patterns that cover virtually all search result display needs:</p> <p>Pattern 1 \u2014 Grouped-by-file with context (ripgrep, ag). Results cluster under filename headers with line numbers and highlighted matches. Blank lines separate file groups. This is the default TTY mode for both <code>rg</code> and <code>ag</code>, and the most natural pattern for code/content search:</p> <pre><code>src/components/SearchBar.tsx\n 12:  function handleSearch(query: string) {\n 13-    const results = performSearch(query);\n 14-    setResults(results);\n\nsrc/utils/api.ts\n 45:  export async function handleSearch(q) {\n 46-    const resp = await fetch(`/api?q=${q}`);\n</code></pre> <p>File paths render in magenta bold, line numbers in green, match text in red bold, and context lines appear dimmed with <code>-</code> separators. Match lines use <code>:</code> separators. The <code>--</code> divider separates non-adjacent context blocks within the same file.</p> <p>Pattern 2 \u2014 Aligned tabular (kubectl, gh). Columnar data with aligned headers and whitespace-padded columns. GitHub CLI's <code>gh search repos</code> and kubectl's default output both use this for metadata-rich listings:</p> <pre><code>NAME                  DESCRIPTION                   \u2605      LANG   UPDATED\nBurntSushi/ripgrep    Recursively search director\u2026   48.2k  Rust   2d ago\nsharkdp/fd            A simple, fast alternative\u2026    35.1k  Rust   1w ago\nsharkdp/bat           A cat clone with wings         51.2k  Rust   5d ago\n</code></pre> <p>Pattern 3 \u2014 Interactive filtered list (fzf). Real-time filtering with a prompt, scrollable list, match highlighting, and an optional preview pane. The user types to narrow results dynamically, with a <code>7/245</code> info line showing matches from total candidates.</p> <p>Pattern 4 \u2014 Rich metadata cards. A composite pattern combining panels with inline badges, scores, tags, timestamps, and code snippets \u2014 ideal for knowledge search results:</p> <pre><code>\u256d\u2500 1. auth/middleware.ts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Score: 0.95 \u2500\u2500 2 days ago \u2500\u256e\n\u2502  Tags: [auth] [middleware] [security]    Category: Backend            \u2502\n\u2502                                                                       \u2502\n\u2502  14 \u2502 export function authMiddleware(req, res, next) {                \u2502\n\u2502  15 \u2502   const token = req.headers.authorization;                      \u2502\n\u2502  16 \u2502   if (!validateToken(token)) {                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Pattern 5 \u2014 Compact table with expandable rows. A hybrid that shows a scored, sortable table with the ability to expand individual results in-place \u2014 the best default for knowledge search:</p> <pre><code> #  FILE                        LINE  SCORE  UPDATED     TAGS\n 1  src/db/connection.ts          23  0.97   2d ago      db,core\n 2  src/db/pool.ts                45  0.91   1w ago      db,pool\n 3  tests/db/connection.test.ts   12  0.85   3d ago      db,test\n\n 5 of 23 results | [Enter] expand | [/] filter | [n] next page\n</code></pre> <p>Pattern 6 \u2014 Structured data output (jq, httpie). Pretty-printed JSON/YAML with indentation and syntax coloring, used when piping to other tools. Every well-designed CLI should auto-detect TTY versus pipe and switch between human-readable and machine-readable output accordingly.</p> <p>A universal design principle emerges: all excellent CLI tools default to human-friendly output on TTY and machine-friendly output when piped. The <code>NO_COLOR</code> env var and <code>--color=auto|always|never</code> flag are now standard conventions.</p>"},{"location":"research/session-enrichment/cli-ux-patterns/#building-with-rich-components-patterns-and-responsive-layouts","title":"Building with Rich: components, patterns, and responsive layouts","text":"<p>The Python <code>rich</code> library (55K+ GitHub stars) provides composable renderables that nest freely \u2014 the \"Lego brick\" approach. The key insight is matching Rich components to specific display needs.</p> <p>For list views, <code>rich.table.Table</code> auto-resizes columns to fit terminal width. Use <code>ratio</code> for proportional columns and <code>no_wrap=True</code> for titles. Colored badges are created with console markup: <code>[bold white on blue] tag [/]</code>. Score badges can use conditional coloring \u2014 green for high (&gt;0.8), yellow for medium (0.5\u20130.8), red for low (&lt;0.5). Row styles like <code>row_styles=[\"\", \"dim\"]</code> create zebra striping for scanability.</p> <p>For detail views, <code>rich.panel.Panel</code> creates bordered cards with title and subtitle. Nest any renderable inside \u2014 tables, syntax-highlighted code via <code>rich.syntax.Syntax</code>, markdown via <code>rich.markdown.Markdown</code>, or compose multiple elements with <code>rich.console.group()</code>. The <code>Panel.fit()</code> method auto-sizes to content, while <code>Panel(expand=True)</code> fills the terminal width.</p> <p>For hierarchical/categorized results, <code>rich.tree.Tree</code> provides visual hierarchy with guide lines \u2014 perfect for results grouped by project, tag, or category:</p> <pre><code>\ud83d\udd0d Search Results\n\u251c\u2500\u2500 Python (5 results)\n\u2502   \u251c\u2500\u2500 Getting Started with Rich \u2014 Score: 95%\n\u2502   \u2514\u2500\u2500 Advanced CLI Patterns \u2014 Score: 82%\n\u2514\u2500\u2500 Design (3 results)\n    \u2514\u2500\u2500 Terminal UX Best Practices \u2014 Score: 78%\n</code></pre> <p>For streaming results, <code>rich.live.Live</code> provides flicker-free live updates. Combine with <code>Layout</code> for dashboard-style multi-pane displays. The <code>Live</code> context manager supports printing above the live display and alternate-screen mode for fullscreen applications.</p> <p>A critical responsive pattern uses <code>Console.width</code> for adaptive layouts:</p> <pre><code>console = Console()\nif console.width &gt;= 120:\n    # Wide: multi-column card layout with Columns\n    console.print(Columns(cards, equal=True))\nelif console.width &gt;= 80:\n    # Medium: table layout with full metadata\n    console.print(make_table(results))\nelse:\n    # Narrow: stacked compact cards, heavy truncation\n    for r in results:\n        console.print(make_compact_card(r))\n</code></pre> <p>Rich auto-detects terminal capabilities (color depth, width, interactivity) and degrades gracefully. When piping to a file, it strips ANSI control codes automatically. Custom themes centralize styling decisions:</p> <pre><code>from rich.theme import Theme\ncustom_theme = Theme({\n    \"result.title\": \"bold cyan\",\n    \"result.score.high\": \"bold green\",\n    \"result.score.low\": \"red\",\n    \"tag\": \"bold white on dark_blue\",\n    \"meta\": \"dim italic\",\n})\n</code></pre> <p>Notable open-source projects demonstrating Rich's capabilities include rich-cli (file viewing with syntax highlighting), Memray (Bloomberg's memory profiler), Toolong (log file viewer), and ghtop (GitHub activity dashboard).</p>"},{"location":"research/session-enrichment/cli-ux-patterns/#command-hierarchy-the-search-filter-drill-down-workflow","title":"Command hierarchy: the search \u2192 filter \u2192 drill-down workflow","text":"<p>The most effective knowledge base CLI tools follow the <code>APPNAME VERB NOUN --ADJECTIVE</code> pattern (Cobra convention). Research across <code>nb</code>, <code>kb</code>, <code>tldr</code>, <code>cheat.sh</code>, and <code>howdoi</code> reveals a clear ideal command structure.</p> <p>Core subcommands that every knowledge CLI needs: <code>search</code> (full-text with filters), <code>show</code>/<code>view</code> (single item detail), <code>list</code> (browse/filter), <code>tags</code> (list all tags with counts), <code>stats</code> (KB statistics and visualizations), and <code>explore</code> (interactive browsing mode). Single-letter aliases like <code>s</code>, <code>q</code>, <code>l</code> accelerate power users \u2014 <code>nb</code>'s approach of mapping <code>a</code>/<code>e</code>/<code>s</code>/<code>q</code>/<code>d</code> to add/edit/show/search/delete is proven effective.</p> <p>The progressive refinement workflow is the heart of knowledge search:</p> <pre><code># Stage 1: Broad search\nkb search \"python async\"\n\n# Stage 2: Filter results\nkb search \"python async\" --tag tutorial --after 2024-01-01 --sort score\n\n# Stage 3: Drill into specific result\nkb show 42 --related\n\n# Interactive mode (combines all stages)\nkb explore \"python async\"\n</code></pre> <p>Standard flags should be consistent across subcommands. Filtering uses <code>--tag/-t</code> (repeatable), <code>--category/-c</code>, <code>--after</code>/<code>--before</code> for dates, <code>--sort</code> (relevance|date|title), and <code>--limit/-n</code>. Output control uses <code>--json/-j</code>, <code>--format</code> (human|json|csv|plain), <code>--verbose/-v</code>, and <code>--quiet/-q</code>. The <code>--interactive/-i</code> flag or dedicated <code>explore</code> command enables fzf-style fuzzy browsing.</p> <p>Output format switching should be automatic: detect <code>sys.stdout.isatty()</code> and emit rich colored output for humans, JSON when piped. Explicit <code>--json</code> overrides this. This pattern, used by <code>gh</code>, <code>kubectl</code>, and <code>nb</code>, is the gold standard.</p> <p>The <code>nb</code> knowledge base tool stands out as the most comprehensive reference implementation with selector syntax (<code>notebook:item</code>), hierarchical tags (<code>#project/design/ui</code>), flexible identifiers (by ID, filename, title, or <code>--last</code>), and piped content support (<code>echo \"content\" | nb add</code>).</p>"},{"location":"research/session-enrichment/cli-ux-patterns/#interactive-patterns-that-avoid-full-tui-complexity","title":"Interactive patterns that avoid full TUI complexity","text":"<p>The most effective interactive CLIs compose simple elements rather than building monolithic TUI applications. Three tools define the landscape.</p> <p>fzf is the canonical interactive filter. Its genius lies in operating as a Unix filter \u2014 it takes input on stdin, provides fuzzy selection, and outputs the selected item on stdout. Key patterns include <code>--preview 'bat --color=always {}'</code> for live content preview, <code>--height 50%</code> for partial-screen operation (avoiding full TUI), and composable keybindings via <code>--bind 'ctrl-e:execute(edit {})'</code>. The <code>--layout=reverse</code> flag places the list at top with input at bottom, which feels more natural.</p> <p>gum (Charmbracelet) provides atomic interactive elements that compose in shell scripts. <code>gum filter</code> provides fzf-like fuzzy filtering, <code>gum choose</code> offers selection lists, <code>gum input</code> handles text entry, <code>gum confirm</code> adds yes/no dialogs, and <code>gum spin</code> wraps long operations with spinners. The real power is composition \u2014 chaining these into drill-down workflows:</p> <pre><code>QUERY=$(gum input --placeholder \"Search...\")\nSELECTED=$(kb search \"$QUERY\" --format plain | gum filter)\nACTION=$(gum choose \"View\" \"Edit\" \"Copy\" \"Delete\")\n</code></pre> <p>For Python, the recommended interactive libraries are:</p> <ul> <li><code>iterfzf</code> \u2014 Bundles the fzf binary, streams items lazily, supports preview and multi-select. Best UX, but depends on fzf binary</li> <li><code>InquirerPy</code> \u2014 Most actively maintained Inquirer port with a fuzzy prompt that mimics fzf. Built on prompt_toolkit. Pure Python</li> <li><code>questionary</code> \u2014 Clean API for select/text/confirm/checkbox. Also built on prompt_toolkit</li> <li><code>prompt_toolkit</code> \u2014 Low-level foundation with FuzzyCompleter, keybindings, and auto-suggestions</li> </ul> <p>The recommended stack for a Python knowledge CLI combines <code>click</code> or <code>typer</code> for command structure, <code>rich</code> for output formatting (not interactive), and <code>iterfzf</code> or <code>InquirerPy</code> for interactive selection. This separation of concerns \u2014 framework for commands, library for display, tool for interaction \u2014 produces cleaner architecture than any monolithic TUI approach.</p> <p>The drill-down pattern across all these tools follows three stages: list view (scored, filterable results) \u2192 preview (inline or side-pane content preview) \u2192 detail view (full content with metadata and actions). Standard keyboard conventions include <code>\u2191/\u2193</code> for navigation, <code>Enter</code> for selection, <code>Tab</code> for multi-select, <code>Ctrl-/</code> for toggling preview, and <code>Esc</code> for cancellation.</p>"},{"location":"research/session-enrichment/cli-ux-patterns/#terminal-visualization-without-a-tui-framework","title":"Terminal visualization without a TUI framework","text":"<p>Activity graphs, score distributions, and per-project breakdowns are achievable in pure terminal output. <code>plotext</code> (2.1K GitHub stars) is the gold standard for Python terminal plotting \u2014 it supports bar charts, heatmaps, histograms, and scatter plots with matplotlib-like syntax, zero dependencies, and official Rich integration.</p> <p>Sparklines are the highest-density visualization for inline use. A 10-line Python function using Unicode block characters (<code>\u2581\u2582\u2583\u2584\u2585\u2586\u2587\u2588</code>) creates them:</p> <pre><code>def sparkline(values):\n    chars = \"\u2581\u2582\u2583\u2584\u2585\u2586\u2587\u2588\"\n    mn, mx = min(values), max(values)\n    rng = mx - mn or 1\n    return \"\".join(chars[min(7, int((v - mn) / rng * 7))] for v in values)\n</code></pre> <p>These embed directly in Rich tables for trend columns:</p> <pre><code>Project     Notes  7-day trend   Avg Score\nresearch      42   \u2583\u2585\u2587\u2588\u2586\u2584\u2587      0.82\nwork          28   \u2585\u2583\u2586\u2587\u2585\u2584\u2583      0.74\npersonal      15   \u2581\u2583\u2582\u2585\u2587\u2583\u2586      0.69\n</code></pre> <p>GitHub-style activity heatmaps use Unicode block characters (<code>\u2591\u2592\u2593\u2588</code>) or colored spaces. <code>termgraph</code> has a built-in <code>--calendar</code> mode, and <code>plotext</code>'s <code>matrix_plot</code> handles arbitrary heatmap data. For embedding in Rich layouts, <code>termcharts</code> provides bar and pie charts that accept a <code>rich=True</code> parameter and return Rich-compatible renderables.</p> <p>Horizontal bar charts are the most effective terminal visualization for category breakdowns:</p> <pre><code>Notes by Tag\n  python     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 42\n  research   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c       28\n  terminal   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c             15\n  ux         \u2588\u2588\u2588\u2588\u258c                 9\n</code></pre> <p>The recommended visualization stack: plotext for full charts, custom sparkline function for inline trends, termcharts for Rich-integrated charts, and Rich tables with embedded Unicode for composite displays.</p>"},{"location":"research/session-enrichment/cli-ux-patterns/#prioritizing-metadata-when-every-character-counts","title":"Prioritizing metadata when every character counts","text":"<p>Terminal space is fundamentally constrained. Research from clig.dev, Nielsen Norman Group's progressive disclosure principles, and Algolia's search UX guidelines converge on a four-level metadata priority framework.</p> <p>Level 1 \u2014 Always visible (works at 60 columns): title/name (truncated with ellipsis), relevance score (compact format like <code>0.92</code> or <code>\u2605\u2605\u2605\u2605\u2606</code>), and type indicator (single icon or character code).</p> <p>Level 2 \u2014 Important (shown at 80+ columns): relative date (<code>2d ago</code>), primary tags (top 2\u20133, with <code>+N</code> overflow indicator), and source/project identifier.</p> <p>Level 3 \u2014 Contextual (shown at 120+ columns or with <code>--verbose</code>): full tag list, summary snippet (first 80 characters), file path, and word count.</p> <p>Level 4 \u2014 Detail (shown on item selection): full content preview, all metadata fields, related items, and edit history.</p> <p>Graceful degradation follows specific truncation patterns. Dates compress from <code>2025-02-15 14:30:00</code> \u2192 <code>Feb 15, 2025</code> \u2192 <code>2d ago</code> \u2192 <code>2d</code>. Tags compress from full list \u2192 <code>#python #viz +3</code> \u2192 <code>#python +4</code>. Titles truncate with trailing ellipsis. Paths use middle truncation: <code>/home/user/.../project/file.md</code>.</p> <p>Here is the same search displayed at three terminal widths:</p> <p>Wide (120 columns) \u2014 full table with all metadata columns: <pre><code> Score  Title                                        Date       Tags                   Project\n 0.95   Plotext terminal charts research notes        Feb 15     #python #plotext #cli   research\n 0.87   Rich library visualization capabilities       Feb 12     #python #rich #tui      cli-tools\n</code></pre></p> <p>Standard (80 columns) \u2014 two-line format, tags truncated with overflow count: <pre><code> \u2605\u2605\u2605\u2605\u2605 Plotext terminal charts research notes\n       Feb 15 \u00b7 #python #plotext +1 \u00b7 research\n \u2605\u2605\u2605\u2605\u2606 Rich library visualization capabilities\n       Feb 12 \u00b7 #python #rich +1 \u00b7 cli-tools\n</code></pre></p> <p>Narrow (60 columns) \u2014 single-line with heavy truncation: <pre><code>.95 Plotext terminal charts rese\u2026    2d ago\n.87 Rich library visualization \u2026    5d ago\n</code></pre></p> <p>Tools like <code>kubectl</code> use the <code>-o wide</code> flag for explicit width expansion. <code>eza</code> auto-calculates optimal grid columns from terminal width. <code>docker ps</code> truncates container IDs to 12 characters and commands with ellipsis. The pattern is consistent: fixed-width left columns for identifiers, elastic right columns for descriptions, and explicit flags for progressively wider output.</p>"},{"location":"research/session-enrichment/cli-ux-patterns/#conclusion-a-concrete-blueprint","title":"Conclusion: a concrete blueprint","text":"<p>The optimal architecture for a CLI knowledge search tool combines five layers: <code>click</code>/<code>typer</code> for command structure with <code>VERB NOUN --ADJECTIVE</code> conventions; <code>rich</code> for all output formatting using Tables (list view), Panels (detail view), and Trees (categorized view); <code>iterfzf</code> or <code>InquirerPy</code> for interactive selection without TUI overhead; <code>plotext</code> with inline sparklines for terminal visualization; and width-aware rendering at three breakpoints (60/80/120 columns).</p> <p>The most important design insight across all researched tools is that composition beats monolithism. ripgrep pipes to fzf which pipes to bat. gum chains atomic interactive elements. Rich nests renderables inside renderables. The tools that achieve the best UX are those that do one thing excellently and compose with others \u2014 and the best knowledge CLIs will follow the same principle, offering <code>--json</code> output for piping, <code>--interactive</code> mode for exploration, and rich formatted output as the human-friendly default.</p>"},{"location":"research/session-enrichment/conversation-reconstruction/","title":"Reconstructing linear conversations from tree-structured dialogue data","text":"<p>The most effective algorithm for extracting the active conversation path from tree-structured JSONL data is leaf-to-root backtracking: build a UUID index in O(n), identify the active leaf (last message by file position), then walk backward via <code>parentUuid</code> pointers to root and reverse. This approach naturally ignores the ~92% of abandoned messages, requires no fork-point heuristics during traversal, and is used by every production implementation found \u2014 from ChatGPT's export parser to the <code>cchat</code> tool for Claude Code. Fork classification, compaction stitching, and abandoned branch analysis are layered on top of this core algorithm, and a rich body of prior art from email threading, chat platforms, dialogue systems research, and version control informs each layer.</p>"},{"location":"research/session-enrichment/conversation-reconstruction/#the-canonical-path-extraction-algorithm","title":"The canonical path extraction algorithm","text":"<p>The fundamental algorithm is simple and well-established across every system that processes tree-structured conversations. The ChatGPT export format stores an explicit <code>current_node</code> pointer to the active leaf; Claude Code's JSONL does not, so the leaf must be inferred. The last message by file position in the JSONL is the most reliable heuristic for identifying the active endpoint, with latest timestamp as a fallback. From there, the walk is mechanical:</p> <pre><code>def extract_active_path(messages):\n    by_uuid = {m['uuid']: m for m in messages}\n    leaf = max(messages, key=lambda m: m.get('_line_num', 0))\n    path, current = [], leaf\n    while current:\n        path.append(current)\n        current = by_uuid.get(current.get('parentUuid'))\n    return list(reversed(path))\n</code></pre> <p>This pattern appears identically in the ChatGPT community parser (walks from <code>current_node</code> via <code>parent</code> pointers, reverses), in OpenAssistant's tree dataset tools (<code>visit_messages_depth_first</code> with predicate filtering), and in the <code>cchat</code> CLI tool for Claude Code. The alternative \u2014 root-to-leaf forward traversal with child selection heuristics \u2014 is less reliable because it must make a heuristic decision at every fork point, whereas backtracking from a known endpoint produces a guaranteed single path with O(n) index build plus O(d) traversal where d is typically only ~8% of total messages.</p> <p>For cases where the active leaf is ambiguous, three fallback heuristics exist in order of reliability: deepest leaf by path length (longest conversation is usually the active one), leaf with the latest timestamp, and leaf with the most descendants in its subtree. The topological-sort longest-path algorithm provides a mathematically sound O(V+E) solution that finds the deepest leaf without heuristics, useful when file ordering or timestamps are unreliable.</p> <p>The version control analogy is precise. Git's <code>--first-parent</code> traversal solves the same problem: extracting the \"mainline\" from a DAG by ignoring merge history. Each conversation fork is analogous to a git branch, and the active path is analogous to following first-parent pointers from HEAD to the initial commit. Git's merge-base algorithm (finding the common ancestor of two branches) directly applies to identifying where user rewinds diverged.</p>"},{"location":"research/session-enrichment/conversation-reconstruction/#classifying-forks-three-distinct-patterns-require-different-detection-logic","title":"Classifying forks: three distinct patterns require different detection logic","text":"<p>The critical insight is that most apparent fork points are not real conversation branches. In Claude Code sessions, a parent assistant message that invokes multiple tools generates multiple <code>tool_result</code> children \u2014 these share the same parent but represent parallel execution, not divergent conversation paths. Real forks fall into exactly two categories: compaction events and user rewinds.</p> <p>Parallel tool events are detected by checking whether all children of a multi-child parent are <code>tool_result</code> or progress messages with no <code>user</code>-role messages among them, and whether their timestamps cluster within seconds of each other. The <code>cchat</code> tool calls this \"mechanical fan-out\" and filters it explicitly. The detection logic checks content block types \u2014 if every child contains <code>tool_result</code> type content, the fork is parallel execution, not a branch.</p> <p>Compaction events carry explicit signals: a <code>type: \"system\"</code> with <code>subtype: \"compact_boundary\"</code>, a <code>compactMetadata</code> field, content containing \"This session is being continued from a previous conversation,\" or the presence of a <code>logicalParentUuid</code> field that links across compaction boundaries. The <code>cchat</code> tool stitches across these boundaries using <code>logicalParentUuid</code> with a positional fallback when that link is broken. Claude Code's auto-compact triggers at 75-92% context window capacity, and the compaction summary replaces the full conversation history with a condensed version.</p> <p>User rewinds are the residual category after eliminating parallel tools and compaction. The signal is multiple <code>user</code>-role messages sharing the same parent, or a new user message whose parent is an ancestor of (rather than the immediate predecessor of) the previous user message. These represent the user going back and trying a different approach. Kim et al. (2022) formalized this pattern as \"turnback utterances\" in dialogue state tracking, demonstrating that current DST models significantly degrade when users change their minds \u2014 performance only recovers when turnback scenarios are explicitly included in training data.</p> <p>A documented edge case: Claude Code issue #22526 reports \"phantom\" <code>parentUuid</code> references pointing to non-existent UUIDs, creating orphaned messages. The Claude Context Repair Tool handles this by linking orphans to the nearest valid message by timestamp, then filtering to the primary session ID.</p>"},{"location":"research/session-enrichment/conversation-reconstruction/#prior-art-spans-five-distinct-research-traditions","title":"Prior art spans five distinct research traditions","text":"<p>Email threading provides the most directly applicable algorithmic precedent. Jamie Zawinski's threading algorithm (1997), formalized in RFC 5256, builds parent-child trees from Message-ID and References headers in a five-step process: index by ID, link via references, find roots, prune empty containers, and group by subject as fallback. The core data structure \u2014 a container holding parent-child relationships indexed by unique identifier \u2014 is identical to the UUID/parentUuid conversation tree. The JWZ algorithm's handling of missing intermediate messages through \"dummy container\" nodes parallels handling broken parent chains in conversation data.</p> <p>Conversation disentanglement is a rich academic field. Elsner and Charniak's foundational work (2008, 2010) established the paradigm of predicting reply-to links in interleaved IRC chat using discourse features \u2014 speaker identity, time gaps, content similarity, and mention patterns. Kummerfeld et al. (2019) scaled this to 77,500 annotated Ubuntu IRC messages and found that reply-structure graph annotations (each message linked to its parent) form exactly the tree/forest structure present in Claude Code transcripts. Recent advances include BERT-based approaches (DialBERT, 2020), contrastive learning methods (2022), and zero-shot approaches that eliminate the need for labeled data (Chi &amp; Rudnicky, 2021). However, these methods solve the inverse problem \u2014 inferring tree structure from flat text \u2014 while the Claude Code problem starts with known structure and must select a path through it.</p> <p>Dialogue state tracking with rollbacks directly addresses the rewind problem. Kim et al. (2022) defined four types of \"turnback\" situations in their \"Oh My Mistake!\" paper and showed that injecting turnback scenarios into MultiWOZ training data recovers the performance degradation. The S3-DST framework combines joint segmentation with state tracking for long conversations, using structured prompting with pre-analytical recollection \u2014 relevant for processing sessions that span compaction boundaries.</p> <p>Branch prediction in tree-structured conversations is studied by Meital et al. (2024) in \"The Branch Not Taken,\" which defines the task of predicting whether a new comment replies to a leaf (continuing linearly) or an intermediate node (creating a branch). Their GLOBS model uses DistilBERT reply-to scores combined with structural and temporal features. Key finding: branching correlates with more participants and occurs more at earlier tree levels \u2014 a pattern that may translate to coding sessions where early architectural decisions are more likely to spawn rewinds than late implementation details.</p> <p>Chat platform engineering reveals a consistent industry convergence on shallow threading. Slack deliberately abandoned nested replies after extensive experimentation (2015-2017), settling on single-level parent-plus-flat-replies. Discord models threads as sub-channels. Matrix's MSC3440 implements single-level threads after the deeper MSC2836 proposal proved too complex for federation. Zulip uses topic-label threading with no parent-child linking at all. Reddit's open-sourced comment tree implementation uses the same adjacency-list pattern (<code>parent_id \u2192 [child_ids]</code>) with explicit orphan detection. The universal reconstruction pattern across all platforms is: build a lookup table, identify roots, walk the tree, select paths by score or pointer, reverse if walking bottom-up.</p>"},{"location":"research/session-enrichment/conversation-reconstruction/#compaction-boundaries-demand-a-hybrid-analysis-strategy","title":"Compaction boundaries demand a hybrid analysis strategy","text":"<p>The research strongly supports treating compaction events as explicit segmentation boundaries while attempting partial reconstruction across them. Three approaches exist for handling compacted segments, each with distinct tradeoffs.</p> <p>Segmented analysis treats each inter-compaction segment independently, which is semantically honest because the AI literally had different context before and after compaction. This avoids the false precision of analyzing cross-boundary patterns with data that was summarized away. JetBrains' NeurIPS 2025 workshop paper found that observation masking (hiding environment outputs while preserving reasoning history) matched LLM summarization in both cost savings and problem-solving ability \u2014 suggesting that what's lost in compaction is often redundant observation data rather than critical reasoning context.</p> <p>Unified analysis attempts to reconstruct the full session using compaction summaries as bridge context, supplemented by external artifacts (file changes, git commits, test results). Factory.ai's approach maintains \"persistent, anchored summaries\" rather than re-summarizing, and their key insight \u2014 \"minimize tokens per task, not per request\" \u2014 argues that over-compression forces agents to re-fetch information, actually increasing total costs. This implies that compaction summaries alone may be insufficient bridges, and external artifacts are necessary for accurate cross-boundary analysis.</p> <p>The recommended hybrid uses segmented analysis for fine-grained metrics (error rates, correction patterns, tool usage frequency) where compaction genuinely changes the AI's available context, and unified analysis for high-level patterns (session productivity, task completion, user satisfaction trajectory) where compaction summaries provide adequate continuity. Robert Lavigne's layered compression taxonomy suggests using distilled representations of older context, summarized versions of recent context, and full detail for the current segment \u2014 a pattern that maps directly to how compaction creates a natural hierarchy of detail levels across a session.</p> <p>The SeCom framework (ICLR 2025) provides a concrete implementation: it partitions long conversations into topically coherent segments for memory management, then uses compression-based denoising on memory units to enhance retrieval across segment boundaries. The C-DIC system treats conversations as \"interleaved contextual threads\" with revisable per-thread compression states, using a retrieve-revise-write-back loop for cross-turn memory sharing \u2014 a pattern directly applicable to maintaining coherence across compaction boundaries.</p>"},{"location":"research/session-enrichment/conversation-reconstruction/#abandoned-branches-are-an-underexploited-gold-mine","title":"Abandoned branches are an underexploited gold mine","text":"<p>The evidence for analyzing abandoned branches is overwhelming, drawing from three converging research programs. The Dialogue Breakdown Detection Challenge (DBDC, five iterations from 2015-2022) established that detecting and analyzing conversation failures is both feasible and valuable, using a three-level annotation scheme (not-a-breakdown, possible breakdown, breakdown) with 30 annotators per turn. BERT-based approaches with continued pre-training achieved state-of-the-art results, beating baselines by over 12% accuracy.</p> <p>Each branch abandonment creates a natural RLHF preference pair. The conversation state at the fork point is the context; the path the user continued on is the \"chosen\" response; the abandoned branch is the \"rejected\" response. With 92% of messages on abandoned branches, this represents an enormous untapped dataset of implicit negative feedback. Rafailov et al.'s DPO framework can consume these pairs directly without reward model training. The depth of an abandoned branch before abandonment serves as an implicit severity signal \u2014 short abandoned branches suggest quick user recognition of a bad approach, while long ones indicate the AI was confidently wrong for many turns.</p> <p>The \"rewind distance\" metric \u2014 how far back users go before trying a new approach \u2014 directly measures the AI's inability to self-correct. Lu, Zhang, and Chen (2019) demonstrated with Hindsight Experience Replay that learning from failed dialogues improves learning rate over standard experience replay, providing theoretical grounding for mining abandoned branches. IBM Watson's Dialog Flow Analysis provides production precedent: it visualizes where users abandon conversations, extracts keywords from abandoned paths, and uses this to identify improvement opportunities in intents, entities, and dialog implementations.</p> <p>Practical analysis of abandoned branches should classify abandonment causes into a coding-session-specific taxonomy (wrong algorithmic approach, stuck in error loops, scope creep, misunderstood requirements, tooling failures), measure branch depth before abandonment, identify recurring failure patterns using dialogue breakdown detection techniques, and track the relationship between rewind distance and task complexity.</p>"},{"location":"research/session-enrichment/conversation-reconstruction/#concrete-implementation-recommendations","title":"Concrete implementation recommendations","text":"<p>The complete pipeline for processing Claude Code session transcripts should proceed in five phases. First, parse and index: load JSONL, build the <code>uuid \u2192 message</code> hash map and <code>parent \u2192 [children]</code> adjacency list, detect and repair orphaned messages using timestamp-based linking. Second, extract the active path: identify the active leaf by file position, backtrack to root via <code>parentUuid</code>, and stitch across compaction boundaries using <code>logicalParentUuid</code> with positional fallback. Third, classify all fork points: iterate the children map, classify each multi-child node as parallel tools (all children are tool_results), compaction (system boundary markers present), or user rewind (residual category), and record metadata for each fork. Fourth, segment at compaction boundaries: split the active path at compaction events, preserve summaries as bridge context, and flag analysis confidence levels for within-segment versus cross-segment findings. Fifth, analyze abandoned branches: for each fork classified as a user rewind, extract the abandoned subtree, compute depth and rewind distance metrics, classify the abandonment cause, and structure as preference pairs for downstream training.</p> <p>The OpenAssistant (OASST) dataset provides the closest open-source precedent at scale: 208,584 messages in 70,642 conversation trees with explicit <code>message_id</code>/<code>parent_id</code> structure, rank annotations at branch points, and established tooling for depth-first traversal and path extraction by rank. The <code>cchat</code> tool for Claude Code specifically handles compaction stitching, branch detection filtering of mechanical fan-out, and UUID tree resolution \u2014 making it the most directly relevant existing implementation. ChatTree provides real-time visualization of conversation trees with zoom and drag navigation, useful for debugging tree reconstruction algorithms.</p>"},{"location":"research/session-enrichment/conversation-reconstruction/#conclusion","title":"Conclusion","text":"<p>The tree-to-linear reconstruction problem is well-solved algorithmically \u2014 leaf-to-root backtracking is the universal approach \u2014 but the real complexity lies in fork classification and deciding what to do with the 92% of messages that aren't on the active path. The strongest finding from this research is that abandoned branches contain far more diagnostic value than the active path itself: they reveal failure modes, create natural preference training data, and encode implicit user feedback about AI behavior. No existing system fully exploits this signal. The compaction problem is best handled through hybrid segmented-unified analysis, treating compaction summaries as lossy bridges and supplementing with external artifacts. The most underexplored opportunity is applying RLHF preference learning frameworks to the natural chosen/rejected pairs that branch points create \u2014 a direction that connects tree-structured session data directly to model improvement pipelines.</p>"},{"location":"research/session-enrichment/prompts/","title":"Session Enrichment Research Prompts","text":"<p>Paste these into web AI (Claude.ai, Gemini, ChatGPT) for research. Each prompt is self-contained with full context.</p>"},{"location":"research/session-enrichment/prompts/#prompt-1-session-level-enrichment-schema","title":"Prompt 1: Session-Level Enrichment Schema","text":"<pre><code>I'm building BrainLayer \u2014 an open-source persistent memory layer for AI agents (Claude Code specifically). It indexes Claude Code conversation transcripts into a SQLite database with sqlite-vec for semantic search.\n\nCurrently we have CHUNK-LEVEL enrichment \u2014 each 2000-char chunk gets: summary, tags, importance (1-10), intent (debugging/implementing/discussing/etc), key_symbols, language.\n\nI want to add SESSION-LEVEL enrichment \u2014 analyzing full conversations as a unit. This would extract things you can't see from individual chunks:\n\n- What corrections did the user make? (Claude did X, user said \"no, do Y\")\n- What decisions were made and WHY?\n- What patterns of frustration emerged?\n- What worked well vs poorly?\n- What instructions did the user repeat (Claude forgot/ignored)?\n- What new knowledge was established?\n\n**Technical context:**\n- Sessions are JSONL files, 100-4000+ messages each\n- Message types: user, assistant, progress (tool calls), system\n- Sessions have a UUID tree (parentUuid chains) \u2014 but \"forks\" are mostly just parallel tool results sharing a parent, not real conversation branches\n- Compaction events are marked by \"This session is being continued from a previous conversation\" in user messages\n- We already have 268K chunks across 800+ sessions\n- Local LLM available (Ollama with GLM-4.7-Flash, ~8K context) but also Gemini Flash (free, 1M context)\n\n**Questions:**\n1. What metadata fields should a session enrichment record have? Design the schema.\n2. How should we handle sessions that are 200K+ tokens? Staged approach? Sliding window? Map-reduce?\n3. Should corrections/learnings be their own entity (separate from session summary) so they're independently searchable?\n4. How would you structure the LLM prompt that analyzes a session?\n5. What about linking learnings across sessions? (Same mistake in session A and session C = pattern)\n6. How does this relate to existing chunk enrichment \u2014 should session enrichment reference chunk IDs?\n</code></pre>"},{"location":"research/session-enrichment/prompts/#prompt-2-brainstore-write-side-mcp-tool-design","title":"Prompt 2: brainStore \u2014 Write-Side MCP Tool Design","text":"<pre><code>I'm building BrainLayer \u2014 an open-source memory layer for AI agents, exposed as an MCP (Model Context Protocol) server. Currently all 12 MCP tools are READ-ONLY (search, think, recall, context, etc).\n\nI want to add a WRITE-SIDE tool called brainlayer_store (or brainStore) that lets any Claude Code session quickly store:\n- Feature ideas that come up during work\n- Mistakes/corrections (like \"/learn-mistake\" but automatic)\n- Decisions made (\"we chose X over Y because Z\")\n- Quick notes (\"this API is flaky, retry needed\")\n\n**Current architecture:**\n- Python MCP server using `mcp` library\n- SQLite database with `apsw` (268K chunks, ~3.8GB)\n- Embeddings via sentence-transformers (bge-large-en-v1.5, 1024 dims)\n- Enrichment via local Ollama (GLM-4.7-Flash)\n\n**Design questions:**\n1. Schema: What table structure? Should stored items go into the existing `chunks` table or a new `notes`/`store` table?\n2. Should items be embedded at write time (slow, ~1-2s) or queued for batch embedding later?\n3. Categories: What types make sense? (idea, mistake, decision, learning, todo, bookmark?)\n4. Should brainlayer_store return confirmation or also return related existing memories? (\"You stored X. FYI, you noted something similar 3 days ago: Y\")\n5. MCP tool signature \u2014 what parameters? (content, type, project, tags, priority?)\n6. How does this integrate with the existing search? Should stored items be searchable via brainlayer_search?\n7. Security: Can any MCP client write? Should there be auth/scoping?\n8. Aggregation: How to cluster/summarize stored items periodically (nightly batch)?\n\n**Context:** We already have a \"/learn-mistake\" skill in our golems system that stores mistakes to JSON files and clusters them nightly using embeddings. brainStore would replace that with a proper database-backed system available to ALL Claude Code sessions, not just golems.\n</code></pre>"},{"location":"research/session-enrichment/prompts/#prompt-3-conversation-forkbranch-detection","title":"Prompt 3: Conversation Fork/Branch Detection","text":"<pre><code>I'm analyzing Claude Code session transcripts (JSONL format) for a memory system. Each message has a `uuid` and `parentUuid` field, forming a tree structure.\n\nIn a real session I analyzed:\n- 4,118 messages with UUIDs\n- 321 on the \"active path\" (tracing from last message back to root)\n- 3,797 \"abandoned\" messages (92%!)\n- 539 apparent \"fork points\" (parent with &gt;1 child)\n\nBut investigating further:\n- Only 8 are compaction events (marked by \"This session is being continued from a previous conversation\")\n- The other 531 \"forks\" are just parallel events \u2014 tool results and progress events sharing the same parent assistant message. Not real conversation branches.\n- Real user rewinds (checkpoints) are rare (~5-8 per long session)\n\n**For session-level enrichment, I need to:**\n1. Reconstruct the actual linear conversation from the JSONL tree\n2. Distinguish: compaction (content was summarized), real rewind (user changed direction), parallel tool events (not a fork)\n3. Handle compaction gracefully \u2014 the summary replaces earlier content, but the enrichment should know context was lost\n\n**Questions:**\n1. What's the best algorithm to reconstruct the \"true\" conversation from this tree?\n2. Should abandoned branches be analyzed too? (They show what Claude tried before the user corrected)\n3. How to detect real user rewinds vs compaction vs parallel events?\n4. For session enrichment: should we enrich each \"segment\" between compactions separately, or try to reconstruct the full session?\n5. Is there prior art in conversation analysis / dialogue systems for handling branching conversations?\n</code></pre>"},{"location":"research/session-enrichment/prompts/#prompt-4-auto-extracting-learnings-from-268k-chunks","title":"Prompt 4: Auto-Extracting Learnings from 268K Chunks","text":"<pre><code>I have a SQLite database with 268,864 indexed chunks from Claude Code conversations. Each chunk has:\n- content (the actual text, 50-2000 chars)\n- content_type (user_message, assistant_text, ai_code, stack_trace, file_read, git_diff)\n- source (claude_code, whatsapp, youtube)\n- intent (debugging, implementing, discussing, deciding, configuring, reviewing)\n- importance (1-10 float)\n- tags (JSON array of topic tags)\n- summary (1-sentence summary)\n- project (which codebase)\n- created_at (timestamp)\n\nI want to automatically extract \"learnings\" \u2014 patterns like:\n- Repeated corrections: user keeps telling Claude the same thing\n- Common mistakes: same type of error across sessions\n- Established rules: \"always do X, never do Y\" type instructions\n- Tool preferences: \"use Read not cat\", \"use bun not npm\"\n- Architecture decisions: \"we chose X because Y\"\n\n**The challenge:** Individual chunks lack conversation context. A user message saying \"no, RTL\" only makes sense next to the assistant message that got it wrong.\n\n**Approaches I'm considering:**\n1. **Keyword mining:** Find chunks with correction signals (\"no\", \"wrong\", \"I said\", \"not what I meant\", \"actually\", \"stop\") and pull surrounding context\n2. **High-importance user messages:** Filter importance &gt;= 7, intent = \"discussing\" or \"deciding\"\n3. **Session-level analysis:** Process full sessions (see separate prompt) to extract corrections in context\n4. **Clustering:** Embed all user corrections, cluster by similarity, surface repeated patterns\n5. **Diff analysis:** Compare what user asked vs what Claude produced (using git_diff chunks nearby)\n\n**Questions:**\n1. Which approach (or combination) would give the best signal-to-noise ratio?\n2. How to distinguish genuine corrections from normal conversation flow?\n3. How to cluster learnings across 800+ sessions into actionable rules?\n4. Output format: How should extracted learnings be structured for auto-generating CLAUDE.md rules?\n5. How to handle learnings that contradict each other (user changed their mind over time)?\n6. Scale: Can this run on 268K chunks locally, or does it need sampling?\n</code></pre>"},{"location":"research/session-enrichment/prompts/#prompt-5-cli-ux-for-memoryknowledge-tools","title":"Prompt 5: CLI UX for Memory/Knowledge Tools","text":"<p><pre><code>I'm designing the CLI experience for BrainLayer, an open-source memory layer for AI agents. Current CLI commands are bare-bones \u2014 `brainlayer search \"query\"` returns results with score, project, and raw content.\n\nThe database has rich enrichment data that isn't being displayed:\n- summary (1-sentence), tags (topic array), importance (1-10), intent (debugging/implementing/etc)\n- 144K enriched chunks out of 268K total\n- Per-source data: claude_code (250K chunks), whatsapp (16K, with contact_name), youtube (2K)\n\n**Current display (ugly):**\n</code></pre> 1. (score: 0.823) (golems) Raw chunk content here up to 500 chars... ID: chunk_abc123 <pre><code>**I want to design a much nicer CLI with:**\n1. Rich display using the `rich` Python library (tables, colors, panels)\n2. Enrichment data visible (tags as colored badges, importance as stars/bars, intent as icons)\n3. Smart truncation (show summary instead of raw content when available)\n4. Multiple display modes (compact list, detailed cards, table format)\n5. Interactive features (filter by tag, sort by importance, drill into context)\n6. `brainlayer insights` \u2014 show extracted learnings/patterns\n7. `brainlayer stats --detailed` \u2014 per-project, per-source breakdowns with charts\n8. `brainlayer timeline` \u2014 show activity over time (like GitHub contribution graph but for knowledge)\n\n**Constraints:**\n- Must work in standard terminal (no TUI framework, just rich library)\n- Fast \u2014 search results should appear in &lt;2 seconds\n- The library already uses `typer` for CLI and `rich` for output\n\n**Questions:**\n1. What's the ideal search result layout? Show me ASCII mockups.\n2. How should enrichment data be prioritized in limited terminal space?\n3. What CLI commands/subcommands make sense for a knowledge tool?\n4. Any prior art in CLI tools with great search UX? (ripgrep, fzf, etc)\n5. How to handle the transition between \"quick search\" and \"deep exploration\"?\n</code></pre></p>"},{"location":"research/session-enrichment/prompts/#how-to-use-these","title":"How to Use These","text":"<ol> <li>Start with Prompt 1 (schema design) \u2014 this shapes everything else</li> <li>Prompt 2 (brainStore) can run in parallel \u2014 it's independent</li> <li>Prompt 3 (fork detection) informs the implementation of Prompt 1</li> <li>Prompt 4 (auto-learnings) builds on top of session enrichment</li> <li>Prompt 5 (CLI UX) can run anytime \u2014 it's the presentation layer</li> </ol> <p>Save responses to <code>docs/research/session-enrichment/</code> for reference.</p>"},{"location":"research/session-enrichment/session-enrichment-architecture/","title":"Session-level enrichment architecture for BrainLayer","text":"<p>BrainLayer's next evolution should treat sessions as first-class analytical units with a hybrid flat-column/JSON schema, a tiered processing pipeline that routes sessions by size between Gemini Flash and local map-reduce, and a corrections-as-entities subsystem where user corrections graduate into reusable rules through spaced-repetition-inspired confidence scoring. This design draws on proven patterns from Zep's temporal knowledge graph, Mem0's AUDN memory loop, and Cognee's DataPoint model \u2014 adapted for SQLite with sqlite-vec and FTS5. The result: 268K chunks across 800+ sessions become a searchable knowledge base that captures not just what happened, but what was learned, what failed, and what rules emerged.</p>"},{"location":"research/session-enrichment/session-enrichment-architecture/#1-the-schema-should-blend-relational-precision-with-json-flexibility","title":"1. The schema should blend relational precision with JSON flexibility","text":"<p>The central design tension is between queryable flat columns (for dashboards, filtering, aggregation) and flexible JSON columns (for variable-length arrays like decisions, corrections, and tool stats). The right answer is a hybrid: flat columns for anything you filter or sort on, JSON for everything else, with SQLite generated columns bridging the gap when a JSON field later needs indexing.</p> <p>A single <code>session_enrichments</code> table works best here since the relationship is strictly 1:1 (one enrichment per session). Normalization into separate tables is warranted only for many-to-many relationships like topics and tool usage, where you need efficient reverse lookups (\"find all sessions about TypeScript\" or \"aggregate Bash tool success rates\").</p> <pre><code>CREATE TABLE session_enrichments (\n    -- Identity\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    session_id TEXT NOT NULL UNIQUE,\n    file_path TEXT,\n    enrichment_version TEXT NOT NULL DEFAULT '1.0',\n    enrichment_model TEXT,\n    enrichment_timestamp TEXT NOT NULL DEFAULT (strftime('%Y-%m-%dT%H:%M:%fZ','now')),\n\n    -- Timing (flat \u2014 for temporal queries)\n    session_start_time TEXT,\n    session_end_time TEXT,\n    duration_seconds INTEGER,\n\n    -- Message dynamics (flat \u2014 for aggregation dashboards)\n    message_count INTEGER NOT NULL DEFAULT 0,\n    user_message_count INTEGER NOT NULL DEFAULT 0,\n    assistant_message_count INTEGER NOT NULL DEFAULT 0,\n    tool_call_count INTEGER NOT NULL DEFAULT 0,\n    total_input_tokens INTEGER,\n    total_output_tokens INTEGER,\n    compaction_count INTEGER DEFAULT 0,\n\n    -- Content analysis (flat \u2014 for filtering)\n    session_summary TEXT,\n    primary_intent TEXT,\n    narrative_arc TEXT,\n    complexity_score INTEGER CHECK(complexity_score BETWEEN 1 AND 10),\n    outcome TEXT CHECK(outcome IN ('success','partial_success','failure','abandoned','ongoing')),\n\n    -- Content analysis (JSON \u2014 variable-length, read-heavy)\n    secondary_intents TEXT DEFAULT '[]',\n    topic_tags TEXT DEFAULT '[]',\n    topic_evolution TEXT DEFAULT '[]',\n\n    -- Quality scores (flat \u2014 for dashboards and alerts)\n    session_quality_score INTEGER CHECK(session_quality_score BETWEEN 1 AND 10),\n    ai_effectiveness_score INTEGER CHECK(ai_effectiveness_score BETWEEN 1 AND 10),\n    tool_usage_quality_score INTEGER CHECK(tool_usage_quality_score BETWEEN 1 AND 10),\n    frustration_level INTEGER CHECK(frustration_level BETWEEN 1 AND 10),\n    success_rate REAL CHECK(success_rate BETWEEN 0.0 AND 1.0),\n\n    -- Quality narratives (text \u2014 for human reading)\n    what_worked TEXT,\n    what_failed TEXT,\n    quality_justification TEXT,\n\n    -- Decisions and corrections (JSON \u2014 variable-length arrays)\n    decisions_made TEXT DEFAULT '[]',\n    corrections TEXT DEFAULT '[]',\n    repeated_instructions TEXT DEFAULT '[]',\n    frustration_signals TEXT DEFAULT '[]',\n\n    -- Knowledge (JSON \u2014 flexible structured data)\n    new_knowledge TEXT DEFAULT '[]',\n    rules_established TEXT DEFAULT '[]',\n\n    -- Tool usage (JSON \u2014 per-tool stats)\n    tool_usage_stats TEXT DEFAULT '[]',\n    most_used_tools TEXT DEFAULT '[]',\n    tool_failures TEXT DEFAULT '[]',\n\n    -- Embedding for semantic search\n    summary_embedding BLOB\n);\n</code></pre> <p>The JSON columns use SQLite's <code>json_each()</code> table-valued function for querying. When a JSON field becomes a frequent filter target, promote it without migration:</p> <pre><code>-- Note: tool_usage_stats stores a JSON array (e.g., [{\"tool_name\": \"Read\", \"count\": 42}])\n-- Default should be '[]' not '{}' to match this array access pattern\nALTER TABLE session_enrichments ADD COLUMN primary_tool TEXT\n    GENERATED ALWAYS AS (json_extract(tool_usage_stats, '$[0].tool_name')) VIRTUAL;\nCREATE INDEX idx_primary_tool ON session_enrichments(primary_tool);\n</code></pre> <p>Three supporting tables handle many-to-many relationships and the processing pipeline. <code>session_topics</code> enables reverse lookups by topic. <code>session_tool_usage</code> enables per-tool analytics across sessions. <code>enrichment_jobs</code> tracks the multi-pass processing state, storing intermediate outputs from each pass so failed jobs can resume without reprocessing.</p> <p>For full-text search, an FTS5 virtual table in content-sync mode indexes the narrative fields (summary, what_worked, what_failed, quality_justification) with Porter stemming. Triggers keep it synchronized. For vector search, sqlite-vec stores the summary embedding as a BLOB in the main table \u2014 at 268K rows with 384-dim float32 vectors, brute-force cosine similarity runs in 50\u2013200ms, which is acceptable for batch analysis and tolerable for interactive queries. Binary quantization (384 bits instead of 384 floats) can accelerate this 30\u00d7 if needed.</p>"},{"location":"research/session-enrichment/session-enrichment-architecture/#2-route-sessions-by-size-through-a-tiered-processing-pipeline","title":"2. Route sessions by size through a tiered processing pipeline","text":"<p>The fundamental constraint is that sessions range from a few thousand tokens to 200K+, and the available models span from an 8K-context local LLM to Gemini Flash's 1M context window. Research on long-context LLM performance reveals a critical insight: advertised context windows are not effective context windows. Gemini Flash shows reliable performance up to ~100K tokens, but developer reports indicate degradation after that, with \"lost in the middle\" effects causing missed or confused information.</p> <p>The optimal architecture is a three-tier routing system:</p> <p>Tier 1 (\u2264100K tokens, ~60\u201370% of sessions): Full-context Gemini Flash. Send the entire session in a single API call with structured extraction prompts. This produces the highest quality results with the simplest implementation. Place the transcript first in the prompt and extraction instructions last \u2014 Anthropic's research shows queries at the end of long contexts improve quality by up to 30%.</p> <p>Tier 2 (100K\u2013500K tokens): Chunked Gemini Flash. Split into 2\u20135 overlapping segments with 15% overlap, process each segment, then run a lightweight merge/reconciliation pass. Split on message boundaries, never mid-message. Include speaker metadata and timestamps in the overlap region for context continuity.</p> <p>Tier 3 (&gt;500K tokens or rate-limited): Map-reduce with local LLM. The LLM\u00d7MapReduce approach from Tsinghua University (2024) is the gold standard here. Each chunk gets a structured extraction pass (the \"map\") producing metadata plus a confidence score. The \"reduce\" phase merges results using confidence-weighted reconciliation. Key finding: LLM\u00d7MapReduce with a small model (4B parameters) outperformed 70B-scale models on long-context benchmarks, because the small model processes focused chunks more accurately than a large model processes diffuse context.</p> <pre><code>def route_session(token_count, gemini_available=True):\n    if gemini_available and token_count &lt;= 100_000:\n        return \"gemini-full-context\"\n    elif gemini_available and token_count &lt;= 500_000:\n        return \"gemini-chunked\"\n    else:\n        return \"local-map-reduce\"\n</code></pre> <p>For the local map-reduce path with GLM-4.7-Flash's 8K context: use 4,000\u20136,000 token chunks with 15\u201320% overlap, split on message boundaries. This leaves room for the extraction prompt and output tokens. Each map output should include a rationale, extracted fields, and a per-field confidence score. The reduce phase can use either Gemini (if available) or iterative local merging.</p> <p>Cost and throughput for 800 sessions via Gemini Flash free tier: not feasible. At 20\u2013100 requests per day on the free tier, processing would take 8\u201340 days. Paid Tier 1 (just enabling billing, no minimum) costs roughly $0.30/M input tokens. Processing all 800 sessions (estimated 160M input tokens) costs approximately $52 total, or ~$26 via the Batch API at 50% discount. This is the pragmatic choice. Alternatively, the Gemini CLI provides dramatically better free-tier limits (60 RPM, 1,000 RPD).</p>"},{"location":"research/session-enrichment/session-enrichment-architecture/#3-corrections-must-be-first-class-searchable-entities-not-embedded-json","title":"3. Corrections must be first-class searchable entities, not embedded JSON","text":"<p>This is the most consequential architectural decision. Research across Zep, Mem0, Cognee, and knowledge graph literature converges on a clear answer: corrections, learnings, and rules deserve their own table with their own lifecycle. Embedding them only as JSON arrays inside session enrichments makes them invisible to cross-session analysis.</p> <p>The key insight comes from Zep/Graphiti's temporal knowledge graph: facts change over time, and the system must track not just what's currently true but how knowledge evolved. Corrections follow a natural graduation pipeline:</p> <ul> <li>Correction (single instance, session-bound) \u2192 seen once, low confidence</li> <li>Pattern (2\u20133 occurrences across sessions) \u2192 reinforced, growing confidence  </li> <li>Preference (stable, high confidence) \u2192 consistent across many sessions</li> <li>Rule (permanent, always applied) \u2192 explicitly confirmed or 5+ consistent occurrences</li> </ul> <p>This maps to a spaced-repetition-inspired confidence model: <code>effective_weight = base_confidence \u00d7 (reinforcement_count ^ growth_factor) \u00d7 e^(-days_since_last_use / half_life)</code>. Corrections that aren't reinforced decay; those that recur strengthen.</p> <pre><code>CREATE TABLE corrections (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    source_session_id TEXT NOT NULL,\n    source_chunk_id TEXT,\n    correction_type TEXT NOT NULL,        -- preference, factual, style, process, tool_usage\n    original_behavior TEXT,               -- what the agent did wrong\n    corrected_behavior TEXT NOT NULL,     -- what the user wanted\n    rule_text TEXT NOT NULL,              -- normalized, reusable rule statement\n    confidence REAL DEFAULT 0.5,\n    reinforcement_count INTEGER DEFAULT 1,\n    half_life_days REAL DEFAULT 30.0,\n    first_seen_at TEXT NOT NULL,\n    last_seen_at TEXT NOT NULL,\n    status TEXT DEFAULT 'correction',     -- correction \u2192 pattern \u2192 preference \u2192 rule\n    tags TEXT DEFAULT '[]',\n    embedding BLOB,\n    FOREIGN KEY (source_session_id) REFERENCES session_enrichments(session_id)\n);\n\nCREATE TABLE correction_links (\n    source_id INTEGER NOT NULL REFERENCES corrections(id),\n    target_id INTEGER NOT NULL REFERENCES corrections(id),\n    link_type TEXT NOT NULL,              -- duplicate, related, supersedes, contradicts\n    similarity_score REAL,\n    PRIMARY KEY (source_id, target_id)\n);\n</code></pre> <p>Why separate from session_enrichments? Three reasons. First, corrections need their own embeddings for semantic deduplication \u2014 you need to find \"user said don't use var\" in session 47 when the same correction appears in session 312. Second, corrections accumulate metadata over their lifetime (reinforcement_count, confidence) that doesn't belong to any single session. Third, the <code>correction_links</code> table enables a graph of related corrections that spans sessions.</p> <p>The <code>rule_text</code> field is critical: it's the normalized, agent-consumable statement derived from the raw correction context. \"No, use const not var\" becomes \"Always use const instead of var in JavaScript/TypeScript files.\" This normalization step happens during extraction and makes corrections directly injectable into future agent prompts.</p> <p>Mem0's AUDN (Add/Update/Delete/Noop) pattern provides the right operational model for processing new corrections: embed the candidate, search existing corrections by cosine similarity, and use the LLM to decide whether to ADD a new correction, UPDATE an existing one (incrementing reinforcement_count), DELETE a contradicted one, or NOOP.</p>"},{"location":"research/session-enrichment/session-enrichment-architecture/#4-three-pass-prompting-extracts-layered-metadata-without-overwhelming-the-llm","title":"4. Three-pass prompting extracts layered metadata without overwhelming the LLM","text":"<p>Research from Anthropic, OpenAI, and conversation intelligence platforms (Gong, Chorus, CallMiner) converges on a principle: single-task prompts outperform multi-task prompts for extraction quality. A three-pass strategy gives each pass the LLM's full attention on one cognitive task.</p> <p>Pass 1 \u2014 Structure and narrative (broad understanding). Extract: session summary, primary intent, topic evolution, narrative arc, complexity score, outcome, message counts. This pass establishes the \"what happened\" frame that contextualizes everything else. The prompt should use XML tags for structure, place the transcript first with instructions after, and request JSON output matching a strict schema.</p> <p>Pass 2 \u2014 Detailed signal extraction (specific signals). Takes Pass 1 output as context. Extract: decisions made (with rationale and outcome), user corrections (what was wrong, what was right, severity), repeated instructions, frustration signals (explicit and implicit), tool usage statistics, new knowledge established, rules the user expressed. This pass benefits from the narrative frame established in Pass 1, allowing it to focus on fine-grained extraction without needing to understand the overall arc.</p> <p>Pass 3 \u2014 Quality scoring (judgment). Takes Pass 1 and Pass 2 outputs \u2014 not the full transcript. Score: session quality, AI effectiveness, tool usage quality, communication quality, recovery quality. Also produce what_worked, what_failed, and success_rate. This pass works from distilled information, making it suitable for a smaller model.</p> <p>Key prompting techniques that significantly improve extraction quality:</p> <ul> <li>\"Quote before answering\": Instruct the LLM to quote relevant transcript excerpts before making claims. This grounds responses in evidence and dramatically reduces hallucination on long documents.</li> <li>Selective attention instructions: \"Focus primarily on user messages expressing intent, moments where direction changes, error/failure events, and the final outcome. Skim routine tool outputs confirming success.\"</li> <li>Temperature 0.0\u20130.2 for extraction tasks ensures factual, deterministic outputs.</li> <li>Structured output enforcement: Use JSON schema constraints (Gemini's <code>response_schema</code> or function calling) for guaranteed schema compliance.</li> </ul> <p>For the map-reduce path, Pass 1 runs independently on each chunk (the \"map\"). Passes 2 and 3 run on the merged Pass 1 output (the \"reduce\"). Each map output includes per-field confidence scores for conflict resolution during merging.</p>"},{"location":"research/session-enrichment/session-enrichment-architecture/#5-cross-session-pattern-detection-needs-both-real-time-dedup-and-periodic-batch-analysis","title":"5. Cross-session pattern detection needs both real-time dedup and periodic batch analysis","text":"<p>Detecting that the same correction appears across sessions requires two complementary mechanisms: real-time similarity matching during ingestion and periodic batch clustering for pattern discovery.</p> <p>Real-time dedup during correction ingestion uses sqlite-vec. When a new correction is extracted, embed it and query the vec_corrections table for neighbors within cosine distance &lt; 0.15 (i.e., cosine similarity &gt; 0.85 \u2014 <code>vec_distance_cosine</code> returns distance where lower = more similar). If a near-duplicate exists, invoke the AUDN loop: the LLM decides whether to merge (incrementing reinforcement_count and updating confidence) or keep as distinct.</p> <pre><code>-- Find similar existing corrections (distance &lt; 0.15 = similarity &gt; 0.85)\nSELECT c.id, c.rule_text, c.confidence, c.reinforcement_count,\n       vec_distance_cosine(vc.embedding, :new_embedding) as distance\nFROM vec_corrections vc\nJOIN corrections c ON c.id = vc.correction_id\nWHERE vec_distance_cosine(vc.embedding, :new_embedding) &lt; 0.15\nORDER BY distance\nLIMIT 5;\n</code></pre> <p>Periodic batch analysis (nightly or after every N sessions) handles broader pattern discovery that real-time matching misses:</p> <ol> <li>Fetch all corrections added since the last batch run</li> <li>Run HDBSCAN clustering on the full correction embedding space (HDBSCAN is preferred over K-means because it auto-detects cluster count and handles varying cluster densities)</li> <li>For each cluster, generate a descriptive label using the LLM</li> <li>Within clusters, merge near-duplicates (cosine similarity &gt; 0.85)</li> <li>Promote corrections exceeding confidence thresholds (correction \u2192 pattern at 2+ occurrences, pattern \u2192 preference at consistent cross-session presence, preference \u2192 rule at 5+ reinforcements or explicit user confirmation)</li> <li>Decay old corrections not reinforced recently (reduce effective_weight)</li> <li>Generate a summary report of emerging patterns</li> </ol> <p>This mirrors how log analysis systems (Amazon CloudWatch Patterns, LogCluster) detect recurring issues: knowledge base initialization followed by online matching against existing clusters, with unmatched items seeding new clusters. Customer support ticket clustering research confirms this two-phase approach (real-time classification + periodic re-clustering) as the most practical architecture.</p> <p>For the 268K-row scale, UMAP dimensionality reduction before HDBSCAN improves clustering quality on high-dimensional embeddings. The batch job stores cluster assignments in a <code>correction_clusters</code> table, enabling queries like \"show me the top 10 most reinforced correction clusters.\"</p>"},{"location":"research/session-enrichment/session-enrichment-architecture/#6-session-enrichment-should-reference-chunks-bidirectionally","title":"6. Session enrichment should reference chunks bidirectionally","text":"<p>The relationship between session-level and chunk-level enrichment should be bidirectional but loosely coupled. Session enrichments reference specific chunk IDs where key events occurred (a correction at chunk #47, a decision at chunk #112), and chunk-level retrieval can be enhanced by session-level metadata.</p> <p>Session \u2192 Chunk references: The JSON arrays in session_enrichments (decisions_made, corrections, frustration_signals) should include a <code>chunk_id</code> or <code>message_index</code> field pointing to the specific chunk where the event was detected. This enables drill-down: a user investigating a low-quality session can jump directly to the problematic exchange.</p> <p>Chunk \u2192 Session enhancement: When retrieving chunks via semantic search (the primary BrainLayer use case), session-level metadata acts as a reranking signal. A chunk from a session with quality_score=9 and outcome=success is more likely to contain reliable information than the same semantic match from a session with quality_score=2 and outcome=failure. Implement this as a weighted score:</p> <pre><code>SELECT c.*, se.session_quality_score, se.outcome,\n    (0.7 * semantic_score + 0.2 * (se.session_quality_score / 10.0) \n     + 0.1 * CASE se.outcome WHEN 'success' THEN 1.0 \n            WHEN 'partial_success' THEN 0.6 ELSE 0.2 END) as enhanced_score\nFROM chunks c\nJOIN session_enrichments se ON se.session_id = c.session_id\nORDER BY enhanced_score DESC;\n</code></pre> <p>This pattern draws from Kernel Memory's tags system (metadata attached at the document level flows down to enhance chunk-level retrieval) and Cognee's strict provenance linking (inferred information always links back to source documents). The key principle is that session context enriches chunk retrieval without replacing it \u2014 chunks remain the primary unit of semantic search, but session metadata provides quality signals and navigational context.</p> <p>Do not store session-level embeddings in the same vec0 table as chunk embeddings. They exist at different semantic granularities. Use separate vec0 tables (or separate embedding columns) so similarity searches within each level remain clean.</p>"},{"location":"research/session-enrichment/session-enrichment-architecture/#7-lessons-from-zep-mem0-cognee-and-letta-shape-the-design","title":"7. Lessons from Zep, Mem0, Cognee, and Letta shape the design","text":"<p>Each existing project contributes a distinct architectural insight that BrainLayer should incorporate:</p> <p>Zep/Graphiti's bi-temporal model is the most sophisticated approach to handling changing facts. Every fact carries four timestamps: <code>t_created</code>, <code>t_expired</code> (ingestion time) and <code>t_valid</code>, <code>t_invalid</code> (event time). When new information contradicts old, the old edge gets invalidated rather than deleted. For BrainLayer's corrections table, this translates to <code>first_seen_at</code>, <code>last_seen_at</code>, and <code>invalidated_at</code> columns \u2014 never delete corrections, only mark them superseded.</p> <p>Mem0's incremental AUDN loop (Add/Update/Delete/Noop) provides the operational model for processing corrections. Rather than reanalyzing entire sessions when new information arrives, process corrections incrementally: embed, search, decide operation. This scales to continuous enrichment as new sessions arrive without reprocessing the entire corpus.</p> <p>Cognee's Memify pattern \u2014 post-processing that compresses repeated patterns from session traces into reusable meta-structures \u2014 maps directly to the batch clustering pipeline described above. Cognee also validates that memory updates actually improve agent performance, a principle BrainLayer should adopt: track whether surfacing a correction during retrieval actually prevents the error from recurring.</p> <p>Letta/MemGPT's two-tier memory (editable core memory + archival vector store) suggests that BrainLayer's most important corrections and rules should be promoted to a \"core rules\" set that's always injected into the agent prompt, while the full corrections database serves as searchable archival memory.</p> <p>LangMem's schema-based extraction using Pydantic models for structured memory types (semantic, episodic, procedural) provides a clean software pattern: define extraction schemas as data classes, use them to constrain LLM output, and store the validated results directly in SQLite.</p> <p>One pattern all projects share: LLM-based conflict resolution over brittle rules. When two corrections contradict each other or a new fact conflicts with an old one, delegate the resolution decision to the LLM rather than building complex rule-based logic. This is both more robust and simpler to maintain.</p>"},{"location":"research/session-enrichment/session-enrichment-architecture/#conclusion-a-practical-implementation-roadmap","title":"Conclusion: a practical implementation roadmap","text":"<p>The system described here is ambitious but modular \u2014 each component delivers value independently and can be built incrementally. Start with the session_enrichments table and the three-pass Gemini pipeline, which immediately makes 800+ sessions searchable by quality, intent, outcome, and narrative. Add the corrections table and real-time dedup second, since this is where the highest long-term value accumulates. Add batch clustering and correction graduation third, once enough corrections exist to make pattern detection meaningful.</p> <p>Three non-obvious insights emerged from this research. First, effective context windows are roughly half of advertised context windows \u2014 plan the tiered routing around 100K tokens, not 1M. Second, the correction graduation pipeline (correction \u2192 pattern \u2192 preference \u2192 rule) is the single most valuable feature for an AI agent memory system, yet no existing open-source project implements it fully. Third, session-level quality scores as chunk reranking signals create a virtuous cycle: high-quality sessions get their chunks surfaced more often, which means the agent learns from its best interactions rather than its worst.</p> <p>The total investment to process all 800 existing sessions through Gemini Flash Batch API is approximately $26. The schema handles 268K+ rows comfortably within SQLite's capabilities, with sqlite-vec providing 50\u2013200ms vector search and FTS5 enabling instant keyword search. The architecture is designed to grow: as BrainLayer indexes more sessions, the corrections knowledge base becomes increasingly valuable, with each new correction either reinforcing existing patterns or revealing new ones.</p>"}]}